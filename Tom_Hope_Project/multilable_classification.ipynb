{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdf80586-5304-45e2-8065-775b3c9e8211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 13:15:04.739451: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-07 13:15:04.751043: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-07 13:15:04.754494: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 13:15:04.763289: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-07 13:15:08.013726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from evaluate import load\n",
    "import datetime\n",
    "import os\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26ed199-2780-4a6a-9e54-a6aa62dff636",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETSCAN_DATASET_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/targetscan_RNA_combined_nrows_90000_hg38.csv\"\n",
    "KMER_SIZE = 6\n",
    "model_name = \"InstaDeepAI/nucleotide-transformer-500m-1000g\"\n",
    "MAX_TOKEN = 1000\n",
    "BLANK_LABLE_VALUE = -100\n",
    "ADD_BLANKS_TO_END = False\n",
    "TRAIN_AMOUNT = 0.005\n",
    "EPOCH_NUM = 10\n",
    "METRIC = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eae0bf35-22cd-485a-b9c9-7b48364d4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliceAI data:\n",
    "TRAIN_DF_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/spliceai_train.csv\"\n",
    "TEST_DF_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/spliceai_test.csv\"\n",
    "train_df = pd.read_csv(TRAIN_DF_PATH)\n",
    "test_df = pd.read_csv(TEST_DF_PATH)\n",
    "SPLICEAI_LABLES = ['none', 'start','end']\n",
    "\n",
    "SPLICE_AI = True\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c219cad5-a9e3-4eda-9122-f6ac587d4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_representation_to_list(df,seq_col_name=\"sequnces\",labels_col_name=\"labels\"):\n",
    "    df[\"result\"] = df[seq_col_name].str.replace(\"'\",\"\")\n",
    "    df[\"result\"] = df[\"result\"].apply(lambda x:x.strip('][').split(', '))\n",
    "    df[\"stags\"] = df[labels_col_name].apply(lambda x:x.strip('][').replace(\"'\",\"\").split(', '))\n",
    "    df['stags'] = df['stags'].apply(lambda x: list(map(SPLICEAI_LABLES.index, x)))\n",
    "    return df\n",
    "if SPLICE_AI == True:\n",
    "    train_df = convert_list_representation_to_list(train_df)\n",
    "    test_df = convert_list_representation_to_list(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd43617e-e04d-4d82-86e8-ac6f59b70d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "def create_sequnces_from_df(df):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for row_idx, row in df.iterrows():\n",
    "        if row_idx % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            # print(filename.split(\"/\")[-1])\n",
    "            print(str(int(100*(row_idx / len(df)))) + \"%\")\n",
    "\n",
    "        curr_label = []\n",
    "        for i, seq in enumerate(row[\"result\"]):\n",
    "            curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "        \n",
    "        label = np.zeros(len(curr_label)//KMER_SIZE,dtype=np.int64)\n",
    "        for i in range(0,len(curr_label),KMER_SIZE):\n",
    "            # if i+KMER_SIZE <len(curr_label) and 1 in curr_label[i:i+KMER_SIZE]) :\n",
    "            #     label[i//KMER_SIZE:i//6+1] = 1\n",
    "            \n",
    "            for j in range(1,len(SPLICEAI_LABLES)):\n",
    "                if i+KMER_SIZE <len(curr_label) and j in curr_label[i:i+KMER_SIZE]:\n",
    "                    label[i//KMER_SIZE:i//KMER_SIZE+1] = j\n",
    "                    break\n",
    "            else: # unneeded\n",
    "                label[i//KMER_SIZE:i//KMER_SIZE+1] = 0\n",
    "        seq_str = \"\".join(row[\"result\"])[:len(curr_label)]\n",
    "        last_i = 0\n",
    "        \n",
    "        for i in range(0,len(label),MAX_TOKEN):\n",
    "            if sum(label[i:i+MAX_TOKEN]) == 0:\n",
    "                last_i = i\n",
    "                continue\n",
    "            sequences.append(seq_str[i*6:(i+MAX_TOKEN)*6])\n",
    "            labels.append(label[i:i+MAX_TOKEN])\n",
    "            last_i = i\n",
    "        if ADD_BLANKS_TO_END:\n",
    "            sequences.append(seq_str[last_i*6:])\n",
    "            \n",
    "            labels.append(np.concatenate(\n",
    "                [label[last_i:] ,  \n",
    "                np.array(\n",
    "                    [BLANK_LABLE_VALUE]* \n",
    "                    (MAX_TOKEN - len(label[last_i:])))]))\n",
    "    return sequences, labels\n",
    "train_seq, train_labels = create_sequnces_from_df(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8496343-4ba5-430e-ad2a-289e7f2af8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = train_seq[:int(len(train_seq) * TRAIN_AMOUNT)]\n",
    "train_labels = train_labels[:int(len(train_labels) * TRAIN_AMOUNT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ae572d-e698-46f0-be00-c125f64a0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158884e9-4e4e-473f-90c4-a0ff61f48700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6cd0e-45e6-49f8-beba-cb9281b902d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
