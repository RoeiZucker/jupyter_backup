{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af5d6f2e",
   "metadata": {},
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers as well as some other libraries. Uncomment the following cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5bf8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers evaluate datasets requests pandas sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e71a3f",
   "metadata": {},
   "source": [
    "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
    "\n",
    "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
    "\n",
    "First you have to store your authentication token from the Hugging Face website (sign up here if you haven't already!) then execute the following cell and input your username and password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25b8526a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31f43c7e49e40e19f520d9eb42e8ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b2712",
   "metadata": {},
   "source": [
    "Then you need to install Git-LFS. Uncomment the following instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19e8f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbad4e",
   "metadata": {},
   "source": [
    "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d107b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"protein_language_modeling_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0749e1",
   "metadata": {},
   "source": [
    "# Fine-Tuning Protein Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81db83",
   "metadata": {},
   "source": [
    "In this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on tasks of interest. If that sentence feels a bit intimidating to you, don't panic - there's [a blog post](https://huggingface.co/blog/deep-learning-with-proteins) that explains the concepts here in much more detail.\n",
    "\n",
    "The specific model we're going to use is ESM-2, which is the state-of-the-art protein language model at the time of writing (November 2022). The citation for this model is [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
    "\n",
    "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints (at time of writing) are:\n",
    "\n",
    "| Checkpoint name | Num layers | Num parameters |\n",
    "|------------------------------|----|----------|\n",
    "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
    "| `esm2_t36_3B_UR50D`          | 36 | 3B      | \n",
    "| `esm2_t33_650M_UR50D`        | 33 | 650M    | \n",
    "| `esm2_t30_150M_UR50D`        | 30 | 150M    | \n",
    "| `esm2_t12_35M_UR50D`         | 12 | 35M     | \n",
    "| `esm2_t6_8M_UR50D`           | 6  | 8M      | \n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! Also, note that memory usage for attention during training will scale as `O(batch_size * num_layers * seq_len^2)`, so larger models on long sequences will use quite a lot of memory! We will use the `esm2_t12_35M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e605a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6ac19",
   "metadata": {},
   "source": [
    "# Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb400c",
   "metadata": {},
   "source": [
    "One of the most common tasks you can perform with a language model is **sequence classification**. In sequence classification, we classify an entire protein into a category, from a list of two or more possibilities. There's no limit on the number of categories you can use, or the specific problem you choose, as long as it's something the model could in theory infer from the raw protein sequence. To keep things simple for this example, though, let's try classifying proteins by their cellular localization - given their sequence, can we predict if they're going to be found in the cytosol (the fluid inside the cell) or embedded in the cell membrane?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc122f",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91d394",
   "metadata": {},
   "source": [
    "In this section, we're going to gather some training data from UniProt. Our goal is to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences, which will just be strings like \"MNKL...\", where each letter represents a single amino acid in the complete protein. `labels` will be a list of the category for each sequence. The categories will just be integers, with 0 representing the first category, 1 representing the second and so on. In other words, if `sequences[i]` is a protein sequence then `labels[i]` should be its corresponding category. These will form the **training data** we're going to use to teach the model the task we want it to do.\n",
    "\n",
    "If you're adapting this notebook for your own use, this will probably be the main section you want to change! You can do whatever you want here, as long as you create those two lists by the end of it. If you want to follow along with this example, though, first we'll need to `import requests` and set up our query to UniProt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c718ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Ccc_subcellular_location&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2edc14",
   "metadata": {},
   "source": [
    "This query URL might seem mysterious, but it isn't! To get it, we searched for `(organism_id:9606) AND (reviewed:true) AND (length:[80 TO 500])` on UniProt to get a list of reasonably-sized human proteins,\n",
    "then selected 'Download', and set the format to TSV and the columns to `Sequence` and `Subcellular location [CC]`, since those contain the data we care about for this task.\n",
    "\n",
    "Once that's done, selecting `Generate URL for API` gives you a URL you can pass to Requests. Alternatively, if you're not on Colab you can just download the data through the web interface and open the file locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd03ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_request = requests.get(query_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7217b77",
   "metadata": {},
   "source": [
    "To get this data into Pandas, we use a `BytesIO` object, which Pandas will treat like a file. If you downloaded the data as a file you can skip this bit and just pass the filepath directly to `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2c05017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0AVI4</td>\n",
       "      <td>MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Endoplasmic reticulum me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0PJY2</td>\n",
       "      <td>MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Nucleus {ECO:0000269|Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11981</th>\n",
       "      <td>Q9H8W2</td>\n",
       "      <td>MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>Q9HAA7</td>\n",
       "      <td>MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11983</th>\n",
       "      <td>Q9NZ38</td>\n",
       "      <td>MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>Q9UFV3</td>\n",
       "      <td>MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11985</th>\n",
       "      <td>Q9Y6C7</td>\n",
       "      <td>MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11986 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "1          A0AVI4  MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...   \n",
       "2          A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "4          A0PJY2  MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...   \n",
       "...           ...                                                ...   \n",
       "11981      Q9H8W2  MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...   \n",
       "11982      Q9HAA7  MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...   \n",
       "11983      Q9NZ38  MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...   \n",
       "11984      Q9UFV3  MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...   \n",
       "11985      Q9Y6C7  MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "1      SUBCELLULAR LOCATION: Endoplasmic reticulum me...  \n",
       "2           SUBCELLULAR LOCATION: Nucleus {ECO:0000305}.  \n",
       "3      SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...  \n",
       "4      SUBCELLULAR LOCATION: Nucleus {ECO:0000269|Pub...  \n",
       "...                                                  ...  \n",
       "11981                                                NaN  \n",
       "11982                                                NaN  \n",
       "11983                                                NaN  \n",
       "11984                                                NaN  \n",
       "11985                                                NaN  \n",
       "\n",
       "[11986 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "\n",
    "df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdf34b",
   "metadata": {},
   "source": [
    "Nice! Now we have some proteins and their subcellular locations. Let's start filtering this down. First, let's ditch the columns without subcellular location information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d87663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Drop proteins with missing columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d1af5c",
   "metadata": {},
   "source": [
    "Now we'll make one dataframe of proteins that contain `cytosol` or `cytoplasm` in their subcellular localization column, and a second that mentions the `membrane` or `cell membrane`. To ensure we don't get overlap, we ensure each dataframe only contains proteins that don't match the other search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c831bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic = df['Subcellular location [CC]'].str.contains(\"Cytosol\") | df['Subcellular location [CC]'].str.contains(\"Cytoplasm\")\n",
    "membrane = df['Subcellular location [CC]'].str.contains(\"Membrane\") | df['Subcellular location [CC]'].str.contains(\"Cell membrane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41139a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A1E959</td>\n",
       "      <td>MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A1XBS5</td>\n",
       "      <td>MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A2RU49</td>\n",
       "      <td>MSSGNYQQSEALSKPTFSEEQASALVESVFGLKVSKVRPLPSYDDQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A2RUH7</td>\n",
       "      <td>MEAATAPEVAAGSKLKVKEASPADAEPPQASPGQGAGSPTPQLLPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, myofibril, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>A4D126</td>\n",
       "      <td>MEAGPPGSARPAEPGPCLSGQRGADHTASASLQSVAGTEPGRHPQA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11527</th>\n",
       "      <td>Q8TDY3</td>\n",
       "      <td>MFNPHALDSPAVIFDNGSGFCKAGLSGEFGPRHMVSSIVGHLKFQA...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>Q8WWF8</td>\n",
       "      <td>MAGTARHDREMAIQAKKKLTTATDPIERLRLQCLARGSAGIKGLGR...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11666</th>\n",
       "      <td>Q9NUJ7</td>\n",
       "      <td>MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>Q9NVM6</td>\n",
       "      <td>MAVTKELLQMDLYALLGIEEKAADKEVKKAYRQKALSCHPDKNPDN...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm {ECO:0000250|U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11677</th>\n",
       "      <td>Q9P2W6</td>\n",
       "      <td>MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cytoplasm.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2625 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "9      A1E959  MKIIILLGFLGATLSAPLIPQRLMSASNSNELLLNLNNGQLLPLQL...   \n",
       "14     A1XBS5  MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...   \n",
       "18     A2RU49  MSSGNYQQSEALSKPTFSEEQASALVESVFGLKVSKVRPLPSYDDQ...   \n",
       "20     A2RUH7  MEAATAPEVAAGSKLKVKEASPADAEPPQASPGQGAGSPTPQLLPP...   \n",
       "21     A4D126  MEAGPPGSARPAEPGPCLSGQRGADHTASASLQSVAGTEPGRHPQA...   \n",
       "...       ...                                                ...   \n",
       "11527  Q8TDY3  MFNPHALDSPAVIFDNGSGFCKAGLSGEFGPRHMVSSIVGHLKFQA...   \n",
       "11540  Q8WWF8  MAGTARHDREMAIQAKKKLTTATDPIERLRLQCLARGSAGIKGLGR...   \n",
       "11666  Q9NUJ7  MGGQVSASNSFSRLHCRNANEDWMSALCPRLWDVPLHHLSIPGSHD...   \n",
       "11668  Q9NVM6  MAVTKELLQMDLYALLGIEEKAADKEVKKAYRQKALSCHPDKNPDN...   \n",
       "11677  Q9P2W6  MGRTWCGMWRRRRPGRRSAVPRWPHLSSQSGVEPPDRWTGTPGWPS...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "9      SUBCELLULAR LOCATION: Secreted {ECO:0000250|Un...  \n",
       "14     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "18        SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "20     SUBCELLULAR LOCATION: Cytoplasm, myofibril, sa...  \n",
       "21     SUBCELLULAR LOCATION: Cytoplasm, cytosol {ECO:...  \n",
       "...                                                  ...  \n",
       "11527  SUBCELLULAR LOCATION: Cytoplasm, cytoskeleton ...  \n",
       "11540     SUBCELLULAR LOCATION: Cytoplasm {ECO:0000305}.  \n",
       "11666  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000269|P...  \n",
       "11668  SUBCELLULAR LOCATION: Cytoplasm {ECO:0000250|U...  \n",
       "11677                   SUBCELLULAR LOCATION: Cytoplasm.  \n",
       "\n",
       "[2625 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cytosolic_df = df[cytosolic & ~membrane]\n",
    "cytosolic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be5c420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Subcellular location [CC]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A2RU14</td>\n",
       "      <td>MAGTVLGVGAGVFILALLWVAVLLLCVLLSRASGAARFSVIFLFFG...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A5X5Y0</td>\n",
       "      <td>MEGSWFHRKRFSFYLLLGFLLQGRGVTFTINCSGFGQHGADPTALN...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Postsynaptic cell membra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>A6ND01</td>\n",
       "      <td>MACWWPLLLELWTVMPTWAGDELLNICMNAKHHKRVPSPEDKLYEE...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Cell membrane {ECO:00002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11901</th>\n",
       "      <td>Q86UQ5</td>\n",
       "      <td>MQSDIYHPGHSFPSWVLCWVHSCGHEGHLRETAEIRKTHQNGDLQI...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11925</th>\n",
       "      <td>Q8N8V8</td>\n",
       "      <td>MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11965</th>\n",
       "      <td>Q96N68</td>\n",
       "      <td>MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11972</th>\n",
       "      <td>Q9H0A3</td>\n",
       "      <td>MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11975</th>\n",
       "      <td>Q9H354</td>\n",
       "      <td>MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...</td>\n",
       "      <td>SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2541 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "17         A2RU14  MAGTVLGVGAGVFILALLWVAVLLLCVLLSRASGAARFSVIFLFFG...   \n",
       "32         A5X5Y0  MEGSWFHRKRFSFYLLLGFLLQGRGVTFTINCSGFGQHGADPTALN...   \n",
       "35         A6ND01  MACWWPLLLELWTVMPTWAGDELLNICMNAKHHKRVPSPEDKLYEE...   \n",
       "...           ...                                                ...   \n",
       "11901      Q86UQ5  MQSDIYHPGHSFPSWVLCWVHSCGHEGHLRETAEIRKTHQNGDLQI...   \n",
       "11925      Q8N8V8  MLLKVRRASLKPPATPHQGAFRAGNVIGQLIYLLTWSLFTAWLRPP...   \n",
       "11965      Q96N68  MQGQGALKESHIHLPTEQPEASLVLQGQLAESSALGPKGALRPQAQ...   \n",
       "11972      Q9H0A3  MMNNTDFLMLNNPWNKLCLVSMDFCFPLDFVSNLFWIFASKFIIVT...   \n",
       "11975      Q9H354  MNKHNLRLVQLASELILIEIIPKLFLSQVTTISHIKREKIPPNHRK...   \n",
       "\n",
       "                               Subcellular location [CC]  \n",
       "0      SUBCELLULAR LOCATION: [Isoform 1]: Membrane {E...  \n",
       "3      SUBCELLULAR LOCATION: Secreted {ECO:0000303|Pu...  \n",
       "17     SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "32     SUBCELLULAR LOCATION: Postsynaptic cell membra...  \n",
       "35     SUBCELLULAR LOCATION: Cell membrane {ECO:00002...  \n",
       "...                                                  ...  \n",
       "11901  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11925  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11965  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "11972  SUBCELLULAR LOCATION: Membrane {ECO:0000255}; ...  \n",
       "11975  SUBCELLULAR LOCATION: Membrane {ECO:0000305}; ...  \n",
       "\n",
       "[2541 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "membrane_df = df[membrane & ~cytosolic]\n",
    "membrane_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8cea6",
   "metadata": {},
   "source": [
    "We're almost done! Now, let's make a list of sequences from each df and generate the associated labels. We'll use `0` as the label for cytosolic proteins and `1` as the label for membrane proteins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "023ec31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cytosolic_sequences = cytosolic_df[\"Sequence\"].tolist()\n",
    "cytosolic_labels = [0 for protein in cytosolic_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0e7318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "membrane_sequences = membrane_df[\"Sequence\"].tolist()\n",
    "membrane_labels = [1 for protein in membrane_sequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4bbda2",
   "metadata": {},
   "source": [
    "Now we can concatenate these lists together to get the `sequences` and `labels` lists that will form our final training data. Don't worry - they'll get shuffled during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dec7a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = cytosolic_sequences + membrane_sequences\n",
    "labels = cytosolic_labels + membrane_labels\n",
    "\n",
    "# Quick check to make sure we got it right\n",
    "len(sequences) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc782dd0",
   "metadata": {},
   "source": [
    "Phew!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aac39c",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9099e7c",
   "metadata": {},
   "source": [
    "Since the data we're loading isn't prepared for us as a machine learning dataset, we'll have to split the data into train and test sets ourselves! We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "366147ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29b4ed",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02baaf7",
   "metadata": {},
   "source": [
    "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
    "\n",
    "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different. Let's get our tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddbe2b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16be37",
   "metadata": {},
   "source": [
    "Let's try a single sequence to see what the outputs from our tokenizer look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "687386af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 20, 23, 4, 5, 6, 23, 11, 14, 10, 15, 5, 5, 5, 14, 6, 10, 6, 5, 4, 14, 10, 5, 10, 4, 14, 10, 11, 5, 14, 5, 5, 5, 11, 20, 18, 16, 14, 5, 5, 15, 10, 6, 18, 11, 12, 9, 8, 4, 7, 5, 15, 13, 6, 6, 11, 6, 6, 6, 11, 6, 6, 6, 6, 5, 6, 8, 21, 4, 4, 5, 5, 5, 5, 8, 9, 9, 14, 4, 10, 14, 11, 5, 4, 17, 19, 14, 21, 14, 8, 5, 5, 9, 5, 5, 18, 7, 8, 6, 18, 14, 5, 5, 5, 5, 5, 6, 5, 6, 10, 8, 4, 19, 6, 6, 14, 9, 4, 7, 18, 14, 9, 5, 20, 17, 21, 14, 5, 4, 11, 7, 21, 14, 5, 21, 16, 4, 6, 5, 8, 14, 4, 16, 14, 14, 21, 8, 18, 18, 6, 5, 16, 21, 10, 13, 14, 4, 21, 18, 19, 14, 22, 7, 4, 10, 17, 10, 18, 18, 6, 21, 10, 18, 16, 5, 8, 13, 7, 14, 16, 13, 6, 4, 4, 4, 21, 6, 14, 18, 5, 10, 15, 14, 15, 10, 12, 10, 11, 5, 18, 8, 14, 8, 16, 4, 4, 10, 4, 9, 10, 5, 18, 9, 15, 17, 21, 19, 7, 7, 6, 5, 9, 10, 15, 16, 4, 5, 6, 8, 4, 8, 4, 8, 9, 11, 16, 7, 15, 7, 22, 18, 16, 17, 10, 10, 11, 15, 19, 15, 10, 16, 15, 4, 9, 9, 9, 6, 14, 9, 8, 9, 16, 15, 15, 15, 6, 8, 21, 21, 12, 17, 10, 22, 10, 12, 5, 11, 15, 16, 5, 17, 6, 9, 13, 12, 13, 7, 11, 8, 17, 13, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a719808",
   "metadata": {},
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56e26ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3681d1",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85089e49",
   "metadata": {},
   "source": [
    "Now we want to turn this data into a dataset that PyTorch can load samples from. We can use the HuggingFace `Dataset` class for this, although if you prefer you can also use `torch.utils.data.Dataset`, at the cost of some more boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb79ba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 3874\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e809e47",
   "metadata": {},
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "090acc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3874\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9aaa8",
   "metadata": {},
   "source": [
    "Looks good! We're ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af074a5c",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab5d70",
   "metadata": {},
   "source": [
    "Next, we want to load our model. Make sure to use exactly the same model as you used when loading the tokenizer, or your model might not understand the tokenization scheme you're using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc164b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 16:29:45.692031: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 16:29:45.703391: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 16:29:45.706809: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 16:29:45.715788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 16:29:47.604999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = max(train_labels + test_labels) + 1  # Add 1 since 0 can be a label\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcba23",
   "metadata": {},
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
    "\n",
    "Next, we initialize our `TrainingArguments`. These control the various training hyperparameters, and will be passed to our `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775cb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-localization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95d099",
   "metadata": {},
   "source": [
    "Next, we define the metric we will use to evaluate our models and write a `compute_metrics` function. We can load this from the `evaluate` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "471cef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dcf25",
   "metadata": {},
   "source": [
    "And at last we're ready to initialize our `Trainer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e212b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32924d0d",
   "metadata": {},
   "source": [
    "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it one last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a pad method that will do all of this right for us, and the `Trainer` will use it. You can customize this part by defining and passing your own `data_collator` which will receive samples like the dictionaries seen above and will need to return a dictionary of tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7a24c",
   "metadata": {},
   "source": [
    "We can now finetune our model by just calling the `train` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c3cf6da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1455' max='1455' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1455/1455 06:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.155993</td>\n",
       "      <td>0.957430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.150983</td>\n",
       "      <td>0.956656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.159956</td>\n",
       "      <td>0.956656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1455, training_loss=0.1718378912542284, metrics={'train_runtime': 420.1825, 'train_samples_per_second': 27.659, 'train_steps_per_second': 3.463, 'total_flos': 1046785174993044.0, 'train_loss': 0.1718378912542284, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec59f4",
   "metadata": {},
   "source": [
    "Nice! After three epochs we have a model accuracy of ~94%. Note that we didn't do a lot of work to filter the training data or tune hyperparameters for this experiment, and also that we used one of the smallest ESM-2 models. With a larger starting model and more effort to ensure that the training data categories were cleanly separable, accuracy could almost certainly go a lot higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ef458",
   "metadata": {},
   "source": [
    "***\n",
    "# Token classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d701ed",
   "metadata": {},
   "source": [
    "Another common language model task is **token classification**. In this task, instead of classifying the whole sequence into a single category, we categorize each token (amino acid, in this case!) into one or more categories. This kind of model could be useful for:\n",
    "\n",
    "- Predicting secondary structure\n",
    "- Predicting buried vs. exposed residues\n",
    "- Predicting residues that will receive post-translational modifications\n",
    "- Predicting residues involved in binding pockets or active sites\n",
    "- Probably several other things, it's been a while since I was a postdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e00afe",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9e75c",
   "metadata": {},
   "source": [
    "In this section, we're going to gather some training data from UniProt. As in the sequence classification example, we aim to create two lists: `sequences` and `labels`. Unlike in that example, however, the `labels` are more than just single integers. Instead, the label for each sample will be **one integer per token in the input**. This should make sense - when we do token classification, different tokens in the input may have different categories!\n",
    "\n",
    "To demonstrate token classification, we're going to go back to UniProt and get some data on protein secondary structures. As above, this will probably the main section you want to change when adapting this code to your own problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf52cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Cft_strand%2Cft_helix&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c902be",
   "metadata": {},
   "source": [
    "This time, our UniProt search was `(organism_id:9606) AND (reviewed:true) AND (length:[100 TO 1000])` as it was in the first example, but instead of `Subcellular location [CC]` we take the `Helix` and `Beta strand` columns, as they contain the secondary structure information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be65f529",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_request = requests.get(query_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f683dd7",
   "metadata": {},
   "source": [
    "To get this data into Pandas, we use a `BytesIO` object, which Pandas will treat like a file. If you downloaded the data as a file you can skip this bit and just pass the filepath directly to `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f49439ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Beta strand</th>\n",
       "      <th>Helix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0AVI4</td>\n",
       "      <td>MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"</td>\n",
       "      <td>HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0PJY2</td>\n",
       "      <td>MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11981</th>\n",
       "      <td>Q9H8W2</td>\n",
       "      <td>MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11982</th>\n",
       "      <td>Q9HAA7</td>\n",
       "      <td>MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11983</th>\n",
       "      <td>Q9NZ38</td>\n",
       "      <td>MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>Q9UFV3</td>\n",
       "      <td>MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11985</th>\n",
       "      <td>Q9Y6C7</td>\n",
       "      <td>MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11986 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "1          A0AVI4  MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...   \n",
       "2          A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "3          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "4          A0PJY2  MDSSCHNATTKMLATAPARGNMMSTSKPLAFSIERIMARTPEPKAL...   \n",
       "...           ...                                                ...   \n",
       "11981      Q9H8W2  MRPGSSPRAPECGAPALPRPQLDRLPARPAPSRGRGAPSLRWPAKE...   \n",
       "11982      Q9HAA7  MLFGIRILVNTPSPLVTGLHHYNPSIHRDQGECANQWRKGPGSAHL...   \n",
       "11983      Q9NZ38  MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...   \n",
       "11984      Q9UFV3  MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...   \n",
       "11985      Q9Y6C7  MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...   \n",
       "\n",
       "                                           Beta strand  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2      STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "...                                                ...   \n",
       "11981                                              NaN   \n",
       "11982                                              NaN   \n",
       "11983                                              NaN   \n",
       "11984                                              NaN   \n",
       "11985                                              NaN   \n",
       "\n",
       "                                                   Helix  \n",
       "0                                                    NaN  \n",
       "1                                                    NaN  \n",
       "2      HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...  \n",
       "3                                                    NaN  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "11981                                                NaN  \n",
       "11982                                                NaN  \n",
       "11983                                                NaN  \n",
       "11984                                                NaN  \n",
       "11985                                                NaN  \n",
       "\n",
       "[11986 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "\n",
    "df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736010f0",
   "metadata": {},
   "source": [
    "Since not all proteins have this structural information, we discard proteins that have no annotated beta strands or alpha helices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39ce9a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Beta strand</th>\n",
       "      <th>Helix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"</td>\n",
       "      <td>HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>A1L3X0</td>\n",
       "      <td>MAFSDLTSRTVHLYDNWIKDADPRVEDWLLMSSPLPQTILLGFYVY...</td>\n",
       "      <td>STRAND 97..99; /evidence=\"ECO:0007829|PDB:6Y7F\"</td>\n",
       "      <td>HELIX 17..20; /evidence=\"ECO:0007829|PDB:6Y7F\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A1XBS5</td>\n",
       "      <td>MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HELIX 2..6; /evidence=\"ECO:0007829|PDB:8CEG\"; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A1Z1Q3</td>\n",
       "      <td>MYPSNKKKKVWREEKERLLKMTLEERRKEYLRDYIPLNSILSWKEE...</td>\n",
       "      <td>STRAND 71..77; /evidence=\"ECO:0007829|PDB:4IQY...</td>\n",
       "      <td>HELIX 11..19; /evidence=\"ECO:0007829|PDB:4IQY\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A2RUC4</td>\n",
       "      <td>MAGQHLPVPRLEGVSREQFMQHLYPQRKPLVLEGIDLGPCTSKWTV...</td>\n",
       "      <td>STRAND 10..13; /evidence=\"ECO:0007829|PDB:3AL5...</td>\n",
       "      <td>HELIX 16..22; /evidence=\"ECO:0007829|PDB:3AL5\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11631</th>\n",
       "      <td>Q9H0W7</td>\n",
       "      <td>MPTNCAAAGCATTYNKHINISFHRFPLDPKRRKEWVRLVRRKNFVP...</td>\n",
       "      <td>STRAND 7..9; /evidence=\"ECO:0007829|PDB:2D8R\";...</td>\n",
       "      <td>HELIX 29..38; /evidence=\"ECO:0007829|PDB:2D8R\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>Q9NVM6</td>\n",
       "      <td>MAVTKELLQMDLYALLGIEEKAADKEVKKAYRQKALSCHPDKNPDN...</td>\n",
       "      <td>STRAND 172..176; /evidence=\"ECO:0007829|PDB:2D...</td>\n",
       "      <td>HELIX 191..199; /evidence=\"ECO:0007829|PDB:2D9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11674</th>\n",
       "      <td>Q9P1F3</td>\n",
       "      <td>MNVDHEVNLLVEEIHRLGSKNADGKLSVKFGVLFRDDKCANLFEAL...</td>\n",
       "      <td>STRAND 24..29; /evidence=\"ECO:0007829|PDB:2L2O...</td>\n",
       "      <td>HELIX 3..17; /evidence=\"ECO:0007829|PDB:2L2O\";...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>Q9P298</td>\n",
       "      <td>MSANRRWWVPPDDEDCVSEKLLRKTRESPLVPIGLGGCLVVAAYRI...</td>\n",
       "      <td>STRAND 11..14; /evidence=\"ECO:0007829|PDB:2LON...</td>\n",
       "      <td>HELIX 18..24; /evidence=\"ECO:0007829|PDB:2LON\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11683</th>\n",
       "      <td>Q9UIY3</td>\n",
       "      <td>MSASVKESLQLQLLEMEMLFSMFPNQGEVKLEDVNALTNIKRYLEG...</td>\n",
       "      <td>STRAND 28..32; /evidence=\"ECO:0007829|PDB:2DAW...</td>\n",
       "      <td>HELIX 5..22; /evidence=\"ECO:0007829|PDB:2DAW\";...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4229 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "2      A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "13     A1L3X0  MAFSDLTSRTVHLYDNWIKDADPRVEDWLLMSSPLPQTILLGFYVY...   \n",
       "14     A1XBS5  MMRRTLENRNAQTKQLQTAVSNVEKHFGELCQIFAAYVRKTARLRD...   \n",
       "15     A1Z1Q3  MYPSNKKKKVWREEKERLLKMTLEERRKEYLRDYIPLNSILSWKEE...   \n",
       "19     A2RUC4  MAGQHLPVPRLEGVSREQFMQHLYPQRKPLVLEGIDLGPCTSKWTV...   \n",
       "...       ...                                                ...   \n",
       "11631  Q9H0W7  MPTNCAAAGCATTYNKHINISFHRFPLDPKRRKEWVRLVRRKNFVP...   \n",
       "11668  Q9NVM6  MAVTKELLQMDLYALLGIEEKAADKEVKKAYRQKALSCHPDKNPDN...   \n",
       "11674  Q9P1F3  MNVDHEVNLLVEEIHRLGSKNADGKLSVKFGVLFRDDKCANLFEAL...   \n",
       "11676  Q9P298  MSANRRWWVPPDDEDCVSEKLLRKTRESPLVPIGLGGCLVVAAYRI...   \n",
       "11683  Q9UIY3  MSASVKESLQLQLLEMEMLFSMFPNQGEVKLEDVNALTNIKRYLEG...   \n",
       "\n",
       "                                             Beta strand  \\\n",
       "2        STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"   \n",
       "13       STRAND 97..99; /evidence=\"ECO:0007829|PDB:6Y7F\"   \n",
       "14                                                   NaN   \n",
       "15     STRAND 71..77; /evidence=\"ECO:0007829|PDB:4IQY...   \n",
       "19     STRAND 10..13; /evidence=\"ECO:0007829|PDB:3AL5...   \n",
       "...                                                  ...   \n",
       "11631  STRAND 7..9; /evidence=\"ECO:0007829|PDB:2D8R\";...   \n",
       "11668  STRAND 172..176; /evidence=\"ECO:0007829|PDB:2D...   \n",
       "11674  STRAND 24..29; /evidence=\"ECO:0007829|PDB:2L2O...   \n",
       "11676  STRAND 11..14; /evidence=\"ECO:0007829|PDB:2LON...   \n",
       "11683  STRAND 28..32; /evidence=\"ECO:0007829|PDB:2DAW...   \n",
       "\n",
       "                                                   Helix  \n",
       "2      HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...  \n",
       "13     HELIX 17..20; /evidence=\"ECO:0007829|PDB:6Y7F\"...  \n",
       "14     HELIX 2..6; /evidence=\"ECO:0007829|PDB:8CEG\"; ...  \n",
       "15     HELIX 11..19; /evidence=\"ECO:0007829|PDB:4IQY\"...  \n",
       "19     HELIX 16..22; /evidence=\"ECO:0007829|PDB:3AL5\"...  \n",
       "...                                                  ...  \n",
       "11631     HELIX 29..38; /evidence=\"ECO:0007829|PDB:2D8R\"  \n",
       "11668  HELIX 191..199; /evidence=\"ECO:0007829|PDB:2D9...  \n",
       "11674  HELIX 3..17; /evidence=\"ECO:0007829|PDB:2L2O\";...  \n",
       "11676  HELIX 18..24; /evidence=\"ECO:0007829|PDB:2LON\"...  \n",
       "11683  HELIX 5..22; /evidence=\"ECO:0007829|PDB:2DAW\";...  \n",
       "\n",
       "[4229 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_structure_rows = df[\"Beta strand\"].isna() & df[\"Helix\"].isna()\n",
    "df = df[~no_structure_rows]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e372c",
   "metadata": {},
   "source": [
    "Well, this works, but that data still isn't in a clean format that we can use to build our labels. Let's take a look at one sample to see what exactly we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73e99d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"; HELIX 90..96; /evidence=\"ECO:0007829|PDB:7EMF\"; HELIX 112..116; /evidence=\"ECO:0007829|PDB:7EMF\"; HELIX 128..138; /evidence=\"ECO:0007829|PDB:7EMF\"; HELIX 147..152; /evidence=\"ECO:0007829|PDB:7EMF\"'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"Helix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5160a",
   "metadata": {},
   "source": [
    "We'll need to use a [regex](https://docs.python.org/3/howto/regex.html) to pull out each segment that's marked as being a STRAND or HELIX. What we're asking for is a list of everywhere we see the word STRAND or HELIX followed by two numbers separated by two dots. In each case where this pattern is found, we tell the regex to extract the two numbers as a tuple for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7540949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('83', '86'), ('90', '96'), ('112', '116'), ('128', '138'), ('147', '152')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "strand_re = r\"STRAND\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "helix_re = r\"HELIX\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "\n",
    "re.findall(helix_re, df.iloc[0][\"Helix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457b1a0",
   "metadata": {},
   "source": [
    "Looks good! We can use this to build our training data. Recall that the **labels** need to be a list or array of integers that's the same length as the input sequence. We're going to use 0 to indicate residues without any annotated structure, 1 for residues in an alpha helix, and 2 for residues in a beta strand. To build that, we'll start with an array of all 0s, and then fill in values based on the positions that our regex pulls out of the UniProt results.\n",
    "\n",
    "We'll use NumPy arrays rather than lists here, since these allow [slice assignment](https://numpy.org/doc/stable/user/basics.indexing.html#assigning-values-to-indexed-arrays), which will be a lot simpler than editing a list of integers. Note also that UniProt annotates residues starting from 1 (unlike Python, which starts from 0), and region annotations are inclusive (so 1..3 means residues 1, 2 and 3). To turn these into Python slices, we subtract 1 from the start of each annotation, but not the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4c97179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_labels(sequence, strands, helices):\n",
    "    # Start with all 0s\n",
    "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
    "    \n",
    "    if isinstance(helices, float): # Indicates missing (NaN)\n",
    "        found_helices = []\n",
    "    else:\n",
    "        found_helices = re.findall(helix_re, helices)\n",
    "    for helix_start, helix_end in found_helices:\n",
    "        helix_start = int(helix_start) - 1\n",
    "        helix_end = int(helix_end)\n",
    "        assert helix_end <= len(sequence)\n",
    "        labels[helix_start: helix_end] = 1  # Helix category\n",
    "    \n",
    "    if isinstance(strands, float): # Indicates missing (NaN)\n",
    "        found_strands = []\n",
    "    else:\n",
    "        found_strands = re.findall(strand_re, strands)\n",
    "    for strand_start, strand_end in found_strands:\n",
    "        strand_start = int(strand_start) - 1\n",
    "        strand_end = int(strand_end)\n",
    "        assert strand_end <= len(sequence)\n",
    "        labels[strand_start: strand_end] = 2  # Strand category\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7e7fd",
   "metadata": {},
   "source": [
    "Now we've defined a helper function, let's build our lists of sequences and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "313811fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for row_idx, row in df.iterrows():\n",
    "    row_labels = build_labels(row[\"Sequence\"], row[\"Beta strand\"], row[\"Helix\"])\n",
    "    sequences.append(row[\"Sequence\"])\n",
    "    labels.append(row_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2da67ac0-2dc4-49ea-a8de-9a653eef5e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 244\n"
     ]
    }
   ],
   "source": [
    "print(len(sequences[0]),len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dafb0597-76f7-4a63-9939-c5421a17b913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAFSDLTSRTVHLYDNWIKDADPRVEDWLLMSSPLPQTILLGFYVYFVTSLGPKLMENRKPFELKKAMITYNFFIVLFSVYMCYEFVMSGWGIGYSFRCDIVDYSRSPTALRMARTCWLYYFSKFIELLDTIFFVLRKKNSQVTFLHVFHHTIMPWTWWFGVKFAAGGLGTFHALLNTAVHVVMYSYYGLSALGPAYQKYLWWKKYLTSLQLVQFVIVAIHISQFFFMEDCKYQFPVFACIIMSYSFMFLLLFLHFWYRAYTKGQRLPKTVKNGTCKNKDN\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 2 2 2 2\n",
      " 2 2 2 0 1 1 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 2 2 2 2 2 0 0 0 2 2 2 2 2 2 2 2 2 2 0 0 0\n",
      " 0 2 2 2 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 2 2 2 2 0 0 0\n",
      " 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 2 2 2 2 2 2\n",
      " 2 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(sequences[1])\n",
    "print(labels[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc653fad-8b9b-4a6b-961e-0cf02eea5334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(labels[0])\n",
    "# type(sequences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b3ba8",
   "metadata": {},
   "source": [
    "## Creating our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619d9ae",
   "metadata": {},
   "source": [
    "Nice! Now we'll split and tokenize the data, and then create datasets - I'll go through this quite quickly here, since it's identical to how we did it in the sequence classification example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c208c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2182fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3939f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766fe4b",
   "metadata": {},
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8419b5",
   "metadata": {},
   "source": [
    "The key difference here with the above example is that we use `AutoModelForTokenClassification` instead of `AutoModelForSequenceClassification`. We will also need a `data_collator` this time, as we're in the slightly more complex case where both inputs and labels must be padded in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b26b828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eec0005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c7305",
   "metadata": {},
   "source": [
    "Now we set up our `TrainingArguments` as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7724323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-secondary-structure\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5fba9a",
   "metadata": {},
   "source": [
    "Our `compute_metrics` function is a bit more complex than in the sequence classification task, as we need to ignore padding tokens (those where the label is `-100`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "736886a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37491af5",
   "metadata": {},
   "source": [
    "And now we're ready to train our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c97836c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1191' max='1191' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1191/1191 06:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489183</td>\n",
       "      <td>0.798998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.456689</td>\n",
       "      <td>0.812544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.381300</td>\n",
       "      <td>0.456774</td>\n",
       "      <td>0.818371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1191, training_loss=0.422499835240951, metrics={'train_runtime': 365.1624, 'train_samples_per_second': 26.051, 'train_steps_per_second': 3.262, 'total_flos': 863787147135192.0, 'train_loss': 0.422499835240951, 'epoch': 3.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee2d7c0a-62ed-416b-9922-558006f850b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 20, 5, 16, 4, 16, 11, 10, 18, 19, 11, 13, 17, 15, 15, 19, 5, 7, 13, 13, 7, 14, 18, 8, 12, 14, 5, 5, 8, 9, 12, 5, 13, 4, 8, 17, 12, 12, 17, 15, 4, 4, 15, 13, 15, 17, 9, 18, 21, 15, 21, 7, 9, 18, 13, 18, 4, 12, 15, 6, 16, 18, 4, 10, 20, 14, 4, 13, 15, 21, 20, 9, 20, 9, 17, 12, 8, 8, 9, 9, 7, 7, 9, 12, 9, 19, 7, 9, 15, 19, 11, 5, 14, 16, 14, 9, 16, 23, 20, 18, 21, 13, 13, 22, 12, 8, 8, 12, 15, 6, 5, 9, 9, 22, 12, 4, 11, 6, 8, 19, 13, 15, 11, 8, 10, 12, 22, 8, 4, 9, 6, 15, 8, 12, 20, 11, 12, 7, 6, 21, 11, 13, 7, 7, 15, 13, 7, 5, 22, 7, 15, 15, 13, 8, 4, 8, 23, 4, 4, 4, 8, 5, 8, 20, 13, 16, 11, 12, 4, 4, 22, 9, 22, 17, 7, 9, 10, 17, 15, 7, 15, 5, 4, 21, 23, 23, 10, 6, 21, 5, 6, 8, 7, 13, 8, 12, 5, 7, 13, 6, 8, 6, 11, 15, 18, 23, 8, 6, 8, 22, 13, 15, 20, 4, 15, 12, 22, 8, 11, 7, 14, 11, 13, 9, 9, 13, 9, 20, 9, 9, 8, 11, 17, 10, 14, 10, 15, 15, 16, 15, 11, 9, 16, 4, 6, 4, 11, 10, 11, 14, 12, 7, 11, 4, 8, 6, 21, 20, 9, 5, 7, 8, 8, 7, 4, 22, 8, 13, 5, 9, 9, 12, 23, 8, 5, 8, 22, 13, 21, 11, 12, 10, 7, 22, 13, 7, 9, 8, 6, 8, 4, 15, 8, 11, 4, 11, 6, 17, 15, 7, 18, 17, 23, 12, 8, 19, 8, 14, 4, 23, 15, 10, 4, 5, 8, 6, 8, 11, 13, 10, 21, 12, 10, 4, 22, 13, 14, 10, 11, 15, 13, 6, 8, 4, 7, 8, 4, 8, 4, 11, 8, 21, 11, 6, 22, 7, 11, 8, 7, 15, 22, 8, 14, 11, 21, 9, 16, 16, 4, 12, 8, 6, 8, 4, 13, 17, 12, 7, 15, 4, 22, 13, 11, 10, 8, 23, 15, 5, 14, 4, 19, 13, 4, 5, 5, 21, 9, 13, 15, 7, 4, 8, 7, 13, 22, 11, 13, 11, 6, 4, 4, 4, 8, 6, 6, 5, 13, 17, 15, 4, 19, 8, 19, 10, 19, 8, 14, 11, 11, 8, 21, 7, 6, 5, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "425\n",
      "423\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    \n",
    "    print(train_dataset[i])\n",
    "    print(len(train_dataset[i][\"input_ids\"]))\n",
    "    print(len(train_dataset[i][\"labels\"]))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44674010-57a7-42a6-aff6-e926d464666f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423 425\n",
      "262 264\n",
      "193 195\n",
      "160 162\n",
      "208 210\n",
      "386 388\n",
      "289 291\n",
      "176 178\n",
      "94 96\n",
      "231 233\n",
      "281 283\n",
      "209 211\n",
      "269 271\n",
      "228 230\n",
      "445 447\n",
      "106 108\n",
      "318 320\n",
      "340 342\n",
      "94 96\n",
      "255 257\n",
      "296 298\n",
      "375 377\n",
      "391 393\n",
      "498 500\n",
      "359 361\n",
      "219 221\n",
      "489 491\n",
      "389 391\n",
      "458 460\n",
      "423 425\n",
      "241 243\n",
      "443 445\n",
      "187 189\n",
      "396 398\n",
      "261 263\n",
      "198 200\n",
      "318 320\n",
      "399 401\n",
      "481 483\n",
      "400 402\n",
      "470 472\n",
      "207 209\n",
      "124 126\n",
      "151 153\n",
      "177 179\n",
      "383 385\n",
      "117 119\n",
      "178 180\n",
      "416 418\n",
      "261 263\n",
      "420 422\n",
      "406 408\n",
      "176 178\n",
      "199 201\n",
      "244 246\n",
      "313 315\n",
      "328 330\n",
      "283 285\n",
      "307 309\n",
      "477 479\n",
      "245 247\n",
      "440 442\n",
      "243 245\n",
      "473 475\n",
      "500 502\n",
      "178 180\n",
      "341 343\n",
      "439 441\n",
      "155 157\n",
      "446 448\n",
      "275 277\n",
      "387 389\n",
      "453 455\n",
      "273 275\n",
      "215 217\n",
      "441 443\n",
      "288 290\n",
      "442 444\n",
      "193 195\n",
      "475 477\n",
      "146 148\n",
      "184 186\n",
      "225 227\n",
      "245 247\n",
      "343 345\n",
      "270 272\n",
      "384 386\n",
      "428 430\n",
      "378 380\n",
      "254 256\n",
      "230 232\n",
      "218 220\n",
      "400 402\n",
      "416 418\n",
      "298 300\n",
      "202 204\n",
      "233 235\n",
      "212 214\n",
      "168 170\n",
      "128 130\n",
      "482 484\n",
      "96 98\n",
      "361 363\n",
      "184 186\n",
      "308 310\n",
      "273 275\n",
      "181 183\n",
      "165 167\n",
      "297 299\n",
      "469 471\n",
      "103 105\n",
      "151 153\n",
      "103 105\n",
      "137 139\n",
      "331 333\n",
      "107 109\n",
      "351 353\n",
      "333 335\n",
      "228 230\n",
      "126 128\n",
      "152 154\n",
      "416 418\n",
      "434 436\n",
      "296 298\n",
      "193 195\n",
      "219 221\n",
      "432 434\n",
      "475 477\n",
      "347 349\n",
      "464 466\n",
      "473 475\n",
      "366 368\n",
      "492 494\n",
      "421 423\n",
      "331 333\n",
      "304 306\n",
      "346 348\n",
      "324 326\n",
      "112 114\n",
      "416 418\n",
      "398 400\n",
      "144 146\n",
      "190 192\n",
      "348 350\n",
      "428 430\n",
      "470 472\n",
      "205 207\n",
      "223 225\n",
      "488 490\n",
      "374 376\n",
      "138 140\n",
      "379 381\n",
      "455 457\n",
      "422 424\n",
      "264 266\n",
      "184 186\n",
      "311 313\n",
      "149 151\n",
      "317 319\n",
      "433 435\n",
      "335 337\n",
      "195 197\n",
      "277 279\n",
      "382 384\n",
      "281 283\n",
      "359 361\n",
      "115 117\n",
      "365 367\n",
      "256 258\n",
      "120 122\n",
      "377 379\n",
      "294 296\n",
      "355 357\n",
      "319 321\n",
      "403 405\n",
      "276 278\n",
      "431 433\n",
      "205 207\n",
      "466 468\n",
      "355 357\n",
      "229 231\n",
      "226 228\n",
      "454 456\n",
      "353 355\n",
      "391 393\n",
      "389 391\n",
      "207 209\n",
      "312 314\n",
      "317 319\n",
      "158 160\n",
      "232 234\n",
      "485 487\n",
      "234 236\n",
      "491 493\n",
      "277 279\n",
      "251 253\n",
      "175 177\n",
      "235 237\n",
      "179 181\n",
      "204 206\n",
      "349 351\n",
      "153 155\n",
      "380 382\n",
      "270 272\n",
      "145 147\n",
      "434 436\n",
      "391 393\n",
      "412 414\n",
      "417 419\n",
      "418 420\n",
      "489 491\n",
      "269 271\n",
      "177 179\n",
      "111 113\n",
      "495 497\n",
      "209 211\n",
      "270 272\n",
      "132 134\n",
      "267 269\n",
      "370 372\n",
      "490 492\n",
      "334 336\n",
      "244 246\n",
      "288 290\n",
      "146 148\n",
      "250 252\n",
      "166 168\n",
      "363 365\n",
      "101 103\n",
      "257 259\n",
      "339 341\n",
      "198 200\n",
      "198 200\n",
      "285 287\n",
      "394 396\n",
      "383 385\n",
      "99 101\n",
      "472 474\n",
      "131 133\n",
      "359 361\n",
      "404 406\n",
      "492 494\n",
      "218 220\n",
      "98 100\n",
      "425 427\n",
      "188 190\n",
      "256 258\n",
      "372 374\n",
      "296 298\n",
      "135 137\n",
      "211 213\n",
      "303 305\n",
      "365 367\n",
      "494 496\n",
      "125 127\n",
      "493 495\n",
      "200 202\n",
      "105 107\n",
      "347 349\n",
      "442 444\n",
      "415 417\n",
      "206 208\n",
      "189 191\n",
      "229 231\n",
      "373 375\n",
      "175 177\n",
      "332 334\n",
      "147 149\n",
      "396 398\n",
      "156 158\n",
      "173 175\n",
      "404 406\n",
      "128 130\n",
      "395 397\n",
      "292 294\n",
      "89 91\n",
      "409 411\n",
      "390 392\n",
      "124 126\n",
      "464 466\n",
      "388 390\n",
      "204 206\n",
      "292 294\n",
      "406 408\n",
      "378 380\n",
      "210 212\n",
      "437 439\n",
      "406 408\n",
      "252 254\n",
      "200 202\n",
      "348 350\n",
      "461 463\n",
      "164 166\n",
      "412 414\n",
      "390 392\n",
      "499 501\n",
      "222 224\n",
      "310 312\n",
      "152 154\n",
      "121 123\n",
      "368 370\n",
      "261 263\n",
      "89 91\n",
      "174 176\n",
      "236 238\n",
      "437 439\n",
      "184 186\n",
      "219 221\n",
      "225 227\n",
      "389 391\n",
      "183 185\n",
      "241 243\n",
      "352 354\n",
      "268 270\n",
      "388 390\n",
      "241 243\n",
      "345 347\n",
      "428 430\n",
      "358 360\n",
      "142 144\n",
      "232 234\n",
      "261 263\n",
      "299 301\n",
      "208 210\n",
      "374 376\n",
      "391 393\n",
      "421 423\n",
      "177 179\n",
      "131 133\n",
      "287 289\n",
      "406 408\n",
      "296 298\n",
      "345 347\n",
      "144 146\n",
      "490 492\n",
      "285 287\n",
      "411 413\n",
      "109 111\n",
      "417 419\n",
      "327 329\n",
      "90 92\n",
      "314 316\n",
      "396 398\n",
      "441 443\n",
      "225 227\n",
      "258 260\n",
      "351 353\n",
      "419 421\n",
      "272 274\n",
      "434 436\n",
      "360 362\n",
      "339 341\n",
      "483 485\n",
      "130 132\n",
      "399 401\n",
      "473 475\n",
      "419 421\n",
      "159 161\n",
      "447 449\n",
      "282 284\n",
      "206 208\n",
      "289 291\n",
      "382 384\n",
      "235 237\n",
      "485 487\n",
      "268 270\n",
      "201 203\n",
      "452 454\n",
      "357 359\n",
      "183 185\n",
      "443 445\n",
      "461 463\n",
      "240 242\n",
      "325 327\n",
      "394 396\n",
      "458 460\n",
      "353 355\n",
      "118 120\n",
      "234 236\n",
      "461 463\n",
      "354 356\n",
      "377 379\n",
      "116 118\n",
      "344 346\n",
      "338 340\n",
      "488 490\n",
      "434 436\n",
      "375 377\n",
      "456 458\n",
      "354 356\n",
      "341 343\n",
      "195 197\n",
      "221 223\n",
      "445 447\n",
      "185 187\n",
      "403 405\n",
      "417 419\n",
      "281 283\n",
      "128 130\n",
      "267 269\n",
      "346 348\n",
      "346 348\n",
      "455 457\n",
      "355 357\n",
      "328 330\n",
      "388 390\n",
      "184 186\n",
      "326 328\n",
      "82 84\n",
      "195 197\n",
      "480 482\n",
      "368 370\n",
      "436 438\n",
      "128 130\n",
      "206 208\n",
      "427 429\n",
      "142 144\n",
      "418 420\n",
      "327 329\n",
      "103 105\n",
      "392 394\n",
      "283 285\n",
      "116 118\n",
      "306 308\n",
      "356 358\n",
      "335 337\n",
      "81 83\n",
      "312 314\n",
      "450 452\n",
      "347 349\n",
      "339 341\n",
      "121 123\n",
      "242 244\n",
      "455 457\n",
      "323 325\n",
      "330 332\n",
      "476 478\n",
      "114 116\n",
      "133 135\n",
      "174 176\n",
      "354 356\n",
      "486 488\n",
      "337 339\n",
      "375 377\n",
      "427 429\n",
      "452 454\n",
      "91 93\n",
      "124 126\n",
      "326 328\n",
      "484 486\n",
      "387 389\n",
      "488 490\n",
      "235 237\n",
      "147 149\n",
      "192 194\n",
      "480 482\n",
      "117 119\n",
      "405 407\n",
      "314 316\n",
      "101 103\n",
      "182 184\n",
      "152 154\n",
      "107 109\n",
      "304 306\n",
      "351 353\n",
      "353 355\n",
      "274 276\n",
      "204 206\n",
      "210 212\n",
      "117 119\n",
      "374 376\n",
      "226 228\n",
      "445 447\n",
      "144 146\n",
      "255 257\n",
      "284 286\n",
      "145 147\n",
      "107 109\n",
      "261 263\n",
      "346 348\n",
      "132 134\n",
      "467 469\n",
      "126 128\n",
      "147 149\n",
      "462 464\n",
      "498 500\n",
      "372 374\n",
      "121 123\n",
      "191 193\n",
      "109 111\n",
      "492 494\n",
      "476 478\n",
      "374 376\n",
      "418 420\n",
      "360 362\n",
      "334 336\n",
      "407 409\n",
      "205 207\n",
      "233 235\n",
      "453 455\n",
      "414 416\n",
      "403 405\n",
      "218 220\n",
      "250 252\n",
      "256 258\n",
      "160 162\n",
      "160 162\n",
      "247 249\n",
      "314 316\n",
      "166 168\n",
      "233 235\n",
      "485 487\n",
      "393 395\n",
      "308 310\n",
      "426 428\n",
      "422 424\n",
      "305 307\n",
      "252 254\n",
      "212 214\n",
      "380 382\n",
      "192 194\n",
      "208 210\n",
      "187 189\n",
      "416 418\n",
      "476 478\n",
      "205 207\n",
      "471 473\n",
      "410 412\n",
      "355 357\n",
      "228 230\n",
      "207 209\n",
      "219 221\n",
      "255 257\n",
      "464 466\n",
      "157 159\n",
      "334 336\n",
      "288 290\n",
      "172 174\n",
      "215 217\n",
      "281 283\n",
      "463 465\n",
      "201 203\n",
      "379 381\n",
      "495 497\n",
      "160 162\n",
      "382 384\n",
      "379 381\n",
      "191 193\n",
      "222 224\n",
      "218 220\n",
      "339 341\n",
      "380 382\n",
      "367 369\n",
      "168 170\n",
      "393 395\n",
      "391 393\n",
      "463 465\n",
      "188 190\n",
      "228 230\n",
      "117 119\n",
      "92 94\n",
      "152 154\n",
      "107 109\n",
      "343 345\n",
      "497 499\n",
      "279 281\n",
      "494 496\n",
      "440 442\n",
      "495 497\n",
      "261 263\n",
      "330 332\n",
      "94 96\n",
      "270 272\n",
      "373 375\n",
      "89 91\n",
      "425 427\n",
      "152 154\n",
      "433 435\n",
      "444 446\n",
      "215 217\n",
      "147 149\n",
      "127 129\n",
      "300 302\n",
      "131 133\n",
      "280 282\n",
      "405 407\n",
      "316 318\n",
      "150 152\n",
      "475 477\n",
      "384 386\n",
      "101 103\n",
      "494 496\n",
      "283 285\n",
      "199 201\n",
      "483 485\n",
      "346 348\n",
      "360 362\n",
      "308 310\n",
      "454 456\n",
      "403 405\n",
      "490 492\n",
      "381 383\n",
      "363 365\n",
      "305 307\n",
      "180 182\n",
      "428 430\n",
      "433 435\n",
      "480 482\n",
      "419 421\n",
      "444 446\n",
      "181 183\n",
      "205 207\n",
      "379 381\n",
      "224 226\n",
      "135 137\n",
      "192 194\n",
      "349 351\n",
      "120 122\n",
      "260 262\n",
      "486 488\n",
      "348 350\n",
      "328 330\n",
      "377 379\n",
      "452 454\n",
      "121 123\n",
      "449 451\n",
      "140 142\n",
      "228 230\n",
      "214 216\n",
      "368 370\n",
      "100 102\n",
      "255 257\n",
      "87 89\n",
      "319 321\n",
      "445 447\n",
      "380 382\n",
      "482 484\n",
      "326 328\n",
      "290 292\n",
      "235 237\n",
      "206 208\n",
      "367 369\n",
      "184 186\n",
      "230 232\n",
      "463 465\n",
      "307 309\n",
      "227 229\n",
      "160 162\n",
      "466 468\n",
      "320 322\n",
      "213 215\n",
      "213 215\n",
      "117 119\n",
      "125 127\n",
      "110 112\n",
      "344 346\n",
      "476 478\n",
      "208 210\n",
      "292 294\n",
      "398 400\n",
      "296 298\n",
      "202 204\n",
      "289 291\n",
      "162 164\n",
      "276 278\n",
      "271 273\n",
      "467 469\n",
      "241 243\n",
      "97 99\n",
      "400 402\n",
      "483 485\n",
      "300 302\n",
      "429 431\n",
      "415 417\n",
      "254 256\n",
      "427 429\n",
      "419 421\n",
      "341 343\n",
      "463 465\n",
      "350 352\n",
      "347 349\n",
      "444 446\n",
      "348 350\n",
      "299 301\n",
      "261 263\n",
      "310 312\n",
      "275 277\n",
      "338 340\n",
      "93 95\n",
      "293 295\n",
      "88 90\n",
      "250 252\n",
      "393 395\n",
      "300 302\n",
      "372 374\n",
      "426 428\n",
      "267 269\n",
      "287 289\n",
      "434 436\n",
      "302 304\n",
      "394 396\n",
      "487 489\n",
      "130 132\n",
      "304 306\n",
      "327 329\n",
      "184 186\n",
      "354 356\n",
      "138 140\n",
      "112 114\n",
      "377 379\n",
      "350 352\n",
      "345 347\n",
      "464 466\n",
      "136 138\n",
      "213 215\n",
      "381 383\n",
      "117 119\n",
      "121 123\n",
      "398 400\n",
      "276 278\n",
      "166 168\n",
      "200 202\n",
      "414 416\n",
      "178 180\n",
      "382 384\n",
      "329 331\n",
      "212 214\n",
      "391 393\n",
      "447 449\n",
      "210 212\n",
      "196 198\n",
      "241 243\n",
      "104 106\n",
      "210 212\n",
      "261 263\n",
      "142 144\n",
      "396 398\n",
      "181 183\n",
      "154 156\n",
      "287 289\n",
      "249 251\n",
      "447 449\n",
      "453 455\n",
      "337 339\n",
      "142 144\n",
      "412 414\n",
      "158 160\n",
      "211 213\n",
      "377 379\n",
      "263 265\n",
      "284 286\n",
      "290 292\n",
      "230 232\n",
      "165 167\n",
      "280 282\n",
      "113 115\n",
      "350 352\n",
      "340 342\n",
      "279 281\n",
      "169 171\n",
      "333 335\n",
      "490 492\n",
      "219 221\n",
      "126 128\n",
      "251 253\n",
      "272 274\n",
      "497 499\n",
      "246 248\n",
      "256 258\n",
      "468 470\n",
      "294 296\n",
      "225 227\n",
      "359 361\n",
      "424 426\n",
      "353 355\n",
      "469 471\n",
      "321 323\n",
      "357 359\n",
      "180 182\n",
      "334 336\n",
      "396 398\n",
      "420 422\n",
      "219 221\n",
      "425 427\n",
      "386 388\n",
      "227 229\n",
      "433 435\n",
      "323 325\n",
      "164 166\n",
      "167 169\n",
      "110 112\n",
      "163 165\n",
      "262 264\n",
      "334 336\n",
      "453 455\n",
      "484 486\n",
      "445 447\n",
      "218 220\n",
      "167 169\n",
      "266 268\n",
      "249 251\n",
      "217 219\n",
      "363 365\n",
      "98 100\n",
      "358 360\n",
      "466 468\n",
      "151 153\n",
      "454 456\n",
      "226 228\n",
      "82 84\n",
      "100 102\n",
      "437 439\n",
      "126 128\n",
      "273 275\n",
      "252 254\n",
      "412 414\n",
      "421 423\n",
      "471 473\n",
      "215 217\n",
      "213 215\n",
      "205 207\n",
      "246 248\n",
      "387 389\n",
      "421 423\n",
      "220 222\n",
      "313 315\n",
      "462 464\n",
      "361 363\n",
      "187 189\n",
      "184 186\n",
      "439 441\n",
      "209 211\n",
      "305 307\n",
      "96 98\n",
      "130 132\n",
      "414 416\n",
      "207 209\n",
      "313 315\n",
      "93 95\n",
      "380 382\n",
      "211 213\n",
      "388 390\n",
      "115 117\n",
      "323 325\n",
      "164 166\n",
      "453 455\n",
      "195 197\n",
      "113 115\n",
      "93 95\n",
      "297 299\n",
      "181 183\n",
      "335 337\n",
      "401 403\n",
      "385 387\n",
      "128 130\n",
      "170 172\n",
      "463 465\n",
      "471 473\n",
      "226 228\n",
      "476 478\n",
      "249 251\n",
      "356 358\n",
      "161 163\n",
      "411 413\n",
      "268 270\n",
      "203 205\n",
      "353 355\n",
      "452 454\n",
      "225 227\n",
      "460 462\n",
      "245 247\n",
      "171 173\n",
      "344 346\n",
      "463 465\n",
      "416 418\n",
      "227 229\n",
      "204 206\n",
      "290 292\n",
      "296 298\n",
      "187 189\n",
      "390 392\n",
      "184 186\n",
      "129 131\n",
      "344 346\n",
      "104 106\n",
      "357 359\n",
      "390 392\n",
      "229 231\n",
      "420 422\n",
      "359 361\n",
      "276 278\n",
      "172 174\n",
      "457 459\n",
      "470 472\n",
      "246 248\n",
      "339 341\n",
      "147 149\n",
      "345 347\n",
      "140 142\n",
      "358 360\n",
      "433 435\n",
      "199 201\n",
      "179 181\n",
      "92 94\n",
      "115 117\n",
      "271 273\n",
      "89 91\n",
      "317 319\n",
      "413 415\n",
      "120 122\n",
      "254 256\n",
      "450 452\n",
      "273 275\n",
      "258 260\n",
      "351 353\n",
      "464 466\n",
      "362 364\n",
      "178 180\n",
      "398 400\n",
      "239 241\n",
      "373 375\n",
      "167 169\n",
      "454 456\n",
      "330 332\n",
      "354 356\n",
      "356 358\n",
      "396 398\n",
      "397 399\n",
      "390 392\n",
      "439 441\n",
      "363 365\n",
      "406 408\n",
      "240 242\n",
      "284 286\n",
      "483 485\n",
      "135 137\n",
      "352 354\n",
      "343 345\n",
      "241 243\n",
      "372 374\n",
      "285 287\n",
      "172 174\n",
      "129 131\n",
      "382 384\n",
      "98 100\n",
      "125 127\n",
      "133 135\n",
      "350 352\n",
      "358 360\n",
      "259 261\n",
      "323 325\n",
      "269 271\n",
      "114 116\n",
      "416 418\n",
      "273 275\n",
      "112 114\n",
      "286 288\n",
      "417 419\n",
      "322 324\n",
      "396 398\n",
      "239 241\n",
      "346 348\n",
      "260 262\n",
      "355 357\n",
      "465 467\n",
      "370 372\n",
      "368 370\n",
      "338 340\n",
      "458 460\n",
      "224 226\n",
      "339 341\n",
      "399 401\n",
      "434 436\n",
      "282 284\n",
      "378 380\n",
      "248 250\n",
      "150 152\n",
      "287 289\n",
      "272 274\n",
      "236 238\n",
      "134 136\n",
      "440 442\n",
      "314 316\n",
      "496 498\n",
      "194 196\n",
      "84 86\n",
      "283 285\n",
      "162 164\n",
      "131 133\n",
      "323 325\n",
      "398 400\n",
      "201 203\n",
      "226 228\n",
      "184 186\n",
      "142 144\n",
      "465 467\n",
      "303 305\n",
      "198 200\n",
      "398 400\n",
      "397 399\n",
      "261 263\n",
      "335 337\n",
      "385 387\n",
      "474 476\n",
      "472 474\n",
      "211 213\n",
      "384 386\n",
      "479 481\n",
      "395 397\n",
      "201 203\n",
      "314 316\n",
      "269 271\n",
      "186 188\n",
      "479 481\n",
      "297 299\n",
      "337 339\n",
      "149 151\n",
      "97 99\n",
      "410 412\n",
      "268 270\n",
      "207 209\n",
      "158 160\n",
      "354 356\n",
      "447 449\n",
      "200 202\n",
      "255 257\n",
      "427 429\n",
      "142 144\n",
      "153 155\n",
      "437 439\n",
      "231 233\n",
      "306 308\n",
      "457 459\n",
      "279 281\n",
      "456 458\n",
      "223 225\n",
      "205 207\n",
      "338 340\n",
      "469 471\n",
      "368 370\n",
      "146 148\n",
      "264 266\n",
      "171 173\n",
      "222 224\n",
      "277 279\n",
      "327 329\n",
      "359 361\n",
      "477 479\n",
      "367 369\n",
      "282 284\n",
      "147 149\n",
      "274 276\n",
      "405 407\n",
      "417 419\n",
      "224 226\n",
      "129 131\n",
      "172 174\n",
      "220 222\n",
      "416 418\n",
      "114 116\n",
      "325 327\n",
      "287 289\n",
      "264 266\n",
      "376 378\n",
      "133 135\n",
      "241 243\n",
      "349 351\n",
      "400 402\n",
      "349 351\n",
      "207 209\n",
      "329 331\n",
      "375 377\n",
      "486 488\n",
      "454 456\n",
      "131 133\n",
      "110 112\n",
      "398 400\n",
      "243 245\n",
      "490 492\n",
      "195 197\n",
      "222 224\n",
      "413 415\n",
      "139 141\n",
      "107 109\n",
      "220 222\n",
      "270 272\n",
      "326 328\n",
      "180 182\n",
      "500 502\n",
      "226 228\n",
      "320 322\n",
      "220 222\n",
      "232 234\n",
      "479 481\n",
      "270 272\n",
      "451 453\n",
      "324 326\n",
      "476 478\n",
      "341 343\n",
      "250 252\n",
      "201 203\n",
      "292 294\n",
      "419 421\n",
      "247 249\n",
      "178 180\n",
      "386 388\n",
      "403 405\n",
      "381 383\n",
      "247 249\n",
      "332 334\n",
      "434 436\n",
      "85 87\n",
      "379 381\n",
      "251 253\n",
      "223 225\n",
      "277 279\n",
      "239 241\n",
      "306 308\n",
      "227 229\n",
      "417 419\n",
      "286 288\n",
      "207 209\n",
      "399 401\n",
      "223 225\n",
      "399 401\n",
      "287 289\n",
      "153 155\n",
      "246 248\n",
      "201 203\n",
      "163 165\n",
      "302 304\n",
      "421 423\n",
      "240 242\n",
      "257 259\n",
      "429 431\n",
      "407 409\n",
      "241 243\n",
      "365 367\n",
      "364 366\n",
      "460 462\n",
      "211 213\n",
      "148 150\n",
      "469 471\n",
      "207 209\n",
      "458 460\n",
      "273 275\n",
      "316 318\n",
      "357 359\n",
      "224 226\n",
      "316 318\n",
      "153 155\n",
      "244 246\n",
      "331 333\n",
      "482 484\n",
      "94 96\n",
      "203 205\n",
      "277 279\n",
      "175 177\n",
      "364 366\n",
      "377 379\n",
      "370 372\n",
      "470 472\n",
      "172 174\n",
      "408 410\n",
      "81 83\n",
      "465 467\n",
      "441 443\n",
      "361 363\n",
      "380 382\n",
      "338 340\n",
      "495 497\n",
      "133 135\n",
      "125 127\n",
      "432 434\n",
      "475 477\n",
      "198 200\n",
      "375 377\n",
      "298 300\n",
      "326 328\n",
      "414 416\n",
      "188 190\n",
      "418 420\n",
      "443 445\n",
      "254 256\n",
      "354 356\n",
      "177 179\n",
      "412 414\n",
      "450 452\n",
      "193 195\n",
      "346 348\n",
      "402 404\n",
      "226 228\n",
      "198 200\n",
      "465 467\n",
      "214 216\n",
      "312 314\n",
      "489 491\n",
      "411 413\n",
      "478 480\n",
      "277 279\n",
      "222 224\n",
      "463 465\n",
      "116 118\n",
      "469 471\n",
      "336 338\n",
      "158 160\n",
      "119 121\n",
      "255 257\n",
      "479 481\n",
      "257 259\n",
      "476 478\n",
      "323 325\n",
      "209 211\n",
      "97 99\n",
      "488 490\n",
      "355 357\n",
      "400 402\n",
      "367 369\n",
      "443 445\n",
      "134 136\n",
      "422 424\n",
      "147 149\n",
      "227 229\n",
      "209 211\n",
      "446 448\n",
      "364 366\n",
      "270 272\n",
      "422 424\n",
      "291 293\n",
      "317 319\n",
      "275 277\n",
      "354 356\n",
      "247 249\n",
      "300 302\n",
      "93 95\n",
      "411 413\n",
      "190 192\n",
      "129 131\n",
      "81 83\n",
      "423 425\n",
      "388 390\n",
      "170 172\n",
      "245 247\n",
      "217 219\n",
      "356 358\n",
      "403 405\n",
      "216 218\n",
      "484 486\n",
      "134 136\n",
      "373 375\n",
      "415 417\n",
      "213 215\n",
      "247 249\n",
      "147 149\n",
      "175 177\n",
      "196 198\n",
      "219 221\n",
      "291 293\n",
      "391 393\n",
      "179 181\n",
      "192 194\n",
      "115 117\n",
      "83 85\n",
      "134 136\n",
      "204 206\n",
      "376 378\n",
      "154 156\n",
      "108 110\n",
      "101 103\n",
      "300 302\n",
      "177 179\n",
      "280 282\n",
      "190 192\n",
      "482 484\n",
      "381 383\n",
      "493 495\n",
      "445 447\n",
      "148 150\n",
      "450 452\n",
      "365 367\n",
      "203 205\n",
      "465 467\n",
      "311 313\n",
      "211 213\n",
      "337 339\n",
      "339 341\n",
      "151 153\n",
      "491 493\n",
      "180 182\n",
      "349 351\n",
      "292 294\n",
      "183 185\n",
      "322 324\n",
      "452 454\n",
      "89 91\n",
      "233 235\n",
      "218 220\n",
      "98 100\n",
      "372 374\n",
      "218 220\n",
      "379 381\n",
      "218 220\n",
      "411 413\n",
      "406 408\n",
      "112 114\n",
      "166 168\n",
      "212 214\n",
      "498 500\n",
      "481 483\n",
      "375 377\n",
      "280 282\n",
      "260 262\n",
      "241 243\n",
      "331 333\n",
      "352 354\n",
      "189 191\n",
      "412 414\n",
      "141 143\n",
      "430 432\n",
      "348 350\n",
      "492 494\n",
      "128 130\n",
      "137 139\n",
      "426 428\n",
      "391 393\n",
      "166 168\n",
      "359 361\n",
      "114 116\n",
      "483 485\n",
      "371 373\n",
      "254 256\n",
      "301 303\n",
      "349 351\n",
      "201 203\n",
      "346 348\n",
      "249 251\n",
      "175 177\n",
      "180 182\n",
      "452 454\n",
      "453 455\n",
      "215 217\n",
      "323 325\n",
      "175 177\n",
      "256 258\n",
      "117 119\n",
      "323 325\n",
      "472 474\n",
      "361 363\n",
      "453 455\n",
      "315 317\n",
      "399 401\n",
      "191 193\n",
      "375 377\n",
      "406 408\n",
      "309 311\n",
      "125 127\n",
      "494 496\n",
      "116 118\n",
      "211 213\n",
      "493 495\n",
      "213 215\n",
      "252 254\n",
      "400 402\n",
      "127 129\n",
      "168 170\n",
      "208 210\n",
      "145 147\n",
      "490 492\n",
      "296 298\n",
      "362 364\n",
      "350 352\n",
      "229 231\n",
      "292 294\n",
      "365 367\n",
      "491 493\n",
      "177 179\n",
      "235 237\n",
      "259 261\n",
      "88 90\n",
      "240 242\n",
      "169 171\n",
      "155 157\n",
      "91 93\n",
      "220 222\n",
      "398 400\n",
      "285 287\n",
      "482 484\n",
      "470 472\n",
      "118 120\n",
      "92 94\n",
      "261 263\n",
      "229 231\n",
      "172 174\n",
      "243 245\n",
      "176 178\n",
      "115 117\n",
      "292 294\n",
      "412 414\n",
      "333 335\n",
      "496 498\n",
      "208 210\n",
      "143 145\n",
      "329 331\n",
      "261 263\n",
      "361 363\n",
      "453 455\n",
      "140 142\n",
      "465 467\n",
      "391 393\n",
      "188 190\n",
      "147 149\n",
      "311 313\n",
      "246 248\n",
      "205 207\n",
      "217 219\n",
      "461 463\n",
      "291 293\n",
      "281 283\n",
      "343 345\n",
      "249 251\n",
      "384 386\n",
      "300 302\n",
      "204 206\n",
      "375 377\n",
      "291 293\n",
      "193 195\n",
      "118 120\n",
      "349 351\n",
      "240 242\n",
      "101 103\n",
      "334 336\n",
      "327 329\n",
      "220 222\n",
      "406 408\n",
      "258 260\n",
      "475 477\n",
      "495 497\n",
      "266 268\n",
      "246 248\n",
      "401 403\n",
      "237 239\n",
      "432 434\n",
      "195 197\n",
      "298 300\n",
      "297 299\n",
      "367 369\n",
      "212 214\n",
      "202 204\n",
      "343 345\n",
      "335 337\n",
      "222 224\n",
      "114 116\n",
      "261 263\n",
      "215 217\n",
      "326 328\n",
      "242 244\n",
      "228 230\n",
      "196 198\n",
      "236 238\n",
      "487 489\n",
      "487 489\n",
      "277 279\n",
      "394 396\n",
      "160 162\n",
      "394 396\n",
      "400 402\n",
      "363 365\n",
      "400 402\n",
      "223 225\n",
      "331 333\n",
      "188 190\n",
      "188 190\n",
      "91 93\n",
      "127 129\n",
      "331 333\n",
      "412 414\n",
      "494 496\n",
      "331 333\n",
      "392 394\n",
      "459 461\n",
      "263 265\n",
      "400 402\n",
      "281 283\n",
      "241 243\n",
      "438 440\n",
      "139 141\n",
      "454 456\n",
      "198 200\n",
      "341 343\n",
      "410 412\n",
      "364 366\n",
      "279 281\n",
      "423 425\n",
      "206 208\n",
      "210 212\n",
      "161 163\n",
      "245 247\n",
      "261 263\n",
      "365 367\n",
      "374 376\n",
      "248 250\n",
      "456 458\n",
      "289 291\n",
      "314 316\n",
      "310 312\n",
      "265 267\n",
      "237 239\n",
      "171 173\n",
      "420 422\n",
      "397 399\n",
      "192 194\n",
      "351 353\n",
      "289 291\n",
      "232 234\n",
      "134 136\n",
      "376 378\n",
      "217 219\n",
      "364 366\n",
      "418 420\n",
      "199 201\n",
      "445 447\n",
      "147 149\n",
      "360 362\n",
      "140 142\n",
      "420 422\n",
      "163 165\n",
      "499 501\n",
      "152 154\n",
      "244 246\n",
      "471 473\n",
      "130 132\n",
      "129 131\n",
      "328 330\n",
      "418 420\n",
      "162 164\n",
      "229 231\n",
      "340 342\n",
      "384 386\n",
      "206 208\n",
      "270 272\n",
      "227 229\n",
      "143 145\n",
      "458 460\n",
      "231 233\n",
      "160 162\n",
      "148 150\n",
      "493 495\n",
      "327 329\n",
      "381 383\n",
      "165 167\n",
      "417 419\n",
      "395 397\n",
      "460 462\n",
      "256 258\n",
      "396 398\n",
      "138 140\n",
      "140 142\n",
      "316 318\n",
      "128 130\n",
      "475 477\n",
      "348 350\n",
      "94 96\n",
      "423 425\n",
      "174 176\n",
      "135 137\n",
      "372 374\n",
      "313 315\n",
      "253 255\n",
      "409 411\n",
      "483 485\n",
      "268 270\n",
      "165 167\n",
      "116 118\n",
      "424 426\n",
      "290 292\n",
      "236 238\n",
      "234 236\n",
      "282 284\n",
      "348 350\n",
      "176 178\n",
      "325 327\n",
      "264 266\n",
      "353 355\n",
      "348 350\n",
      "407 409\n",
      "207 209\n",
      "176 178\n",
      "383 385\n",
      "225 227\n",
      "270 272\n",
      "403 405\n",
      "91 93\n",
      "132 134\n",
      "179 181\n",
      "271 273\n",
      "196 198\n",
      "350 352\n",
      "282 284\n",
      "92 94\n",
      "472 474\n",
      "424 426\n",
      "238 240\n",
      "197 199\n",
      "189 191\n",
      "432 434\n",
      "291 293\n",
      "381 383\n",
      "97 99\n",
      "304 306\n",
      "200 202\n",
      "303 305\n",
      "350 352\n",
      "335 337\n",
      "239 241\n",
      "318 320\n",
      "258 260\n",
      "207 209\n",
      "254 256\n",
      "227 229\n",
      "256 258\n",
      "350 352\n",
      "398 400\n",
      "99 101\n",
      "343 345\n",
      "184 186\n",
      "260 262\n",
      "198 200\n",
      "203 205\n",
      "484 486\n",
      "182 184\n",
      "329 331\n",
      "440 442\n",
      "313 315\n",
      "397 399\n",
      "344 346\n",
      "293 295\n",
      "375 377\n",
      "248 250\n",
      "85 87\n",
      "203 205\n",
      "322 324\n",
      "346 348\n",
      "410 412\n",
      "396 398\n",
      "419 421\n",
      "322 324\n",
      "134 136\n",
      "139 141\n",
      "145 147\n",
      "396 398\n",
      "400 402\n",
      "123 125\n",
      "86 88\n",
      "383 385\n",
      "444 446\n",
      "421 423\n",
      "423 425\n",
      "414 416\n",
      "325 327\n",
      "345 347\n",
      "352 354\n",
      "261 263\n",
      "380 382\n",
      "221 223\n",
      "322 324\n",
      "180 182\n",
      "403 405\n",
      "330 332\n",
      "163 165\n",
      "457 459\n",
      "282 284\n",
      "135 137\n",
      "180 182\n",
      "427 429\n",
      "248 250\n",
      "381 383\n",
      "456 458\n",
      "275 277\n",
      "299 301\n",
      "188 190\n",
      "142 144\n",
      "149 151\n",
      "142 144\n",
      "91 93\n",
      "332 334\n",
      "447 449\n",
      "261 263\n",
      "415 417\n",
      "102 104\n",
      "474 476\n",
      "285 287\n",
      "230 232\n",
      "444 446\n",
      "386 388\n",
      "361 363\n",
      "393 395\n",
      "268 270\n",
      "407 409\n",
      "439 441\n",
      "324 326\n",
      "222 224\n",
      "303 305\n",
      "135 137\n",
      "173 175\n",
      "254 256\n",
      "128 130\n",
      "466 468\n",
      "323 325\n",
      "325 327\n",
      "332 334\n",
      "147 149\n",
      "312 314\n",
      "321 323\n",
      "377 379\n",
      "294 296\n",
      "414 416\n",
      "309 311\n",
      "243 245\n",
      "404 406\n",
      "330 332\n",
      "477 479\n",
      "473 475\n",
      "210 212\n",
      "496 498\n",
      "465 467\n",
      "157 159\n",
      "101 103\n",
      "431 433\n",
      "188 190\n",
      "426 428\n",
      "293 295\n",
      "167 169\n",
      "196 198\n",
      "486 488\n",
      "147 149\n",
      "391 393\n",
      "304 306\n",
      "361 363\n",
      "302 304\n",
      "259 261\n",
      "313 315\n",
      "360 362\n",
      "328 330\n",
      "206 208\n",
      "334 336\n",
      "366 368\n",
      "371 373\n",
      "213 215\n",
      "88 90\n",
      "333 335\n",
      "464 466\n",
      "495 497\n",
      "108 110\n",
      "408 410\n",
      "414 416\n",
      "180 182\n",
      "288 290\n",
      "426 428\n",
      "456 458\n",
      "393 395\n",
      "275 277\n",
      "426 428\n",
      "112 114\n",
      "180 182\n",
      "479 481\n",
      "148 150\n",
      "316 318\n",
      "390 392\n",
      "255 257\n",
      "459 461\n",
      "410 412\n",
      "496 498\n",
      "376 378\n",
      "478 480\n",
      "442 444\n",
      "194 196\n",
      "495 497\n",
      "384 386\n",
      "350 352\n",
      "384 386\n",
      "359 361\n",
      "454 456\n",
      "410 412\n",
      "272 274\n",
      "213 215\n",
      "349 351\n",
      "122 124\n",
      "322 324\n",
      "463 465\n",
      "285 287\n",
      "394 396\n",
      "161 163\n",
      "234 236\n",
      "156 158\n",
      "229 231\n",
      "232 234\n",
      "400 402\n",
      "398 400\n",
      "181 183\n",
      "303 305\n",
      "130 132\n",
      "384 386\n",
      "447 449\n",
      "176 178\n",
      "313 315\n",
      "286 288\n",
      "144 146\n",
      "235 237\n",
      "231 233\n",
      "270 272\n",
      "98 100\n",
      "200 202\n",
      "200 202\n",
      "243 245\n",
      "296 298\n",
      "213 215\n",
      "166 168\n",
      "216 218\n",
      "244 246\n",
      "471 473\n",
      "350 352\n",
      "193 195\n",
      "347 349\n",
      "384 386\n",
      "404 406\n",
      "471 473\n",
      "129 131\n",
      "342 344\n",
      "299 301\n",
      "237 239\n",
      "479 481\n",
      "439 441\n",
      "110 112\n",
      "335 337\n",
      "320 322\n",
      "423 425\n",
      "435 437\n",
      "273 275\n",
      "104 106\n",
      "398 400\n",
      "353 355\n",
      "355 357\n",
      "472 474\n",
      "91 93\n",
      "134 136\n",
      "154 156\n",
      "126 128\n",
      "367 369\n",
      "322 324\n",
      "86 88\n",
      "273 275\n",
      "261 263\n",
      "164 166\n",
      "336 338\n",
      "251 253\n",
      "273 275\n",
      "162 164\n",
      "274 276\n",
      "493 495\n",
      "435 437\n",
      "419 421\n",
      "415 417\n",
      "310 312\n",
      "146 148\n",
      "424 426\n",
      "251 253\n",
      "364 366\n",
      "116 118\n",
      "298 300\n",
      "462 464\n",
      "445 447\n",
      "115 117\n",
      "214 216\n",
      "238 240\n",
      "180 182\n",
      "260 262\n",
      "461 463\n",
      "309 311\n",
      "372 374\n",
      "332 334\n",
      "317 319\n",
      "85 87\n",
      "480 482\n",
      "326 328\n",
      "246 248\n",
      "478 480\n",
      "301 303\n",
      "292 294\n",
      "484 486\n",
      "136 138\n",
      "142 144\n",
      "178 180\n",
      "347 349\n",
      "427 429\n",
      "350 352\n",
      "222 224\n",
      "212 214\n",
      "181 183\n",
      "199 201\n",
      "253 255\n",
      "312 314\n",
      "184 186\n",
      "440 442\n",
      "406 408\n",
      "451 453\n",
      "219 221\n",
      "291 293\n",
      "304 306\n",
      "473 475\n",
      "475 477\n",
      "112 114\n",
      "430 432\n",
      "339 341\n",
      "441 443\n",
      "349 351\n",
      "480 482\n",
      "251 253\n",
      "467 469\n",
      "480 482\n",
      "127 129\n",
      "478 480\n",
      "253 255\n",
      "217 219\n",
      "436 438\n",
      "132 134\n",
      "365 367\n",
      "245 247\n",
      "212 214\n",
      "204 206\n",
      "363 365\n",
      "449 451\n",
      "115 117\n",
      "163 165\n",
      "462 464\n",
      "158 160\n",
      "222 224\n",
      "378 380\n",
      "161 163\n",
      "190 192\n",
      "221 223\n",
      "95 97\n",
      "127 129\n",
      "282 284\n",
      "314 316\n",
      "138 140\n",
      "218 220\n",
      "403 405\n",
      "392 394\n",
      "101 103\n",
      "478 480\n",
      "399 401\n",
      "358 360\n",
      "442 444\n",
      "431 433\n",
      "134 136\n",
      "347 349\n",
      "415 417\n",
      "451 453\n",
      "271 273\n",
      "123 125\n",
      "255 257\n",
      "367 369\n",
      "403 405\n",
      "324 326\n",
      "335 337\n",
      "99 101\n",
      "147 149\n",
      "116 118\n",
      "466 468\n",
      "118 120\n",
      "264 266\n",
      "282 284\n",
      "183 185\n",
      "201 203\n",
      "457 459\n",
      "301 303\n",
      "354 356\n",
      "263 265\n",
      "373 375\n",
      "372 374\n",
      "402 404\n",
      "250 252\n",
      "194 196\n",
      "467 469\n",
      "220 222\n",
      "267 269\n",
      "329 331\n",
      "289 291\n",
      "386 388\n",
      "167 169\n",
      "299 301\n",
      "332 334\n",
      "403 405\n",
      "184 186\n",
      "466 468\n",
      "484 486\n",
      "420 422\n",
      "151 153\n",
      "352 354\n",
      "270 272\n",
      "335 337\n",
      "339 341\n",
      "334 336\n",
      "435 437\n",
      "453 455\n",
      "374 376\n",
      "115 117\n",
      "216 218\n",
      "217 219\n",
      "489 491\n",
      "108 110\n",
      "328 330\n",
      "157 159\n",
      "491 493\n",
      "462 464\n",
      "483 485\n",
      "477 479\n",
      "227 229\n",
      "229 231\n",
      "384 386\n",
      "246 248\n",
      "334 336\n",
      "358 360\n",
      "335 337\n",
      "348 350\n",
      "480 482\n",
      "365 367\n",
      "464 466\n",
      "125 127\n",
      "458 460\n",
      "186 188\n",
      "258 260\n",
      "352 354\n",
      "391 393\n",
      "265 267\n",
      "363 365\n",
      "115 117\n",
      "127 129\n",
      "160 162\n",
      "312 314\n",
      "176 178\n",
      "476 478\n",
      "428 430\n",
      "170 172\n",
      "183 185\n",
      "403 405\n",
      "246 248\n",
      "408 410\n",
      "466 468\n",
      "469 471\n",
      "142 144\n",
      "293 295\n",
      "399 401\n",
      "159 161\n",
      "435 437\n",
      "427 429\n",
      "393 395\n",
      "425 427\n",
      "370 372\n",
      "199 201\n",
      "384 386\n",
      "292 294\n",
      "204 206\n",
      "430 432\n",
      "253 255\n",
      "321 323\n",
      "448 450\n",
      "457 459\n",
      "416 418\n",
      "340 342\n",
      "359 361\n",
      "301 303\n",
      "135 137\n",
      "257 259\n",
      "363 365\n",
      "375 377\n",
      "420 422\n",
      "212 214\n",
      "170 172\n",
      "425 427\n",
      "94 96\n",
      "362 364\n",
      "289 291\n",
      "451 453\n",
      "263 265\n",
      "252 254\n",
      "214 216\n",
      "150 152\n",
      "117 119\n",
      "191 193\n",
      "370 372\n",
      "335 337\n",
      "431 433\n",
      "443 445\n",
      "390 392\n",
      "112 114\n",
      "109 111\n",
      "334 336\n",
      "313 315\n",
      "491 493\n",
      "313 315\n",
      "472 474\n",
      "193 195\n",
      "467 469\n",
      "309 311\n",
      "254 256\n",
      "358 360\n",
      "191 193\n",
      "497 499\n",
      "294 296\n",
      "487 489\n",
      "436 438\n",
      "346 348\n",
      "361 363\n",
      "194 196\n",
      "162 164\n",
      "196 198\n",
      "465 467\n",
      "302 304\n",
      "393 395\n",
      "145 147\n",
      "277 279\n",
      "169 171\n",
      "297 299\n",
      "194 196\n",
      "442 444\n",
      "265 267\n",
      "199 201\n",
      "342 344\n",
      "99 101\n",
      "282 284\n",
      "308 310\n",
      "120 122\n",
      "271 273\n",
      "165 167\n",
      "179 181\n",
      "471 473\n",
      "425 427\n",
      "360 362\n",
      "184 186\n",
      "425 427\n",
      "443 445\n",
      "400 402\n",
      "216 218\n",
      "112 114\n",
      "360 362\n",
      "377 379\n",
      "233 235\n",
      "299 301\n",
      "201 203\n",
      "112 114\n",
      "411 413\n",
      "395 397\n",
      "330 332\n",
      "147 149\n",
      "435 437\n",
      "450 452\n",
      "318 320\n",
      "208 210\n",
      "463 465\n",
      "218 220\n",
      "322 324\n",
      "485 487\n",
      "183 185\n",
      "218 220\n",
      "406 408\n",
      "187 189\n",
      "479 481\n",
      "326 328\n",
      "500 502\n",
      "215 217\n",
      "85 87\n",
      "448 450\n",
      "298 300\n",
      "331 333\n",
      "313 315\n",
      "246 248\n",
      "404 406\n",
      "359 361\n",
      "219 221\n",
      "299 301\n",
      "464 466\n",
      "318 320\n",
      "124 126\n",
      "119 121\n",
      "161 163\n",
      "498 500\n",
      "440 442\n",
      "216 218\n",
      "389 391\n",
      "200 202\n",
      "440 442\n",
      "98 100\n",
      "160 162\n",
      "416 418\n",
      "398 400\n",
      "266 268\n",
      "122 124\n",
      "393 395\n",
      "87 89\n",
      "189 191\n",
      "201 203\n",
      "404 406\n",
      "262 264\n",
      "197 199\n",
      "137 139\n",
      "482 484\n",
      "185 187\n",
      "329 331\n",
      "198 200\n",
      "238 240\n",
      "306 308\n",
      "179 181\n",
      "338 340\n",
      "90 92\n",
      "307 309\n",
      "121 123\n",
      "320 322\n",
      "458 460\n",
      "397 399\n",
      "334 336\n",
      "162 164\n",
      "460 462\n",
      "103 105\n",
      "472 474\n",
      "324 326\n",
      "497 499\n",
      "429 431\n",
      "283 285\n",
      "110 112\n",
      "434 436\n",
      "348 350\n",
      "180 182\n",
      "296 298\n",
      "414 416\n",
      "181 183\n",
      "452 454\n",
      "499 501\n",
      "422 424\n",
      "160 162\n",
      "261 263\n",
      "152 154\n",
      "418 420\n",
      "243 245\n",
      "380 382\n",
      "352 354\n",
      "352 354\n",
      "364 366\n",
      "323 325\n",
      "396 398\n",
      "265 267\n",
      "288 290\n",
      "117 119\n",
      "415 417\n",
      "415 417\n",
      "199 201\n",
      "492 494\n",
      "400 402\n",
      "388 390\n",
      "205 207\n",
      "384 386\n",
      "209 211\n",
      "406 408\n",
      "457 459\n",
      "445 447\n",
      "389 391\n",
      "395 397\n",
      "460 462\n",
      "268 270\n",
      "222 224\n",
      "359 361\n",
      "209 211\n",
      "208 210\n",
      "245 247\n",
      "425 427\n",
      "358 360\n",
      "193 195\n",
      "301 303\n",
      "373 375\n",
      "194 196\n",
      "411 413\n",
      "119 121\n",
      "144 146\n",
      "420 422\n",
      "113 115\n",
      "260 262\n",
      "437 439\n",
      "425 427\n",
      "179 181\n",
      "135 137\n",
      "176 178\n",
      "308 310\n",
      "351 353\n",
      "390 392\n",
      "370 372\n",
      "337 339\n",
      "332 334\n",
      "424 426\n",
      "248 250\n",
      "394 396\n",
      "497 499\n",
      "173 175\n",
      "172 174\n",
      "370 372\n",
      "381 383\n",
      "186 188\n",
      "492 494\n",
      "145 147\n",
      "133 135\n",
      "308 310\n",
      "373 375\n",
      "371 373\n",
      "165 167\n",
      "419 421\n",
      "206 208\n",
      "260 262\n",
      "481 483\n",
      "112 114\n",
      "299 301\n",
      "257 259\n",
      "205 207\n",
      "485 487\n",
      "470 472\n",
      "395 397\n",
      "488 490\n",
      "97 99\n",
      "492 494\n",
      "300 302\n",
      "360 362\n",
      "359 361\n",
      "485 487\n",
      "166 168\n",
      "366 368\n",
      "115 117\n",
      "87 89\n",
      "304 306\n",
      "488 490\n",
      "469 471\n",
      "359 361\n",
      "411 413\n",
      "455 457\n",
      "128 130\n",
      "232 234\n",
      "367 369\n",
      "480 482\n",
      "437 439\n",
      "317 319\n",
      "140 142\n",
      "215 217\n",
      "185 187\n",
      "123 125\n",
      "118 120\n",
      "123 125\n",
      "125 127\n",
      "99 101\n",
      "214 216\n",
      "400 402\n",
      "299 301\n",
      "451 453\n",
      "136 138\n",
      "116 118\n",
      "259 261\n",
      "359 361\n",
      "148 150\n",
      "217 219\n",
      "330 332\n",
      "429 431\n",
      "400 402\n",
      "337 339\n",
      "333 335\n",
      "183 185\n",
      "305 307\n",
      "205 207\n",
      "119 121\n",
      "194 196\n",
      "142 144\n",
      "310 312\n",
      "103 105\n",
      "426 428\n",
      "147 149\n",
      "225 227\n",
      "401 403\n",
      "479 481\n",
      "382 384\n",
      "458 460\n",
      "100 102\n",
      "198 200\n",
      "227 229\n",
      "101 103\n",
      "282 284\n",
      "325 327\n",
      "94 96\n",
      "190 192\n",
      "372 374\n",
      "250 252\n",
      "259 261\n",
      "161 163\n",
      "386 388\n",
      "396 398\n",
      "201 203\n",
      "323 325\n",
      "177 179\n",
      "344 346\n",
      "417 419\n",
      "285 287\n",
      "198 200\n",
      "375 377\n",
      "213 215\n",
      "272 274\n",
      "278 280\n",
      "431 433\n",
      "303 305\n",
      "265 267\n",
      "450 452\n",
      "263 265\n",
      "228 230\n",
      "377 379\n",
      "388 390\n",
      "325 327\n",
      "123 125\n",
      "228 230\n",
      "200 202\n",
      "427 429\n",
      "106 108\n",
      "177 179\n",
      "377 379\n",
      "406 408\n",
      "93 95\n",
      "351 353\n",
      "384 386\n",
      "273 275\n",
      "432 434\n",
      "249 251\n",
      "136 138\n",
      "443 445\n",
      "217 219\n",
      "427 429\n",
      "385 387\n",
      "267 269\n",
      "194 196\n",
      "314 316\n",
      "105 107\n",
      "361 363\n",
      "473 475\n",
      "334 336\n",
      "360 362\n",
      "257 259\n",
      "183 185\n",
      "429 431\n",
      "355 357\n",
      "309 311\n",
      "422 424\n",
      "261 263\n",
      "348 350\n",
      "202 204\n",
      "174 176\n",
      "303 305\n",
      "272 274\n",
      "118 120\n",
      "379 381\n",
      "317 319\n",
      "437 439\n",
      "95 97\n",
      "352 354\n",
      "242 244\n",
      "468 470\n",
      "404 406\n",
      "419 421\n",
      "369 371\n",
      "147 149\n",
      "247 249\n",
      "474 476\n",
      "374 376\n",
      "255 257\n",
      "217 219\n",
      "240 242\n",
      "390 392\n",
      "121 123\n",
      "158 160\n",
      "178 180\n",
      "444 446\n",
      "308 310\n",
      "276 278\n",
      "208 210\n",
      "218 220\n",
      "353 355\n",
      "101 103\n",
      "236 238\n",
      "215 217\n",
      "407 409\n",
      "383 385\n",
      "407 409\n",
      "477 479\n",
      "369 371\n",
      "340 342\n",
      "204 206\n",
      "158 160\n",
      "293 295\n",
      "233 235\n",
      "242 244\n",
      "111 113\n",
      "147 149\n",
      "176 178\n",
      "212 214\n",
      "320 322\n",
      "142 144\n",
      "369 371\n",
      "255 257\n",
      "260 262\n",
      "419 421\n",
      "371 373\n",
      "221 223\n",
      "414 416\n",
      "376 378\n",
      "147 149\n",
      "317 319\n",
      "260 262\n",
      "227 229\n",
      "344 346\n",
      "210 212\n",
      "414 416\n",
      "406 408\n",
      "142 144\n",
      "323 325\n",
      "256 258\n",
      "293 295\n",
      "347 349\n",
      "149 151\n",
      "175 177\n",
      "465 467\n",
      "461 463\n",
      "120 122\n",
      "290 292\n",
      "164 166\n",
      "220 222\n",
      "300 302\n",
      "284 286\n",
      "297 299\n",
      "97 99\n",
      "477 479\n",
      "393 395\n",
      "394 396\n",
      "449 451\n",
      "339 341\n",
      "243 245\n",
      "382 384\n",
      "280 282\n",
      "385 387\n",
      "423 425\n",
      "269 271\n",
      "319 321\n",
      "209 211\n",
      "108 110\n",
      "478 480\n",
      "328 330\n",
      "165 167\n",
      "207 209\n",
      "427 429\n",
      "474 476\n",
      "288 290\n",
      "346 348\n",
      "123 125\n",
      "299 301\n",
      "195 197\n",
      "238 240\n",
      "469 471\n",
      "155 157\n",
      "411 413\n",
      "191 193\n",
      "395 397\n",
      "474 476\n",
      "227 229\n",
      "481 483\n",
      "362 364\n",
      "364 366\n",
      "437 439\n",
      "126 128\n",
      "224 226\n",
      "381 383\n",
      "396 398\n",
      "296 298\n",
      "252 254\n",
      "267 269\n",
      "307 309\n",
      "459 461\n",
      "362 364\n",
      "197 199\n",
      "491 493\n",
      "345 347\n",
      "197 199\n",
      "310 312\n",
      "469 471\n",
      "462 464\n",
      "403 405\n",
      "263 265\n",
      "364 366\n",
      "294 296\n",
      "296 298\n",
      "418 420\n",
      "92 94\n",
      "395 397\n",
      "219 221\n",
      "281 283\n",
      "439 441\n",
      "468 470\n",
      "346 348\n",
      "305 307\n",
      "198 200\n",
      "298 300\n",
      "407 409\n",
      "240 242\n",
      "188 190\n",
      "130 132\n",
      "433 435\n",
      "207 209\n",
      "399 401\n",
      "359 361\n",
      "313 315\n",
      "279 281\n",
      "234 236\n",
      "197 199\n",
      "103 105\n",
      "317 319\n",
      "249 251\n",
      "432 434\n",
      "423 425\n",
      "156 158\n",
      "282 284\n",
      "208 210\n",
      "442 444\n",
      "323 325\n",
      "189 191\n",
      "337 339\n",
      "352 354\n",
      "330 332\n",
      "99 101\n",
      "476 478\n",
      "108 110\n",
      "449 451\n",
      "331 333\n",
      "266 268\n",
      "292 294\n",
      "439 441\n",
      "189 191\n",
      "406 408\n",
      "179 181\n",
      "429 431\n",
      "108 110\n",
      "355 357\n",
      "199 201\n",
      "281 283\n",
      "199 201\n",
      "238 240\n",
      "114 116\n",
      "340 342\n",
      "359 361\n",
      "295 297\n",
      "474 476\n",
      "126 128\n",
      "236 238\n",
      "160 162\n",
      "436 438\n",
      "204 206\n",
      "211 213\n",
      "304 306\n",
      "341 343\n",
      "417 419\n",
      "108 110\n",
      "367 369\n",
      "107 109\n",
      "433 435\n",
      "257 259\n",
      "270 272\n",
      "316 318\n",
      "419 421\n",
      "113 115\n",
      "223 225\n",
      "282 284\n",
      "326 328\n",
      "448 450\n",
      "166 168\n",
      "342 344\n",
      "173 175\n",
      "167 169\n",
      "345 347\n",
      "167 169\n",
      "247 249\n",
      "422 424\n",
      "130 132\n",
      "417 419\n",
      "473 475\n",
      "423 425\n",
      "314 316\n",
      "147 149\n",
      "301 303\n",
      "479 481\n",
      "228 230\n",
      "329 331\n",
      "115 117\n",
      "153 155\n",
      "415 417\n",
      "392 394\n",
      "323 325\n",
      "117 119\n",
      "219 221\n",
      "183 185\n",
      "142 144\n",
      "175 177\n",
      "297 299\n",
      "342 344\n",
      "477 479\n",
      "395 397\n",
      "110 112\n",
      "219 221\n",
      "463 465\n",
      "153 155\n",
      "99 101\n",
      "496 498\n",
      "499 501\n",
      "109 111\n",
      "410 412\n",
      "310 312\n",
      "415 417\n",
      "145 147\n",
      "148 150\n",
      "409 411\n",
      "227 229\n",
      "232 234\n",
      "411 413\n",
      "386 388\n",
      "242 244\n",
      "492 494\n",
      "412 414\n",
      "91 93\n",
      "135 137\n",
      "326 328\n",
      "226 228\n",
      "315 317\n",
      "303 305\n",
      "322 324\n",
      "278 280\n",
      "435 437\n",
      "476 478\n",
      "281 283\n",
      "365 367\n",
      "168 170\n",
      "269 271\n",
      "443 445\n",
      "239 241\n",
      "196 198\n",
      "142 144\n",
      "425 427\n",
      "392 394\n",
      "291 293\n",
      "256 258\n",
      "182 184\n",
      "457 459\n",
      "102 104\n",
      "150 152\n",
      "387 389\n",
      "211 213\n",
      "426 428\n",
      "490 492\n",
      "142 144\n",
      "409 411\n",
      "212 214\n",
      "462 464\n",
      "354 356\n",
      "283 285\n",
      "295 297\n",
      "259 261\n",
      "178 180\n",
      "266 268\n",
      "249 251\n",
      "493 495\n",
      "389 391\n",
      "118 120\n",
      "380 382\n",
      "128 130\n",
      "132 134\n",
      "351 353\n",
      "321 323\n",
      "493 495\n",
      "188 190\n",
      "284 286\n",
      "295 297\n",
      "351 353\n",
      "249 251\n",
      "118 120\n",
      "380 382\n",
      "341 343\n",
      "215 217\n",
      "309 311\n",
      "337 339\n",
      "498 500\n",
      "299 301\n",
      "236 238\n",
      "288 290\n",
      "211 213\n",
      "328 330\n",
      "132 134\n",
      "382 384\n",
      "82 84\n",
      "483 485\n",
      "286 288\n",
      "351 353\n",
      "355 357\n",
      "406 408\n",
      "301 303\n",
      "184 186\n",
      "336 338\n",
      "99 101\n",
      "262 264\n",
      "472 474\n",
      "206 208\n",
      "500 502\n",
      "486 488\n",
      "298 300\n",
      "261 263\n",
      "402 404\n",
      "496 498\n",
      "357 359\n",
      "335 337\n",
      "349 351\n",
      "354 356\n",
      "178 180\n",
      "327 329\n",
      "339 341\n",
      "244 246\n",
      "253 255\n",
      "359 361\n",
      "466 468\n",
      "250 252\n",
      "377 379\n",
      "234 236\n",
      "462 464\n",
      "103 105\n",
      "362 364\n",
      "327 329\n",
      "228 230\n",
      "364 366\n",
      "247 249\n",
      "193 195\n",
      "219 221\n",
      "242 244\n",
      "385 387\n",
      "202 204\n",
      "399 401\n",
      "426 428\n",
      "399 401\n",
      "375 377\n",
      "483 485\n",
      "500 502\n",
      "419 421\n",
      "228 230\n",
      "266 268\n",
      "386 388\n",
      "373 375\n",
      "273 275\n",
      "81 83\n",
      "225 227\n",
      "238 240\n",
      "201 203\n",
      "113 115\n",
      "80 82\n",
      "189 191\n",
      "118 120\n",
      "301 303\n",
      "175 177\n",
      "370 372\n",
      "305 307\n",
      "325 327\n",
      "376 378\n",
      "209 211\n",
      "355 357\n",
      "311 313\n",
      "430 432\n",
      "202 204\n",
      "375 377\n",
      "392 394\n",
      "189 191\n",
      "165 167\n",
      "216 218\n",
      "437 439\n",
      "479 481\n",
      "325 327\n",
      "199 201\n",
      "471 473\n",
      "106 108\n",
      "292 294\n",
      "451 453\n",
      "398 400\n",
      "201 203\n",
      "150 152\n",
      "499 501\n",
      "419 421\n",
      "295 297\n",
      "465 467\n",
      "251 253\n",
      "225 227\n",
      "116 118\n",
      "197 199\n",
      "152 154\n",
      "184 186\n",
      "172 174\n",
      "172 174\n",
      "338 340\n",
      "397 399\n",
      "403 405\n",
      "337 339\n",
      "488 490\n",
      "386 388\n",
      "174 176\n",
      "412 414\n",
      "249 251\n",
      "145 147\n",
      "346 348\n",
      "474 476\n",
      "195 197\n",
      "137 139\n",
      "176 178\n",
      "480 482\n",
      "265 267\n",
      "341 343\n",
      "166 168\n",
      "361 363\n",
      "394 396\n",
      "323 325\n",
      "476 478\n",
      "179 181\n",
      "204 206\n",
      "313 315\n",
      "360 362\n",
      "277 279\n",
      "233 235\n",
      "495 497\n",
      "299 301\n",
      "176 178\n",
      "265 267\n",
      "335 337\n",
      "303 305\n",
      "218 220\n",
      "123 125\n",
      "205 207\n",
      "419 421\n",
      "220 222\n",
      "145 147\n",
      "476 478\n",
      "295 297\n",
      "320 322\n",
      "381 383\n",
      "458 460\n",
      "99 101\n",
      "289 291\n",
      "94 96\n",
      "132 134\n",
      "145 147\n",
      "197 199\n",
      "333 335\n",
      "493 495\n",
      "241 243\n",
      "273 275\n",
      "476 478\n",
      "244 246\n",
      "239 241\n",
      "159 161\n",
      "127 129\n",
      "198 200\n",
      "330 332\n",
      "181 183\n",
      "192 194\n",
      "453 455\n",
      "167 169\n",
      "468 470\n",
      "498 500\n",
      "261 263\n",
      "180 182\n",
      "319 321\n",
      "183 185\n",
      "107 109\n",
      "280 282\n",
      "243 245\n",
      "271 273\n",
      "440 442\n",
      "395 397\n",
      "315 317\n",
      "186 188\n",
      "376 378\n",
      "494 496\n",
      "131 133\n",
      "346 348\n",
      "310 312\n",
      "204 206\n",
      "432 434\n",
      "108 110\n",
      "267 269\n",
      "407 409\n",
      "435 437\n",
      "223 225\n",
      "222 224\n",
      "151 153\n",
      "366 368\n",
      "313 315\n",
      "337 339\n",
      "409 411\n",
      "468 470\n",
      "499 501\n",
      "338 340\n",
      "348 350\n",
      "126 128\n",
      "185 187\n",
      "422 424\n",
      "313 315\n",
      "350 352\n",
      "350 352\n",
      "399 401\n",
      "461 463\n",
      "369 371\n",
      "406 408\n",
      "493 495\n",
      "281 283\n",
      "440 442\n",
      "412 414\n",
      "449 451\n",
      "170 172\n",
      "327 329\n",
      "477 479\n",
      "462 464\n",
      "426 428\n",
      "228 230\n",
      "364 366\n",
      "372 374\n",
      "214 216\n",
      "112 114\n",
      "161 163\n",
      "304 306\n",
      "95 97\n",
      "423 425\n",
      "140 142\n",
      "483 485\n",
      "117 119\n",
      "159 161\n",
      "323 325\n",
      "246 248\n",
      "495 497\n",
      "328 330\n",
      "180 182\n",
      "406 408\n",
      "282 284\n",
      "128 130\n",
      "371 373\n",
      "295 297\n",
      "486 488\n",
      "423 425\n",
      "432 434\n",
      "393 395\n",
      "80 82\n",
      "362 364\n",
      "84 86\n",
      "141 143\n",
      "184 186\n",
      "377 379\n",
      "212 214\n",
      "450 452\n",
      "126 128\n",
      "465 467\n",
      "127 129\n",
      "350 352\n",
      "251 253\n",
      "470 472\n",
      "489 491\n",
      "472 474\n",
      "301 303\n",
      "460 462\n",
      "113 115\n",
      "296 298\n",
      "421 423\n",
      "151 153\n",
      "431 433\n",
      "219 221\n",
      "262 264\n",
      "466 468\n",
      "358 360\n",
      "471 473\n",
      "106 108\n",
      "227 229\n",
      "312 314\n",
      "470 472\n",
      "452 454\n",
      "283 285\n",
      "451 453\n",
      "373 375\n",
      "159 161\n",
      "84 86\n",
      "259 261\n",
      "297 299\n",
      "365 367\n",
      "198 200\n",
      "192 194\n",
      "175 177\n",
      "380 382\n",
      "263 265\n",
      "292 294\n",
      "432 434\n",
      "490 492\n",
      "336 338\n",
      "351 353\n",
      "332 334\n",
      "237 239\n",
      "186 188\n",
      "221 223\n",
      "360 362\n",
      "380 382\n",
      "114 116\n",
      "457 459\n",
      "386 388\n",
      "243 245\n",
      "454 456\n",
      "401 403\n",
      "253 255\n",
      "362 364\n",
      "415 417\n",
      "456 458\n",
      "398 400\n",
      "258 260\n",
      "338 340\n",
      "258 260\n",
      "235 237\n",
      "146 148\n",
      "478 480\n",
      "318 320\n",
      "459 461\n",
      "249 251\n",
      "314 316\n",
      "382 384\n",
      "98 100\n",
      "265 267\n",
      "214 216\n",
      "114 116\n",
      "137 139\n",
      "450 452\n",
      "385 387\n",
      "212 214\n",
      "333 335\n",
      "125 127\n",
      "89 91\n",
      "374 376\n",
      "381 383\n",
      "445 447\n",
      "152 154\n",
      "351 353\n",
      "441 443\n",
      "308 310\n",
      "218 220\n",
      "461 463\n",
      "130 132\n",
      "242 244\n",
      "247 249\n",
      "351 353\n",
      "344 346\n",
      "387 389\n",
      "107 109\n",
      "193 195\n",
      "473 475\n",
      "185 187\n",
      "258 260\n",
      "277 279\n",
      "354 356\n",
      "339 341\n",
      "265 267\n",
      "359 361\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    print(len(train_dataset[i][\"labels\"]),len(train_dataset[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503fc00",
   "metadata": {},
   "source": [
    "This definitely seems harder than the first task, but we still attain a very respectable accuracy. Remember that to keep this demo lightweight, we used one of the smallest ESM models, focused on human proteins only and didn't put a lot of work into making sure we only included completely-annotated proteins in our training set. With a bigger model and a cleaner, broader training set, accuracy on this task could definitely go a lot higher!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
