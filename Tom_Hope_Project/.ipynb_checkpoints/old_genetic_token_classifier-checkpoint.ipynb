{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d70e49-4efb-494e-ab5e-e3fb7799b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 12:15:36.177868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-30 12:15:36.188772: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-30 12:15:36.192137: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 12:15:36.200888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 12:15:40.269001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets import Features\n",
    "from datasets import ClassLabel\n",
    "from datasets import Sequence\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from transformers import create_optimizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "# import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71f7322-657a-44f9-b09a-534a7ded87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e531a903505a49dabf799d31cacfeb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87f3da7-e593-4369-ae0d-3616c5776f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# sentence  = 'Hello World!'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "# # model     = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# # inputs    = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "# # model     = model.to(device)\n",
    "# # outputs   = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d33bc2-7f92-4e12-860a-2d95108b4b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ed9e8-2003-4536-be94-ca22ed2b0efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5cf111-5a5b-432c-b472-834e6ddc590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0:\"a\",\n",
    "    1:\"b\"\n",
    "}\n",
    "label2id ={\n",
    "    \"a\" : 0,\n",
    "    \"b\" : 1\n",
    "}\n",
    "\n",
    "labels = [0,1]\n",
    "label_list = [\"0\",\"1\"]\n",
    "SAMPLE_TEXT = \"ATATATATATATATATATATATATATATAT\"\n",
    "NAMES = [\"EXAMPLE\",\"TARGETSCAN\"]\n",
    "EXAMPLE_DATASET = 0\n",
    "TARGETSCAN_DATASET = 1\n",
    "CURRENT_DATASET = EXAMPLE_DATASET\n",
    "\n",
    "MODEL_NAMES = [\"zhihan1996/DNABERT-2-117M\",\"InstaDeepAI/nucleotide-transformer-500m-human-ref\"]\n",
    "DNABERT_MODEL = 0\n",
    "SEGMENT_MODEL = 1\n",
    "CURRENT_MODEL = DNABERT_MODEL\n",
    "MODEL_NAME = MODEL_NAMES[SEGMENT_MODEL]\n",
    "NUM_OF_EPOCH = 3\n",
    "SMALL_MODEL = False\n",
    "SAVE_MODEL_NAME = f\"model_{MODEL_NAMES}_dataset_{NAMES[CURRENT_DATASET]}_num_of_epoch_{NUM_OF_EPOCH}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3bbef9e-2a1c-4d60-8945-3970b8ab68a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model_['zhihan1996/DNABERT-2-117M', 'InstaDeepAI/nucleotide-transformer-2.5b-1000g']_dataset_EXAMPLE_num_of_epoch_3\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SMALL_MODEL:\n",
    "    SAVE_MODEL_NAME += \"_small\"\n",
    "SAVE_MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b683fba-71a4-4567-a80e-41f88b4df8cb",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3271e-a884-4a04-b175-e17efd33576b",
   "metadata": {},
   "source": [
    "## Example Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b38a01-5f68-4e61-9c6e-dd5a55061d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_DATASET = 300\n",
    "import random\n",
    "\n",
    "\n",
    "# example dataset\n",
    "MIN_NUM_OF_TOKENS = 30\n",
    "MAX_NUM_OF_TOKENS = 400\n",
    "MIN_SIZE_OF_TOKEN = 3\n",
    "MAX_SIZE_OF_TOKEN = 10\n",
    "REAL_TOKEN = \"GCGCGCGCGCGCGCGC\"\n",
    "FAKE_TOKEN = \"ATATATATATATATAT\"\n",
    "\n",
    "# Generate a random integer between 1 and 10 (inclusive)\n",
    "\n",
    "def create_dataset(number_of_elements : int, curr_index : int):\n",
    "    \n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    ids = []\n",
    "    for i in range(number_of_elements):\n",
    "        num_of_tokens = random.randint(MIN_NUM_OF_TOKENS, MAX_NUM_OF_TOKENS)\n",
    "        length_of_token = random.randint(MIN_SIZE_OF_TOKEN, MAX_SIZE_OF_TOKEN)\n",
    "        tokens.append([REAL_TOKEN[:length_of_token],FAKE_TOKEN[:length_of_token]])\n",
    "        ner_tags.append([1,0])\n",
    "        ids.append(curr_index)\n",
    "        curr_index+=1\n",
    "    ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"tokens\": tokens, \n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"id\":ids,\n",
    "    })\n",
    "    return ds\n",
    "    # pass\n",
    "\n",
    "def create_dataset_dict(number_of_elements : int):\n",
    "    train_ds = create_dataset(number_of_elements,0)\n",
    "    validation_ds = create_dataset(number_of_elements,number_of_elements)\n",
    "    test_ds = create_dataset(number_of_elements,number_of_elements*2)\n",
    "    \n",
    "    ds = DatasetDict({\"train\": train_ds, \"validation\" : validation_ds, \"test\":test_ds})\n",
    "    return ds\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72449a7-c32c-45e4-954b-9c80fef2fa24",
   "metadata": {},
   "source": [
    "## Targetscan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7f8d0a-414c-452a-ad55-81e3ee7986b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETSCAN_DATASET_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/targetscan_RNA_combined_nrows_90000_hg38.csv\"\n",
    "def create_targetscan_dataset(df,start,end):\n",
    "    tokens = df[\"result\"][start:end]\n",
    "    ner_tags = df[\"stags\"][start:end]\n",
    "    ids = df[\"Unnamed: 0\"][start:end]\n",
    "    ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"tokens\": tokens, \n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"id\":ids,\n",
    "    })\n",
    "    return ds\n",
    "\n",
    "def create_targetscan_dataset_dict(df):\n",
    "    train_ds = create_targetscan_dataset(df,2,len(df) // 2 + 10)\n",
    "    validation_ds = create_targetscan_dataset(df,len(df) // 2 + 11,(len(df) // 4) * 3)\n",
    "    test_ds = create_targetscan_dataset(df,(len(df) // 4) * 3,(len(df) // 4) * 4 - 2)\n",
    "    \n",
    "    ds = DatasetDict({\"train\": train_ds, \"validation\" : validation_ds, \"test\":test_ds})\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d022f75-3687-47e4-9ea7-47663988549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetscan_RNA_combined_nrows_90000.csv\n",
      "targetscan_RNA_combined_nrows_90000_hg38.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /sci/nosnap/michall/roeizucker/token_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e9dbd-3017-4686-a642-604146a08303",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece9ea55-a945-45ce-91dd-75b2857f6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CURRENT_DATASET == EXAMPLE_DATASET:\n",
    "    ds = create_dataset_dict(SIZE_OF_DATASET)\n",
    "elif CURRENT_DATASET == TARGETSCAN_DATASET:\n",
    "    df = pd.read_csv(TARGETSCAN_DATASET_PATH)\n",
    "    \n",
    "    SAMPLE_TEXT = df[\"result\"][100].replace(\"'\",\"\").replace(\" \",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    df[\"result\"] = df[\"result\"].str.replace(\"'\",\"\")\n",
    "    df[\"result\"] = df[\"result\"].apply(lambda x:x.strip('][').split(', '))\n",
    "    df[\"stags\"] = df[\"stags\"].apply(lambda x:x.strip('][').split(', '))\n",
    "    df['stags'] = df['stags'].apply(lambda x: list(map(int, x)))\n",
    "    df = df[df[\"result\"].map(len) < 20]\n",
    "    ds = create_targetscan_dataset_dict(df)\n",
    "else:\n",
    "    raise Exception(\"no such dataset\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9fc3c-fe5a-4718-8076-43c72a422d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e8d4277-b167-4f37-895a-727a86b54f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type esm to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['GCGC', 'ATAT'], 'ner_tags': [1, 0], 'id': 0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8aeede72a046af86145b568736d27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m tokenizer(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m], is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(tokenized_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 60\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m     65\u001b[0m seqeval \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseqeval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/datasets/dataset_dict.py:866\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 866\u001b[0m     \u001b[43m{\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    888\u001b[0m )\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/datasets/dataset_dict.py:867\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    866\u001b[0m     {\n\u001b[0;32m--> 867\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    887\u001b[0m     }\n\u001b[1;32m    888\u001b[0m )\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3037\u001b[0m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/datasets/arrow_dataset.py:3438\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3434\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3435\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3436\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3438\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(examples[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m----> 6\u001b[0m     word_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Map tokens to their respective word.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     previous_word_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     label_ids \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:384\u001b[0m, in \u001b[0;36mBatchEncoding.word_ids\u001b[0;34m(self, batch_index)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03mReturn a list mapping the tokens to their actual word in the initial sentence for a fast tokenizer.\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m    (several tokens will be mapped to the same word index if they are parts of that word).\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings:\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    385\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    387\u001b[0m     )\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encodings[batch_index]\u001b[38;5;241m.\u001b[39mword_ids\n",
      "\u001b[0;31mValueError\u001b[0m: word_ids() is not available when using non-fast tokenizers (e.g. instance of a `XxxTokenizerFast` class)."
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\",hidden_size=400, num_hidden_layers=4, num_attention_heads=4,intermediate_size=35,max_position_embeddings=10000)\n",
    "\n",
    "if SMALL_MODEL:\n",
    "    config = BertConfig.from_pretrained(MODEL_NAME,hidden_size=800, num_hidden_layers=8, num_attention_heads=8,intermediate_size=70,max_position_embeddings=10000)\n",
    "else:\n",
    "    config = BertConfig.from_pretrained(MODEL_NAME,max_position_embeddings=10000)\n",
    "# model = AutoModel.from_config(config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = TFAutoModelForTokenClassification.from_config(config)\n",
    "example = ds[\"train\"][0]\n",
    "print(example)\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "# example = ds[\"train\"][0]\n",
    "# label_list = ds[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "batch_size = 16\n",
    "num_train_epochs = NUM_OF_EPOCH\n",
    "num_train_steps = (len(tokenized_ds[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "model.compile(optimizer=optimizer)  # No loss argument!\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback]\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=NUM_OF_EPOCH, callbacks=callbacks)\n",
    "\n",
    "\n",
    "# change evaluation to something generic\n",
    "classifier = pipeline(\"ner\", model=model,tokenizer=tokenizer)\n",
    "print(\"text\",SAMPLE_TEXT,\"size\",SIZE_OF_DATASET)\n",
    "for val in classifier(SAMPLE_TEXT):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbaece9-76e3-4b35-a9cc-2a5b5def36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model.to)\n",
    "print(tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a3794-c56d-4f9b-a07e-d25b4b1d8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7bc46-a051-48c9-b319-016c934f3d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classifier = pipeline(\"ner\", model=model,tokenizer=tokenizer)\n",
    "# print(\"text\",SAMPLE_TEXT,\"size\",SIZE_OF_DATASET)\n",
    "for val in classifier(''.join(tokenized_ds[\"train\"][2][\"tokens\"])):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845834e3-2862-40f1-888c-13cad28ad8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dd50e-c451-4427-8c53-482ace2dc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTED_VAL = 101\n",
    "tokens = []\n",
    "result = classifier(''.join(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"]))\n",
    "for i in range(len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"])):\n",
    "    tokens.extend([str(tokenized_ds[\"train\"][TESTED_VAL][\"ner_tags\"][i])] * len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"][i]))\n",
    "print(\"code\\t\\t\",''.join(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"]))\n",
    "print(\"origin\\t\\t\",''.join(tokens))\n",
    "\n",
    "pred = []\n",
    "for val in result:\n",
    "    pred.extend([val[\"entity\"][-1]]*len(val[\"word\"]))\n",
    "\n",
    "print(\"predicted\\t\",''.join(pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"predicted\")\n",
    "for val in result:\n",
    "    if val[\"entity\"] == 'LABEL_1':\n",
    "        print(val)\n",
    "\n",
    "print(\"source\")\n",
    "count = 0\n",
    "for i in range(len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"])):\n",
    "    if tokenized_ds[\"train\"][TESTED_VAL][\"ner_tags\"][i] == 1:\n",
    "        print(count,\"-\",end=\"\")\n",
    "    count += len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"][i])\n",
    "    if tokenized_ds[\"train\"][TESTED_VAL][\"ner_tags\"][i] == 1:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f66df-730b-47e0-97c5-7fe77684ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_ds[\"train\"][0]\n",
    "''.join(tokenized_ds[\"train\"][2][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8792b-a3c9-48af-a6e9-9cc4d89a6108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_ds[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b043b-393b-4c20-b598-1d430683cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_pretrained(SAVE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b4c4c-7ee4-429e-a170-a3c5dd74d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"modeldir\")\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7123c19-8247-47a5-8ec5-4017ea5c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"ner\", model = SAVE_MODEL_NAME,device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84501c39-a9ef-4d29-af4a-32fe40059db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in pipe(''.join(tokenized_ds[\"train\"][2][\"tokens\"])):\n",
    "    print(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
