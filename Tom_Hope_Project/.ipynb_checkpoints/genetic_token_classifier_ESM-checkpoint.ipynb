{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d70e49-4efb-494e-ab5e-e3fb7799b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 17:52:52.993377: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-30 17:52:53.004334: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-30 17:52:53.007743: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-30 17:52:53.016493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 17:52:55.066918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets import Features\n",
    "from datasets import ClassLabel\n",
    "from datasets import Sequence\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "from transformers import AutoTokenizer, TFEsmForTokenClassification\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from transformers import create_optimizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "# import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71f7322-657a-44f9-b09a-534a7ded87fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d08dde7eaf7b4b56bfb4eb760d74ef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c87f3da7-e593-4369-ae0d-3616c5776f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertModel\n",
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# sentence  = 'Hello World!'\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "# # model     = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# # inputs    = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "# # model     = model.to(device)\n",
    "# # outputs   = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d33bc2-7f92-4e12-860a-2d95108b4b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ed9e8-2003-4536-be94-ca22ed2b0efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5cf111-5a5b-432c-b472-834e6ddc590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0:\"a\",\n",
    "    1:\"b\"\n",
    "}\n",
    "label2id ={\n",
    "    \"a\" : 0,\n",
    "    \"b\" : 1\n",
    "}\n",
    "\n",
    "labels = [0,1]\n",
    "label_list = [\"0\",\"1\"]\n",
    "SAMPLE_TEXT = \"ATATATATATATATATATATATATATATAT\"\n",
    "NAMES = [\"EXAMPLE\",\"TARGETSCAN\"]\n",
    "EXAMPLE_DATASET = 0\n",
    "TARGETSCAN_DATASET = 1\n",
    "CURRENT_DATASET = EXAMPLE_DATASET\n",
    "\n",
    "MODEL_NAMES = [\"zhihan1996/DNABERT-2-117M\",\"InstaDeepAI/nucleotide-transformer-500m-human-ref\"]\n",
    "DNABERT_MODEL = 0\n",
    "SEGMENT_MODEL = 1\n",
    "CURRENT_MODEL = DNABERT_MODEL\n",
    "MODEL_NAME = MODEL_NAMES[SEGMENT_MODEL]\n",
    "NUM_OF_EPOCH = 3\n",
    "SMALL_MODEL = False\n",
    "SAVE_MODEL_NAME = f\"model_{MODEL_NAMES}_dataset_{NAMES[CURRENT_DATASET]}_num_of_epoch_{NUM_OF_EPOCH}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3bbef9e-2a1c-4d60-8945-3970b8ab68a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model_['zhihan1996/DNABERT-2-117M', 'InstaDeepAI/nucleotide-transformer-500m-human-ref']_dataset_EXAMPLE_num_of_epoch_3\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if SMALL_MODEL:\n",
    "    SAVE_MODEL_NAME += \"_small\"\n",
    "SAVE_MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b683fba-71a4-4567-a80e-41f88b4df8cb",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3271e-a884-4a04-b175-e17efd33576b",
   "metadata": {},
   "source": [
    "## Example Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b38a01-5f68-4e61-9c6e-dd5a55061d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE_OF_DATASET = 300\n",
    "import random\n",
    "\n",
    "\n",
    "# example dataset\n",
    "MIN_NUM_OF_TOKENS = 30\n",
    "MAX_NUM_OF_TOKENS = 400\n",
    "MIN_SIZE_OF_TOKEN = 3\n",
    "MAX_SIZE_OF_TOKEN = 10\n",
    "REAL_TOKEN = \"GCGCGCGCGCGCGCGC\"\n",
    "FAKE_TOKEN = \"ATATATATATATATAT\"\n",
    "\n",
    "# Generate a random integer between 1 and 10 (inclusive)\n",
    "\n",
    "def create_dataset(number_of_elements : int, curr_index : int):\n",
    "    \n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    ids = []\n",
    "    for i in range(number_of_elements):\n",
    "        num_of_tokens = random.randint(MIN_NUM_OF_TOKENS, MAX_NUM_OF_TOKENS)\n",
    "        length_of_token = random.randint(MIN_SIZE_OF_TOKEN, MAX_SIZE_OF_TOKEN)\n",
    "        tokens.append([REAL_TOKEN[:length_of_token],FAKE_TOKEN[:length_of_token]])\n",
    "        ner_tags.append([1,0])\n",
    "        ids.append(curr_index)\n",
    "        curr_index+=1\n",
    "    ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"tokens\": tokens, \n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"id\":ids,\n",
    "    })\n",
    "    return ds\n",
    "    # pass\n",
    "\n",
    "def create_dataset_dict(number_of_elements : int):\n",
    "    train_ds = create_dataset(number_of_elements,0)\n",
    "    validation_ds = create_dataset(number_of_elements,number_of_elements)\n",
    "    test_ds = create_dataset(number_of_elements,number_of_elements*2)\n",
    "    \n",
    "    ds = DatasetDict({\"train\": train_ds, \"validation\" : validation_ds, \"test\":test_ds})\n",
    "    return ds\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72449a7-c32c-45e4-954b-9c80fef2fa24",
   "metadata": {},
   "source": [
    "## Targetscan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7f8d0a-414c-452a-ad55-81e3ee7986b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETSCAN_DATASET_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/targetscan_RNA_combined_nrows_90000_hg38.csv\"\n",
    "def create_targetscan_dataset(df,start,end):\n",
    "    tokens = df[\"result\"][start:end]\n",
    "    ner_tags = df[\"stags\"][start:end]\n",
    "    ids = df[\"Unnamed: 0\"][start:end]\n",
    "    ds = Dataset.from_dict(\n",
    "    {\n",
    "        \"tokens\": tokens, \n",
    "        \"ner_tags\": ner_tags,\n",
    "        \"id\":ids,\n",
    "    })\n",
    "    return ds\n",
    "\n",
    "def create_targetscan_dataset_dict(df):\n",
    "    train_ds = create_targetscan_dataset(df,2,len(df) // 2 + 10)\n",
    "    validation_ds = create_targetscan_dataset(df,len(df) // 2 + 11,(len(df) // 4) * 3)\n",
    "    test_ds = create_targetscan_dataset(df,(len(df) // 4) * 3,(len(df) // 4) * 4 - 2)\n",
    "    \n",
    "    ds = DatasetDict({\"train\": train_ds, \"validation\" : validation_ds, \"test\":test_ds})\n",
    "    return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d022f75-3687-47e4-9ea7-47663988549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targetscan_RNA_combined_nrows_90000.csv\n",
      "targetscan_RNA_combined_nrows_90000_hg38.csv\n"
     ]
    }
   ],
   "source": [
    "!ls /sci/nosnap/michall/roeizucker/token_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e9dbd-3017-4686-a642-604146a08303",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ece9ea55-a945-45ce-91dd-75b2857f6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CURRENT_DATASET == EXAMPLE_DATASET:\n",
    "    ds = create_dataset_dict(SIZE_OF_DATASET)\n",
    "elif CURRENT_DATASET == TARGETSCAN_DATASET:\n",
    "    df = pd.read_csv(TARGETSCAN_DATASET_PATH)\n",
    "    \n",
    "    SAMPLE_TEXT = df[\"result\"][100].replace(\"'\",\"\").replace(\" \",\"\").replace(\"[\",\"\").replace(\"]\",\"\")\n",
    "    df[\"result\"] = df[\"result\"].str.replace(\"'\",\"\")\n",
    "    df[\"result\"] = df[\"result\"].apply(lambda x:x.strip('][').split(', '))\n",
    "    df[\"stags\"] = df[\"stags\"].apply(lambda x:x.strip('][').split(', '))\n",
    "    df['stags'] = df['stags'].apply(lambda x: list(map(int, x)))\n",
    "    df = df[df[\"result\"].map(len) < 20]\n",
    "    ds = create_targetscan_dataset_dict(df)\n",
    "else:\n",
    "    raise Exception(\"no such dataset\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb9fc3c-fe5a-4718-8076-43c72a422d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e8d4277-b167-4f37-895a-727a86b54f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['GCGCGCGCG', 'ATATATATA'], 'ner_tags': [1, 0], 'id': 0}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m example \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(example)\n\u001b[0;32m---> 58\u001b[0m tokenized_input \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m], is_split_into_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(tokenized_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     60\u001b[0m tokenized_ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(tokenize_and_align_labels, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\",hidden_size=400, num_hidden_layers=4, num_attention_heads=4,intermediate_size=35,max_position_embeddings=10000)\n",
    "\n",
    "# if SMALL_MODEL:\n",
    "#     config = BertConfig.from_pretrained(MODEL_NAME,hidden_size=800, num_hidden_layers=8, num_attention_heads=8,intermediate_size=70,max_position_embeddings=10000)\n",
    "# else:\n",
    "#     config = BertConfig.from_pretrained(MODEL_NAME,max_position_embeddings=10000)\n",
    "# # model = AutoModel.from_config(config)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = TFAutoModelForTokenClassification.from_config(config)\n",
    "example = ds[\"train\"][0]\n",
    "print(example)\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "# example = ds[\"train\"][0]\n",
    "# label_list = ds[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "batch_size = 16\n",
    "num_train_epochs = NUM_OF_EPOCH\n",
    "num_train_steps = (len(tokenized_ds[\"train\"]) // batch_size) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_ds[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=2,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "model.compile(optimizer=optimizer)  # No loss argument!\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n",
    "callbacks = [metric_callback]\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=NUM_OF_EPOCH, callbacks=callbacks)\n",
    "\n",
    "\n",
    "# change evaluation to something generic\n",
    "classifier = pipeline(\"ner\", model=model,tokenizer=tokenizer)\n",
    "print(\"text\",SAMPLE_TEXT,\"size\",SIZE_OF_DATASET)\n",
    "for val in classifier(SAMPLE_TEXT):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbaece9-76e3-4b35-a9cc-2a5b5def36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(model.to)\n",
    "print(tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a3794-c56d-4f9b-a07e-d25b4b1d8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c7bc46-a051-48c9-b319-016c934f3d04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classifier = pipeline(\"ner\", model=model,tokenizer=tokenizer)\n",
    "# print(\"text\",SAMPLE_TEXT,\"size\",SIZE_OF_DATASET)\n",
    "for val in classifier(''.join(tokenized_ds[\"train\"][2][\"tokens\"])):\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845834e3-2862-40f1-888c-13cad28ad8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27dd50e-c451-4427-8c53-482ace2dc70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTED_VAL = 101\n",
    "tokens = []\n",
    "result = classifier(''.join(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"]))\n",
    "for i in range(len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"])):\n",
    "    tokens.extend([str(tokenized_ds[\"train\"][TESTED_VAL][\"ner_tags\"][i])] * len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"][i]))\n",
    "print(\"code\\t\\t\",''.join(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"]))\n",
    "print(\"origin\\t\\t\",''.join(tokens))\n",
    "\n",
    "pred = []\n",
    "for val in result:\n",
    "    pred.extend([val[\"entity\"][-1]]*len(val[\"word\"]))\n",
    "\n",
    "print(\"predicted\\t\",''.join(pred))\n",
    "\n",
    "\n",
    "\n",
    "print(\"predicted\")\n",
    "for val in result:\n",
    "    if val[\"entity\"] == 'LABEL_1':\n",
    "        print(val)\n",
    "\n",
    "print(\"source\")\n",
    "count = 0\n",
    "for i in range(len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"])):\n",
    "    if tokenized_ds[\"train\"][TESTED_VAL][\"ner_tags\"][i] == 1:\n",
    "        print(count,\"-\",end=\"\")\n",
    "    count += len(tokenized_ds[\"train\"][TESTED_VAL][\"tokens\"][i])\n",
    "    if tokenized_ds[\"train\"][TESTED_VAL][\"ner_tags\"][i] == 1:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f66df-730b-47e0-97c5-7fe77684ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_ds[\"train\"][0]\n",
    "''.join(tokenized_ds[\"train\"][2][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8792b-a3c9-48af-a6e9-9cc4d89a6108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_ds[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b043b-393b-4c20-b598-1d430683cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_pretrained(SAVE_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799b4c4c-7ee4-429e-a170-a3c5dd74d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\"modeldir\")\n",
    "# print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7123c19-8247-47a5-8ec5-4017ea5c6bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"ner\", model = SAVE_MODEL_NAME,device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84501c39-a9ef-4d29-af4a-32fe40059db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in pipe(''.join(tokenized_ds[\"train\"][2][\"tokens\"])):\n",
    "    print(val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
