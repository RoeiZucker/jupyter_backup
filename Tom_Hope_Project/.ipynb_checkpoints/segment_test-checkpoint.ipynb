{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760562b6-0265-449d-b994-61751053452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from evaluate import load\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99c72ab-ae3a-43be-b154-8f51116587e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb\n",
    "# export HF_HOME=/sci/archive/michall/roeizucker/huggingface_modles_cache\n",
    "# set HF_HOME=/sci/archive/michall/roeizucker/huggingface_modles_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cafd26ec-36fb-46b0-abf8-8f4850db8b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 1]]\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 1 0]]\n",
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 1]]\n",
      "[[0 0 0]\n",
      " [0 1 1]\n",
      " [0 0 0]]\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 0 1]]\n",
      "[[0 1 0]\n",
      " [1 0 1]\n",
      " [0 1 0]]\n",
      "[[0 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[1 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "[[1 0 1]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "kernel = np.array([\n",
    "    [1,1,1],\n",
    "    [0,0,0],\n",
    "    [1,1,1]\n",
    "])\n",
    "\n",
    "board =np.array( [[0, 0, 0, 1, 0, 0],\n",
    "    \t\t\t[0, 0, 0, 1, 0, 0],\n",
    "\t\t    \t[0, 1, 1, 0, 1, 0],\n",
    "    \t\t\t[0, 0, 0, 1, 0, 0],\n",
    "    \t\t\t[0, 0, 0, 1, 0, 0],\n",
    "    \t\t\t[0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "height,width =board.shape[0],board.shape[1]\n",
    "\n",
    "# Get the height, width, and number of channels of the kernel\n",
    "kernel_height,kernel_width = kernel.shape[0],kernel.shape[1]\n",
    "\n",
    "# Create a new image of original img size minus the border \n",
    "# where the convolution can't be applied\n",
    "new_img = np.zeros((height-kernel_height+1,width-kernel_width+1,3)) \n",
    "\n",
    "# Loop through each pixel in the image\n",
    "# But skip the outer edges of the image\n",
    "for i in range(kernel_height//2, height-kernel_height//2-1):\n",
    "    for j in range(kernel_width//2, width-kernel_width//2-1):\n",
    "        # Extract a window of pixels around the current pixel\n",
    "        window = board[i-kernel_height//2 : i+kernel_height//2+1,j-kernel_width//2 : j+kernel_width//2+1]\n",
    "        print(window)\n",
    "        print(window*kernel)\n",
    "        print(\"****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cda305d2-f74c-4bc9-a399-2c0185df7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee54550-57f4-4b1b-a7a2-dd78482c6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETSCAN_DATASET_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/targetscan_RNA_combined_nrows_90000_hg38.csv\"\n",
    "KMER_SIZE = 6\n",
    "model_name = \"InstaDeepAI/nucleotide-transformer-500m-1000g\"\n",
    "MAX_TOKEN = 1000\n",
    "BLANK_LABLE_VALUE = -100\n",
    "ADD_BLANKS_TO_END = False\n",
    "TRAIN_AMOUNT = 0.5\n",
    "AMOUNT_OF_ZEROS_TO_IGNORE = 0.975\n",
    "EPOCH_NUM = 10\n",
    "METRIC = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ead6446-f0fc-4f04-af32-3c4cb9710297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1718326-d006-40cf-a53b-66a8bbb09978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliceAI data:\n",
    "TRAIN_DF_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/spliceai_train.csv\"\n",
    "TEST_DF_PATH = \"/sci/nosnap/michall/roeizucker/token_classification/spliceai_test.csv\"\n",
    "train_df = pd.read_csv(TRAIN_DF_PATH)\n",
    "test_df = pd.read_csv(TEST_DF_PATH)\n",
    "SPLICEAI_LABLES = [\"none\",\"start\",\"end\"]\n",
    "SPLICE_AI = True\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceba973c-952b-4218-b3b9-cf894e617bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4748f1-6763-4b23-a323-1b0f7f848652",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00299c80-e99e-4e39-8ae0-42254dc01639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_representation_to_list(df,seq_col_name=\"sequnces\",labels_col_name=\"labels\"):\n",
    "    df[\"result\"] = df[seq_col_name].str.replace(\"'\",\"\")\n",
    "    df[\"result\"] = df[\"result\"].apply(lambda x:x.strip('][').split(', '))\n",
    "    df[\"stags\"] = df[labels_col_name].apply(lambda x:x.strip('][').replace(\"'\",\"\").split(', '))\n",
    "    df['stags'] = df['stags'].apply(lambda x: list(map(SPLICEAI_LABLES.index, x)))\n",
    "    return df\n",
    "def create_sequnces_from_df(df):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for row_idx, row in df.iterrows():\n",
    "        if row_idx % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            # print(filename.split(\"/\")[-1])\n",
    "            print(str(int(100*(row_idx / len(df)))) + \"%\")\n",
    "\n",
    "        curr_label = []\n",
    "        for i, seq in enumerate(row[\"result\"]):\n",
    "            curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "        \n",
    "        label = np.zeros(len(curr_label)//KMER_SIZE,dtype=np.int64)\n",
    "        for i in range(0,len(curr_label),KMER_SIZE):\n",
    "            # if i+KMER_SIZE <len(curr_label) and 1 in curr_label[i:i+KMER_SIZE]) :\n",
    "            #     label[i//KMER_SIZE:i//6+1] = 1\n",
    "            \n",
    "            for j in range(1,len(SPLICEAI_LABLES)):\n",
    "                if i+KMER_SIZE <len(curr_label) and j in curr_label[i:i+KMER_SIZE]:\n",
    "                    label[i//KMER_SIZE:i//KMER_SIZE+1] = j\n",
    "                    break\n",
    "            else: # unneeded\n",
    "                label[i//KMER_SIZE:i//KMER_SIZE+1] = 0\n",
    "        seq_str = \"\".join(row[\"result\"])[:len(curr_label)]\n",
    "        last_i = 0\n",
    "        \n",
    "        for i in range(0,len(label) - MAX_TOKEN,MAX_TOKEN):\n",
    "            if sum(label[i:i+MAX_TOKEN]) == 0:\n",
    "                last_i = i\n",
    "                continue\n",
    "            sequences.append(seq_str[i*6:(i+MAX_TOKEN)*6])\n",
    "            labels.append(label[i:i+MAX_TOKEN])\n",
    "            last_i = i\n",
    "        if ADD_BLANKS_TO_END:\n",
    "            sequences.append(seq_str[last_i*6:])\n",
    "            \n",
    "            labels.append(np.concatenate(\n",
    "                [label[last_i:] ,  \n",
    "                np.array(\n",
    "                    [BLANK_LABLE_VALUE]* \n",
    "                    (MAX_TOKEN - len(label[last_i:])))]))\n",
    "    return sequences, labels\n",
    "def create_datasets(seq,labels):\n",
    "    train_sequences, test_sequences, train_labels, test_labels = train_test_split(seq, labels, test_size=0.25, shuffle=False)\n",
    "    \n",
    "    train_tokenized = tokenizer(\n",
    "        train_sequences,\n",
    "        max_length=1000,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    test_tokenized = tokenizer(\n",
    "        test_sequences,\n",
    "        max_length=1000,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    train_dataset = Dataset.from_dict(train_tokenized)\n",
    "    test_dataset = Dataset.from_dict(test_tokenized)\n",
    "    \n",
    "    train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "    test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "    return train_dataset,test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2ce15a-2f4f-4ee6-a102-dcd8c907f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(train_df.head(1)[\"stags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4513ae43-8855-49f7-a0de-c32643ea9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"train.hf\") and not os.path.exists(\"test.hf\"):\n",
    "    if SPLICE_AI == True:\n",
    "        train_df = convert_list_representation_to_list(train_df)\n",
    "        test_df = convert_list_representation_to_list(test_df)\n",
    "        \n",
    "    train_seq, train_labels = create_sequnces_from_df(train_df)\n",
    "    # ignore some of the 0 values\n",
    "    if AMOUNT_OF_ZEROS_TO_IGNORE!= 0:\n",
    "        for i in range(len(train_labels)):\n",
    "            sequence_labels = train_labels[i]\n",
    "            # for sequence_labels in train_labels:\n",
    "            zero_indices = np.where(sequence_labels == 0)[0]\n",
    "    \n",
    "            # Calculate the number of zeros to replace based on AMOUNT_OF_ZEROS_TO_IGNORE\n",
    "            num_zeros_to_replace = int(AMOUNT_OF_ZEROS_TO_IGNORE * len(zero_indices))\n",
    "            \n",
    "            # Randomly choose the specified percentage of zero indices\n",
    "            indices_to_replace = np.random.choice(zero_indices, num_zeros_to_replace, replace=False)\n",
    "            \n",
    "            # Replace those values with -100\n",
    "            sequence_labels[indices_to_replace] = -100\n",
    "    \n",
    "            train_labels[i] = sequence_labels\n",
    "    train_seq = train_seq[:int(len(train_seq) * TRAIN_AMOUNT)]\n",
    "    train_labels = train_labels[:int(len(train_labels) * TRAIN_AMOUNT)]\n",
    "    train_dataset,test_dataset = create_datasets(train_seq, train_labels)\n",
    "    train_dataset.save_to_disk(\"train.hf\")\n",
    "    test_dataset.save_to_disk(\"test.hf\")\n",
    "\n",
    "    # train_dataset.to_csv(\"curr_train_dataset.csv\")\n",
    "    # test_dataset.to_csv(\"curr_test_dataset.csv\")\n",
    "else:\n",
    "    train_dataset = load_from_disk(\"train.hf\")\n",
    "    test_dataset = load_from_disk(\"test.hf\")\n",
    "\n",
    "    # train_dataset = Dataset.from_dict(load_dataset(\"csv\", data_files=\"curr_train_dataset.csv\"))\n",
    "    # test_dataset = Dataset.from_dict(load_dataset(\"csv\", data_files=\"curr_test_dataset.csv\"))\n",
    "    \n",
    "    # train_dataset = Dataset.from_csv(\"curr_train_dataset.csv\")\n",
    "    # test_dataset = Dataset.from_csv(\"curr_test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec616d9-88b5-4f1c-9ec8-2b3f65bb103f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b798d47f-1786-46de-b4b3-05b0d7baa25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for i in range(len(train_dataset)):\n",
    "    if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(str(int(100*(i / len(train_dataset)))) + \"%\")\n",
    "    for val in train_dataset[i][\"labels\"]:\n",
    "        if val not in dic:\n",
    "            dic[val] = 0\n",
    "        dic[val]+= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f26c3706-5081-4369-8967-6e647e41124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-100 label: 23415074\n",
      "0 label: 603084\n",
      "1 label: 55870\n",
      "2 label: 49972\n"
     ]
    }
   ],
   "source": [
    "for label in dic:\n",
    "    print(f\"{label} label:\",dic[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f0dd5dd-9ca7-46e0-bc8b-893e74471143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.exists(\"curr_train_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd27bc-2247-412b-b5e0-f2ad74d65df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "176e84d8-f87e-4789-83a7-8127b45707eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-1000g and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name,num_labels=len(SPLICEAI_LABLES), device_map=\"auto\")\n",
    "    return model\n",
    "model = create_model()\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b27b4c2-706a-4aa5-b3c5-d4f602208433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "# TODO: USE their parameters and LORA. copy from them\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-secondary-structure\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    eval_strategy = \"no\",\n",
    "    do_eval=False,\n",
    "    # evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=EPOCH_NUM,\n",
    "    weight_decay=0.001,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557e9070-be41-4f07-a0fe-65046704b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric.compute(predictions=np.array([[0,0,0]]), references=np.array([[0,0,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f262cd-147a-4a59-a894-faed3318cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load, combine\n",
    "import numpy as np\n",
    "if METRIC != \"None\":\n",
    "    metric = load(METRIC)\n",
    "    clf_metrics = combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "master_dict = {}\n",
    "def compute_metrics(eval_pred):\n",
    "    # logits, labels = eval_pred\n",
    "    # predictions = np.argmax(logits, axis=-1)\n",
    "    # return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=BLANK_LABLE_VALUE]\n",
    "    labels = labels[labels!=BLANK_LABLE_VALUE]\n",
    "\n",
    "    master_dict[\"predictiosn\"] = predictions\n",
    "    master_dict[\"references\"] = labels\n",
    "    return metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    # return {\"recall\":metric.compute(predictions=master_dict[\"predictiosn\"],references=master_dict[\"references\"],average=None)[\"recall\"][1:].mean()}\n",
    "    # return clf_metrics.compute(predictions=predictions, references=labels,average=\"weighted\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f218937c-5d3f-47a0-a0fb-b26db86ee987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric.compute(predictions=master_dict[\"predictiosn\"], references=master_dict[\"references\"],average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21c1f920-3fb6-4b06-be57-57d793452047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_dict[\"references\"]\n",
    "# assert(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361fc6d-d03b-48a9-a1c6-c7f073061e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1063831/845390017.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5141' max='241240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5141/241240 51:41 < 39:35:11, 1.66 it/s, Epoch 0.21/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.489700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.498400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.503600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "# assert(False)\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=\"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-finetuned-secondary-structure/checkpoint-96498\")\n",
    "# assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69907fee-dd7f-406c-ab28-128a15451365",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f\"spliceAI_NT_0.1_{str(datetime.datetime.now())}_{EPOCH_NUM}_{METRIC}_{TRAIN_AMOUNT}_{AMOUNT_OF_ZEROS_TO_IGNORE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc2fb9d-fcf0-46a9-8531-61a105ea8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df86e70b-cbfa-4109-8930-01751aeb634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history[-0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "935771db-6d99-48e6-b6dc-693421657ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177ecdda-22c1-4636-99dd-2b153212f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3b9cbe-cb2e-4613-99db-2150ab9f78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    # logits, labels = eval_pred\n",
    "    # predictions = np.argmax(logits, axis=-1)\n",
    "    # return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=BLANK_LABLE_VALUE]\n",
    "    labels = labels[labels!=BLANK_LABLE_VALUE]\n",
    "\n",
    "    master_dict[\"predictiosn\"] = predictions\n",
    "    master_dict[\"references\"] = labels\n",
    "    return metric.compute(predictions=predictions, references=labels,average=\"weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f21432-3649-4846-baad-3c622565993d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_for_num(num,dataset,trainer):\n",
    "    predict_dataset = Dataset.from_dict(dataset[num:num+1])\n",
    "    if (len(predict_dataset[0][\"labels\"])) != MAX_TOKEN:\n",
    "        return []\n",
    "    # input()\n",
    "    raw_pred, _, _ = trainer.predict(predict_dataset)\n",
    "    y_pred = np.argmax(raw_pred, axis=2)\n",
    "    counter = 0\n",
    "    flag = False\n",
    "    res = []\n",
    "    for pred,orig in zip(y_pred[0],np.array(dataset[num][\"labels\"])):\n",
    "        if orig==-100:\n",
    "            break\n",
    "        if pred != 0 or orig != 0:\n",
    "        # if pred != 0:\n",
    "            flag = True\n",
    "            # print(num,pred,orig,counter)\n",
    "            res.append((num,pred,orig,counter))\n",
    "        counter+=1\n",
    "    return res\n",
    "    # if flag:\n",
    "    #     print(\"*******\"*11)\n",
    "# results = []\n",
    "# for i in range(0,500):\n",
    "#     # print(i)\n",
    "#     print(i)\n",
    "#     results.extend(show_for_num(i,test_dataset,trainer))\n",
    "# results\n",
    "# show_for_num(17,train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "037641dd-b686-4376-a1a7-8d5a309b942d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset[33][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db71c2-80b6-4e09-8b70-ca74fbf55003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 8\n",
    "for i in range(len(results)):\n",
    "    if results[i][1] !=0 and results[i][2] !=0:\n",
    "        print(results[i])\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1893c-95b9-4f49-a656-625920b837b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dataset = Dataset.from_dict(test_dataset[0:1])\n",
    "raw_pred, _, _ = trainer.predict(predict_dataset)\n",
    "y_pred = np.argmax(raw_pred, axis=2)\n",
    "# print(y_pred[0])\n",
    "# print(np.array(test_dataset[num][\"labels\"]))\n",
    "counter = 8\n",
    "for pred,orig in zip(y_pred[0],np.array(test_dataset[counter][\"labels\"])):\n",
    "    if orig==-100:\n",
    "        break\n",
    "    if orig!=0 or pred != 0:\n",
    "        print(pred,orig,counter)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f498b7-b71e-4d03-8ac0-5c5f6d82f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cba41-da7b-48ab-bbba-49c4953c0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # try to load:\n",
    "# model1 = AutoModelForTokenClassification.from_pretrained(\"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-finetuned-secondary-structure/checkpoint-89272\")\n",
    "# model2 = AutoModelForTokenClassification.from_pretrained(\"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-finetuned-secondary-structure/checkpoint-100431\")\n",
    "# model3 = AutoModelForTokenClassification.from_pretrained(\"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-finetuned-secondary-structure/checkpoint-111590\")\n",
    "# # trainer.train(resume_from_checkpoint=\"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-finetuned-secondary-structure/checkpoint-89272\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e6427-2d5a-4dd1-b83d-d64eca85a74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer1= Trainer(\n",
    "#     model=model1,args=args\n",
    "# )\n",
    "# trainer2= Trainer(\n",
    "#     model=model2,args=args\n",
    "# )\n",
    "# trainer3= Trainer(\n",
    "#     model=model3,args=args\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c31dba-1cb5-4dcf-a127-3e01d4dec3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for trainer in trainers:\n",
    "#     calc_for_num(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87be8ed-6691-4427-ace7-cf0b44b6e671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafa440-890e-4eae-8e29-5151973b889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=data_collator,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc22a5ba-d5fb-4d48-a1e3-586787c0c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "local_metric = load(\"accuracy\")\n",
    "\n",
    "def calc_for_num(trainer,dataset,count):\n",
    "    curr_res = []\n",
    "    for i in range(count):\n",
    "        curr_res.extend(show_for_num(i,dataset,trainer))\n",
    "    return curr_res\n",
    "\n",
    "def delete_me(eval_pred):\n",
    "    # print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return local_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def local_compute_metrics_avaraged(eval_pred,local_metric):\n",
    "    # print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return local_metric.compute(predictions=predictions, references=labels,average=None)\n",
    "\n",
    "def local_compute_metrics_non_avaraged(eval_pred,local_metric):\n",
    "    # print(eval_pred)\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return local_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def do_multiple_evals(regular_metrics,avaraged_metrics,model,dataset):\n",
    "    res = {}\n",
    "    for metric,name in regular_metrics:\n",
    "        try:\n",
    "            trainer = Trainer(model,args,compute_metrics=lambda eval_pred: local_compute_metrics_non_avaraged(eval_pred,metric))\n",
    "            res[name] = trainer.evaluate(Dataset.from_dict(dataset[0:count]))\n",
    "            # del trainer\n",
    "        finally:\n",
    "            del trainer\n",
    "    for metric,name in avaraged_metrics:\n",
    "        try:\n",
    "            trainer = Trainer(model,args,compute_metrics=lambda eval_pred: local_compute_metrics_avaraged(eval_pred,metric))\n",
    "            res[name] = trainer.evaluate(Dataset.from_dict(dataset[0:count]))\n",
    "        finally:\n",
    "            del trainer\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0c2ba16-baae-4f4f-9768-1f266e1f41d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint-96496_train\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'curr_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 22\u001b[0m\n\u001b[1;32m     21\u001b[0m curr_model \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoints_path,dir_name))\n\u001b[0;32m---> 22\u001b[0m curr_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelete_me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m final_results\u001b[38;5;241m.\u001b[39mappend(calc_for_num(curr_trainer,dataset,count))\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    607\u001b[0m ):\n\u001b[0;32m--> 608\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/trainer.py:881\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 881\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/modeling_utils.py:3164\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3161\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3163\u001b[0m         )\n\u001b[0;32m-> 3164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 15.73 GiB of which 4.38 MiB is free. Including non-PyTorch memory, this process has 4.83 GiB memory in use. Process 1289765 has 10.87 GiB memory in use. Of the allocated memory 4.57 GiB is allocated by PyTorch, and 124.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;66;03m# final_results.append()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m curr_model\n\u001b[0;32m---> 30\u001b[0m             \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mcurr_trainer\u001b[49m\n\u001b[1;32m     31\u001b[0m dataset_num\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'curr_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "eval_results = []\n",
    "dir_names = []\n",
    "avaraged_metrics = [(load(\"f1\"),\"f1\"),(load(\"recall\"),\"recall\"),(load(\"precision\"),\"precision\")]\n",
    "regular_metrics = [(load(\"accuracy\"),\"accuracy\")]\n",
    "checkpoints_path = \"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-finetuned-secondary-structure/\"\n",
    "\n",
    "count = 250\n",
    "# dataset = train_dataset\n",
    "\n",
    "\n",
    "dataset_num = 0\n",
    "dataset_names = [\"train\",\"test\"]\n",
    "for dataset in [train_dataset,test_dataset]:\n",
    "    for dir_name in os.listdir(checkpoints_path):\n",
    "        if \"checkpoint\" in dir_name:\n",
    "            dir_names.append(dir_name + \"_\" +dataset_names[dataset_num])\n",
    "            print(dir_name + \"_\" +dataset_names[dataset_num])\n",
    "            try:\n",
    "                # print(os.path.join(checkpoints_path,dir_name))\n",
    "                curr_model = AutoModelForTokenClassification.from_pretrained(os.path.join(checkpoints_path,dir_name))\n",
    "                curr_trainer = Trainer(curr_model,args,compute_metrics = delete_me)\n",
    "                final_results.append(calc_for_num(curr_trainer,dataset,count))\n",
    "                # eval_results.append(curr_trainer.evaluate(Dataset.from_dict(dataset[0:count]))[\"eval_f1\"])\n",
    "                eval_results.append(do_multiple_evals(regular_metrics,avaraged_metrics,curr_model,dataset))\n",
    "\n",
    "            finally:\n",
    "            # final_results.append()\n",
    "                del curr_model\n",
    "                del curr_trainer\n",
    "    dataset_num+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b0f2c-551c-456b-bf46-0090fd8d2578",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(final_results)):\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    for match in final_results[i]:\n",
    "        if match[1] == match[2]:\n",
    "            good+=1\n",
    "        else:\n",
    "            bad+=1\n",
    "    print(dir_names[i])\n",
    "    print(good,bad)\n",
    "    for key in eval_results[i]:\n",
    "        print(key,eval_results[i][key][\"eval_\"+key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ecc8ec-251e-45b9-9d91-c1b5301752cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame([dir_names,final_results,eval_results]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74992c25-875c-40b6-94c5-7d35cfd9e8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.columns = ['name', 'res', 'score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2774c-1ed4-4b90-9b3f-8b3f5d1cbe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = f\"results_9-epoch_{TRAIN_AMOUNT}-ratio-0.9_of_0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c20ae8-8e00-460d-b360-582d66eb02c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a910d6b-2cee-427a-a591-77ad853c65c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df[\"score\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dbf419b-24bf-4a73-869a-e7cdf652e871",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfinal_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      3\u001b[0m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m final_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x:x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "final_df[\"accuracy_score\"] = final_df[\"score\"].apply(lambda x:x[\"accuracy\"][\"eval_accuracy\"])\n",
    "final_df[\"f1_0\"] = final_df[\"score\"].apply(lambda x:x[\"f1\"][\"eval_f1\"][0])\n",
    "final_df[\"f1_1\"] = final_df[\"score\"].apply(lambda x:x[\"f1\"][\"eval_f1\"][1])\n",
    "final_df[\"f1_2\"] = final_df[\"score\"].apply(lambda x:x[\"f1\"][\"eval_f1\"][2])\n",
    "final_df[\"recall_0\"] = final_df[\"score\"].apply(lambda x:x[\"recall\"][\"eval_recall\"][0])\n",
    "final_df[\"recall_1\"] = final_df[\"score\"].apply(lambda x:x[\"recall\"][\"eval_recall\"][1])\n",
    "final_df[\"recall_2\"] = final_df[\"score\"].apply(lambda x:x[\"recall\"][\"eval_recall\"][2])\n",
    "final_df[\"precision_0\"] = final_df[\"score\"].apply(lambda x:x[\"precision\"][\"eval_precision\"][0])\n",
    "final_df[\"precision_1\"] = final_df[\"score\"].apply(lambda x:x[\"precision\"][\"eval_precision\"][1])\n",
    "final_df[\"precision_2\"] = final_df[\"score\"].apply(lambda x:x[\"precision\"][\"eval_precision\"][2])\n",
    "final_df.to_csv(df_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843f6e6-5f05-4aaa-983d-0f08d99864c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def make_plot(col,suf):\n",
    "    temp_df = final_df[[\"name\",col]]\n",
    "    temp_df = temp_df[temp_df[\"name\"].str.endswith(suf)]\n",
    "    temp_df[\"step\"] = (temp_df[\"name\"].str.replace(\"checkpoint-\",\"\").str.replace(\"_\" + suf,\"\").astype(int))\n",
    "    \n",
    "    temp_df.sort_values(by=\"step\")[[col]].reset_index()[col].plot.line()\n",
    "    plt.title(col + \"-\" + suf)\n",
    "    # plt.show()\n",
    "skip = False\n",
    "for score in final_df.columns[3:]:\n",
    "    make_plot(score,\"train\")\n",
    "    make_plot(score,\"test\")\n",
    "    plt.legend([\"train\",\"test\"])\n",
    "    plt.savefig(f\"{score}.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060db81f-8f1f-46b1-996f-f9c51076e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_model = AutoModelForTokenClassification.from_pretrained(os.path.join(checkpoints_path,dir_name))\n",
    "curr_trainer = Trainer(curr_model,args,compute_metrics=local_compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a771cfa-154d-48b1-8d4e-6289b9a74cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "del curr_trainer\n",
    "del curr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3bbb7-933d-43ec-9b80-8c34129d4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_trainer.evaluate(Dataset.from_dict(test_dataset[0:10]))[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeea128-4d86-4137-a53e-a73d4d41abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_trainer.evaluate(Dataset.from_dict(train_dataset[0:10]))[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5bd15-9254-4730-bb7e-60a8f6cda088",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TARGETSCAN_DATASET_PATH)\n",
    "\n",
    "df[\"result\"] = df[\"result\"].str.replace(\"'\",\"\")\n",
    "df[\"result\"] = df[\"result\"].apply(lambda x:x.strip('][').split(', '))\n",
    "df[\"stags\"] = df[\"stags\"].apply(lambda x:x.strip('][').split(', '))\n",
    "df['stags'] = df['stags'].apply(lambda x: list(map(int, x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1563c07-4dad-49bc-b818-84b242494c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7329c62-936e-4a08-9505-80af0b49d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx, row in df.iterrows():\n",
    "\n",
    "    curr_label = []\n",
    "    for i, seq in enumerate(row[\"result\"]):\n",
    "        curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "    # if len(curr_label) > 6000:\n",
    "    #     curr_label = curr_label[:5977] # TODO: change so even after the label it's added, split region to multiple spaces\n",
    "    # if len(curr_label) %6 != 0:\n",
    "    label = np.zeros(len(curr_label)//6,dtype=np.int64)\n",
    "    # else:\n",
    "    #     continue\n",
    "        \n",
    "    for i in range(0,len(curr_label),6):\n",
    "        if i+6 <len(curr_label) and sum(curr_label[i:i+6]) > 1:\n",
    "            label[i//6:i//6+1] = 1\n",
    "        else: # unneeded\n",
    "            label[i//6:i//6+1] = 0\n",
    "    if len(label) > 1000: #TODO: magic number! this is max length of token\n",
    "        label = label[:1000]\n",
    "    if len(label) < 1000:\n",
    "        label = np.concatenate([label, np.array([-100]* (1000 - len(label)))])\n",
    "    sequences.append( \"\".join(row[\"result\"])[:len(curr_label)])\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598fcf59-5d5e-45e4-8b8e-a0ac86f7eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # working\n",
    "# sequences = []\n",
    "# labels = []\n",
    "\n",
    "# for row_idx, row in df.iterrows():\n",
    "\n",
    "#     curr_label = []\n",
    "#     for i, seq in enumerate(row[\"result\"]):\n",
    "#         curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "#     if len(curr_label) > 1000:\n",
    "#         continue\n",
    "#     if len(curr_label) %6 != 0:\n",
    "#         label = np.zeros(len(curr_label)//6,dtype=np.int64)\n",
    "#     else:\n",
    "#         continue\n",
    "        \n",
    "#     for i in range(0,len(curr_label),6):\n",
    "#         if i+6 <len(curr_label) and sum(curr_label[i:i+6]) > 1:\n",
    "#             label[i//6:i//6+1] = 1\n",
    "#         else: # unneeded\n",
    "#             label[i//6:i//6+1] = 0\n",
    "#     sequences.append( \"\".join(row[\"result\"]))\n",
    "#     labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b76279-d434-4be0-aa29-c6c00928359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # working 2\n",
    "# sequences = []\n",
    "# labels = []\n",
    "\n",
    "# for row_idx, row in df.iterrows():\n",
    "\n",
    "#     curr_label = []\n",
    "#     for i, seq in enumerate(row[\"result\"]):\n",
    "#         curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "#     if len(curr_label) > 6000:\n",
    "#         curr_label = curr_label[:5977] # TODO: change so even after the label it's added, split region to multiple spaces\n",
    "#     if len(curr_label) %6 != 0:\n",
    "#         label = np.zeros(len(curr_label)//6,dtype=np.int64)\n",
    "#     else:\n",
    "#         continue\n",
    "        \n",
    "#     for i in range(0,len(curr_label),6):\n",
    "#         if i+6 <len(curr_label) and sum(curr_label[i:i+6]) > 1:\n",
    "#             label[i//6:i//6+1] = 1\n",
    "#         else: # unneeded\n",
    "#             label[i//6:i//6+1] = 0\n",
    "#     sequences.append( \"\".join(row[\"result\"])[:len(curr_label)])\n",
    "#     labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133d2804-a00f-45c8-a752-da334ec37d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for row_idx, row in df.iterrows():\n",
    "\n",
    "    curr_label = []\n",
    "    for i, seq in enumerate(row[\"result\"]):\n",
    "        curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "    # if len(curr_label) > 6000:\n",
    "    #     curr_label = curr_label[:5977] # TODO: change so even after the label it's added, split region to multiple spaces\n",
    "    # if len(curr_label) %6 != 0:\n",
    "    label = np.zeros(len(curr_label)//6,dtype=np.int64)\n",
    "    # else:\n",
    "    #     continue\n",
    "        \n",
    "    for i in range(0,len(curr_label),6):\n",
    "        if i+6 <len(curr_label) and sum(curr_label[i:i+6]) > 1:\n",
    "            label[i//6:i//6+1] = 1\n",
    "        else: # unneeded\n",
    "            label[i//6:i//6+1] = 0\n",
    "    if len(label) > 1000: #TODO: magic number! this is max length of token\n",
    "        label = label[:1000]\n",
    "    if len(label) < 1000:\n",
    "        label = np.concatenate([label, np.array([-100]* (1000 - len(label)))])\n",
    "    sequences.append( \"\".join(row[\"result\"])[:len(curr_label)])\n",
    "    labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300a936-5376-46f7-a691-007f72cd7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sequences[0]) // 6)\n",
    "print(len(labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b22a6d-7e13-4161-a63f-85066b51c210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1dc356-3fd4-4ba5-8fdc-ad99c9f0a638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dc77a0-73cd-47a4-9d5c-15cb04670978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74520e6b-9f98-47de-9b82-fa1a44b637c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_tokenized = tokenizer(\n",
    "    train_sequences,\n",
    "    max_length=1000,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "test_tokenized = tokenizer(\n",
    "    test_sequences,\n",
    "    max_length=1000,\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c797a-3da9-4b67-a593-2f4a896c964d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867b9e9-ca0b-4ebf-8e74-32bd3b07adf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098d82f-75e9-4862-b6cb-aa6a5102c959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893387b-2cbd-47b5-8f5a-3e2ca10d5852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be730f0d-1a6b-4a8d-87c1-a4fd32286c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31495f6e-d135-436e-a868-9befd7de5759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09737e-0a75-4577-801d-3ec8babf6126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 9\n",
    "print(sequences[i])\n",
    "print(labels[i])\n",
    "print(len(sequences[i]))\n",
    "print(len(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362138ff-78a3-47f4-8fbc-5cc7e5c975d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = []\n",
    "# for i,val in enumerate(train_dataset):\n",
    "#     if ((len(val[\"input_ids\"]) -len(val[\"labels\"]))!= 2):\n",
    "#         to_remove.append(i)\n",
    "# # what we don't want\n",
    "# exclude_idx = to_remove\n",
    "\n",
    "# # create new dataset exluding those idx\n",
    "# train_dataset = train_dataset.select(\n",
    "#     (\n",
    "#         i for i in range(len(train_dataset)) \n",
    "#         if i not in set(exclude_idx)\n",
    "#     )\n",
    "# )\n",
    "# a = 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba82117-b333-4cba-9570-5fbe2ed3d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(to_remove[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd48cf4-e184-4745-887d-3bbe40e87b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_dataset[0])\n",
    "# print(sequences[0])\n",
    "# print(labels[0])\n",
    "# # print(train_dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914198ed-8465-423b-a635-68998e530043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = []\n",
    "# print(len(train_dataset))\n",
    "\n",
    "# for i,val in enumerate(train_dataset):\n",
    "#     if ((len(val[\"input_ids\"]) -len(val[\"labels\"]))!= 2):\n",
    "#         # if ((len(val[\"input_ids\"]) -len(val[\"labels\"]))!= 2) or val[\"labels\"][-2] == 1 or val[\"labels\"][-1] == 1:\n",
    "#         to_remove.append(i)\n",
    "#         # print(len(train_sequences[i]) % 6)\n",
    "# # what we don't want\n",
    "# exclude_idx = to_remove\n",
    "\n",
    "# # create new dataset exluding those idx\n",
    "# train_dataset = train_dataset.select(\n",
    "#     (\n",
    "#         i for i in range(len(train_dataset)) \n",
    "#         if i not in set(exclude_idx)\n",
    "#     )\n",
    "# )\n",
    "# print(len(train_dataset))\n",
    "# # a = 1 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272977f-16a9-4441-94e4-3513d82fda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25e06e5-70be-4099-8f4e-b2c306e593ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_print(i):\n",
    "    print(sequences[i])\n",
    "    print(labels[i])\n",
    "    print(len(sequences[i]))\n",
    "    print(len(labels[i]))\n",
    "    print(len(sequences[i])/6)\n",
    "    print(len(labels[i]))\n",
    "    print(train_dataset[i])\n",
    "    print(len(train_dataset[i][\"input_ids\"]))\n",
    "    print(\"#\"*30)\n",
    "# do_print(819)\n",
    "# do_print(820)\n",
    "# do_print(821)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1833a-7097-4054-af0a-1242c30bb486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56e613-26ea-44b5-b945-06aaeeb2dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_remove = []\n",
    "# print(len(test_dataset))\n",
    "\n",
    "# for i,val in enumerate(test_dataset):\n",
    "#     if ((len(val[\"input_ids\"]) -len(val[\"labels\"]))!= 2):\n",
    "#     # if ((len(val[\"input_ids\"]) -len(val[\"labels\"]))!= 2) or val[\"labels\"][-2] == 1 or val[\"labels\"][-1] == 1:\n",
    "#         to_remove.append(i)\n",
    "# # what we don't want\n",
    "# exclude_idx = to_remove\n",
    "\n",
    "# # create new dataset exluding those idx\n",
    "# test_dataset = test_dataset.select(\n",
    "#     (\n",
    "#         i for i in range(len(test_dataset)) \n",
    "#         if i not in set(exclude_idx)\n",
    "#     )\n",
    "# )\n",
    "# print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ceab-2385-494b-a600-7681b7620feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311d26d5-a3ad-4c6e-9d7f-c777025221d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daddcd85-1178-4b3a-b939-e6e1a9260775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# for val in train_dataset:\n",
    "#     print(counter,len(val[\"input_ids\"]),len(val[\"labels\"]))\n",
    "#     counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763a73a-8764-410d-85fa-b5a8515e91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 2\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name,num_labels=num_labels, device_map=\"auto\")\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3d678-b636-43b4-a787-61447b98d4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "587bf142-599c-44eb-8933-0ac6f0d55767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "# TODO: USE their parameters and LORA. copy from them\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-secondary-structure\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66913ce-c096-4c11-bf5b-8636f1a0c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    labels = labels.reshape((-1,))\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    predictions = predictions.reshape((-1,))\n",
    "    predictions = predictions[labels!=-100]\n",
    "    labels = labels[labels!=-100]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179f8cf-3f22-4450-92de-a45aaabf5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export CUDA_LAUNCH_BLOCKING=1\n",
    "# len(train_dataset[1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c816b9-67e9-4c9f-92b6-29d96b1596eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset[3365][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a379e-240d-4f24-8ee8-0ee709a5ea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_dataset[3365][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b9cd2-c566-43c4-ae1a-16f421bbd7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d35f3-9c67-469e-b225-74a1a237ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude_idx = [1296]\n",
    "\n",
    "# # create new dataset exluding those idx\n",
    "# train_dataset = train_dataset.select(\n",
    "#     (\n",
    "#         i for i in range(len(train_dataset)) \n",
    "#         if i not in set(exclude_idx)\n",
    "#     )\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149fd0ca-e92d-4d03-94ee-77fcf748439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c3273-7580-4af7-b234-1dc5d81c9033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !set PYTORCH_CUDA_ALLOC_CONF=\"expandable_segments:True\"\n",
    "# !echo $PYTORCH_CUDA_ALLOC_CONF\n",
    "# !echo 123\n",
    "# trainer.args\n",
    "# for i in range(len(train_dataset)):\n",
    "#     print(i,len(train_dataset[i][\"labels\"]),len(train_dataset[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947f59c-3841-4ea8-8f5e-572b981e4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_dataset)):\n",
    "#     if len(train_dataset[i][\"labels\"]) == 3260:\n",
    "#         print(i)\n",
    "#         print(len(train_dataset[i][\"input_ids\"]))\n",
    "#     # print(len(train_dataset[i][\"input_ids\"]))\n",
    "#     # print(len(train_dataset[i][\"labels\"]))\n",
    "#     # print(len(sequences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759039e5-7a9a-4570-b825-f03abf156c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 5\n",
    "print(len(sequences[i]) // 6)\n",
    "print(len(labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a30c28-5084-4055-abc2-67acf4c14b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_for_num(num):\n",
    "    predict_dataset = Dataset.from_dict(test_dataset[num:num+1])\n",
    "    raw_pred, _, _ = trainer.predict(predict_dataset)\n",
    "    y_pred = np.argmax(raw_pred, axis=2)\n",
    "    # print(y_pred[0])\n",
    "    # print(np.array(test_dataset[num][\"labels\"]))\n",
    "    counter = 0\n",
    "    for pred,orig in zip(y_pred[0],np.array(test_dataset[num][\"labels\"])):\n",
    "        if orig==-100:\n",
    "            break\n",
    "        # if pred == 1 or orig == 1:\n",
    "        if pred == 1:\n",
    "            print(pred,orig,counter)\n",
    "        counter+=1\n",
    "    print(\"*******\"*11)\n",
    "# for i in range(50,90):\n",
    "#     show_for_num(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d458b517-1729-48f8-a16f-e8f5ca2b0330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c288d240-81c9-4f49-81c2-443b2fead1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test_dataset[2][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee89ecf-c58e-4c56-b6be-6281e0add1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export HF_HOME=\"/path/to/another/directory/datasets\"\n",
    "# set  HF_HOME=\"/path/to/another/directory/datasets\"\n",
    "# FastaIndexingError: The FASTA file /cs/usr/roeizucker/.cache/huggingface/datasets/downloads/hg38.fa does not contain a valid sequence. Check that sequence definition lines start with '>'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8b9e1-6dbe-4df5-93e9-86d2e30aa343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d3a52-127f-4ed4-83a1-22e7dc7e4193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd6f71-f57e-4c91-83c8-8ded13ab02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # predictions \n",
    "# predictions = np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54ce0ab-db55-42f1-9be9-97e7be97eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb27cb9-a249-455f-81c2-0e19030691e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predict_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054ab4f-027d-4442-8b85-5fc77318ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, metrics = trainer.predict(predict_dataset, metric_key_prefix=\"predict\")\n",
    "metrics[\"predict_samples\"] =  len(predict_dataset)\n",
    "\n",
    "trainer.log_metrics(\"predict\", metrics)\n",
    "trainer.save_metrics(\"predict\", metrics)\n",
    "\n",
    "# predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21352f7-0af2-4294-9dd6-fe759b22022e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f90a1-e707-4421-b9a3-93baa7d91b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # output_predict_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n",
    "# for index, item in enumerate(predictions):\n",
    "#     # item = label_list[item]\n",
    "#     print(f\"{index}\\t{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94235bee-3ce0-405e-bbbb-b7fccef309e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# Load the promoter dataset from the InstaDeep Hugging Face ressources\n",
    "dataset_name = \"splice_sites_all\"\n",
    "train_dataset_promoter = load_dataset(\n",
    "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\n",
    "        dataset_name,\n",
    "        split=\"train\",\n",
    "        streaming= False,\n",
    "    )\n",
    "# test_dataset_promoter = load_dataset(\n",
    "#         \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "#         dataset_name,\n",
    "#         split=\"test\",\n",
    "#         streaming= False,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c005bbb6-7d44-405b-86cd-7578bb7b3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset_promoter[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199f3e5-05c1-4d2a-a094-4faa6dea2b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Use this parameter to download sequences of arbitrary length (see docs below for edge cases)\n",
    "sequence_length=2048\n",
    "\n",
    "# One of:\n",
    "# [\"variant_effect_causal_eqtl\",\"variant_effect_pathogenic_clinvar\",\n",
    "# \"variant_effect_pathogenic_omim\",\"cage_prediction\", \"bulk_rna_expression\",\n",
    "# \"chromatin_features_histone_marks\",\"chromatin_features_dna_accessibility\",\n",
    "# \"regulatory_element_promoter\",\"regulatory_element_enhancer\"] \n",
    "\n",
    "task_name = \"variant_effect_causal_eqtl\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"InstaDeepAI/genomics-long-range-benchmark\",\n",
    "    task_name=task_name,\n",
    "    sequence_length=sequence_length,\n",
    "    # subset = True, if applicable\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a3d0f-aac1-4e71-b697-d6d790cf4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688cd42-3d02-4f69-bc94-4840b73bfe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyfaidx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
