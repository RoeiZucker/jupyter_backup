{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37a87b44-4406-4bbb-902b-c3436519f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-31 21:19:52.040754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-31 21:19:55.045904: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-31 21:19:55.940085: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-31 21:20:01.032871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-31 21:20:37.152117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, load_dataset, load_from_disk\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from evaluate import load\n",
    "import datetime\n",
    "import os\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5beb3215-1035-4319-a2d0-fe96f8c2b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change to work with classes?\n",
    "NUCLEPTIDE_TRANSFORMER_500M_1000G_NAME = \"InstaDeepAI/nucleotide-transformer-500m-1000g\"\n",
    "INSTADEEP_BASIC_RUN_MODELS = [NUCLEPTIDE_TRANSFORMER_500M_1000G_NAME]\n",
    "INSTADEEP_KMER_SIZE = 6\n",
    "INSTADEEP_BASIC_MAX_TOKEN_LENGTH = 1000\n",
    "\n",
    "#TODO: Change so that these values are taken from JSON\n",
    "TRAIN_PATH = \"table_browser_hg38_unmasked_CpG_train.csv\"\n",
    "TEST_PATH = \"table_browser_hg38_unmasked_CpG_test.csv\"\n",
    "MODEL_NAME = NUCLEPTIDE_TRANSFORMER_500M_1000G_NAME\n",
    "DO_DOWNSAMPLING = False\n",
    "LABLES_USED = [0,1]\n",
    "TRAIN_SIZE = 1\n",
    "BATCH_SIZE = 1\n",
    "SUFFIX = \"unmasked_cpg\"\n",
    "EPOCH_NUM = 10\n",
    "METRIC = \"None\"\n",
    "CONTINUE_FROM = \"/sci/nosnap/michall/roeizucker/jupyter_notebooks/Tom_Hope_Project/InstaDeepAI/nucleotide-transformer-500m-1000g-unmasked_cpg/checkpoint-167115\"\n",
    "DATASET_TRAIN_SAVE_PATH = \"unmasked_cpg_train\"\n",
    "DATASET_TEST_SAVE_PATH = \"unmasked_cpg_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ba0f16-6c17-4d45-8d33-0960eafa5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6335df0b-fd36-485b-976e-69daa92676c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "train_df = train_df.head(int(len(train_df) *TRAIN_SIZE))\n",
    "test_df = pd.read_csv(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1dee6d-8d2d-4e2a-a608-6114ef969477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_INSTADEEP_basic_label_seq_from_df(df):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for row_idx, row in df.iterrows():\n",
    "        if row_idx % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(str(int(100*(row_idx / len(df)))) + \"%\")\n",
    "\n",
    "        curr_label = row[\"labels\"]\n",
    "        # for i, seq in enumerate(row[\"result\"]):\n",
    "        #     curr_label.extend([row[\"stags\"][i]]*len(seq))\n",
    "        \n",
    "        label = np.zeros(len(curr_label)//INSTADEEP_KMER_SIZE,dtype=np.int64)\n",
    "        for i in range(0,len(curr_label),INSTADEEP_KMER_SIZE):\n",
    "            \n",
    "            for j in range(1,len(LABLES_USED)):\n",
    "                # TODO: consinder changing str(j) to something else\n",
    "                if i+INSTADEEP_KMER_SIZE <len(curr_label) and str(j) in curr_label[i:i+INSTADEEP_KMER_SIZE]:\n",
    "                    label[i//INSTADEEP_KMER_SIZE:i//INSTADEEP_KMER_SIZE+1] = j\n",
    "                    break\n",
    "            else: # unneeded\n",
    "                label[i//INSTADEEP_KMER_SIZE:i//INSTADEEP_KMER_SIZE+1] = 0\n",
    "        # seq_str = \"\".join(row[\"result\"])[:len(curr_label)]\n",
    "        seq_str = row[\"seq\"]\n",
    "\n",
    "        # TODO: check why sometimes seq length is not 1000\n",
    "        # TODO: check why sometimes seq is type float\n",
    "        if (len(str(seq_str))) != 6000 or (len(label)) != 1000: \n",
    "            continue\n",
    "        sequences.append(seq_str)\n",
    "        labels.append(label)\n",
    "    return sequences, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4aae6c-a16c-4e12-9630-f7a8e1d5e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n"
     ]
    }
   ],
   "source": [
    "train_sequences,train_labels = create_INSTADEEP_basic_label_seq_from_df(train_df)\n",
    "test_sequences, test_labels = create_INSTADEEP_basic_label_seq_from_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00eaa2d-7429-4d5d-9969-43b1ab9cb137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    AGGTAGCTCTACTGCCTCCTCTTAAAACCAACAAAGGAAAGAGAGA...\n",
       "1    CAGCCTGGGCGACAGAGTGAGTCTAAAAAAAATAAAAAAGGAATTC...\n",
       "Name: seq, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(2)[\"seq\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5c541b-7b77-41d1-b4d5-9d80b8343ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_instadeep_basic(seq,labels,tokenizer):\n",
    "    tokenized_dataset = tokenizer(\n",
    "        seq,\n",
    "        max_length=INSTADEEP_BASIC_MAX_TOKEN_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    )\n",
    "    dataset = Dataset.from_dict(tokenized_dataset)\n",
    "    dataset = dataset.add_column(\"labels\", labels)\n",
    "    return dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "train_dataset = create_datasets_instadeep_basic(train_sequences,train_labels,tokenizer)\n",
    "test_dataset = create_datasets_instadeep_basic(test_sequences, test_labels,tokenizer)\n",
    "#TODO: change so if already saved to disk, load existing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74b0dc7-d404-493c-ac95-215bc34a2d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'table_browser_hg38_unmasked_CpG_train.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e52c6f2-3cef-4468-8f16-a7ef5b23bf59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a36a2a02ad4834bcf28b5daf376748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/33423 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf02018a23147dbb3cebd937bda00db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: check if resaving changes results\n",
    "train_dataset.save_to_disk(DATASET_TRAIN_SAVE_PATH)\n",
    "test_dataset.save_to_disk(DATASET_TEST_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d3289e-99af-46d8-8b44-06c904b16293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99%\n",
      "0 label: 30209385\n",
      "1 label: 3213615\n"
     ]
    }
   ],
   "source": [
    "dic = {}\n",
    "for i in range(len(train_dataset)):\n",
    "    if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(str(int(100*(i / len(train_dataset)))) + \"%\")\n",
    "    for val in train_dataset[i][\"labels\"]:\n",
    "        if val not in dic:\n",
    "            dic[val] = 0\n",
    "        dic[val]+= 1\n",
    "for label in dic:\n",
    "    print(f\"{label} label:\",dic[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c2995d3-7932-4196-bcec-223f4fe536e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1096dd90-5a81-479f-9f6a-9fe2bc0f4f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-1000g and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME,num_labels=len(LABLES_USED), device_map=\"auto\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0832d12-7f41-4d8f-9761-691a5ad7508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "# TODO: USE their parameters and LORA. copy from them\n",
    "args = TrainingArguments(\n",
    "    f\"{MODEL_NAME}-{SUFFIX}\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    do_eval=False,\n",
    "    # evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=3*batch_size,\n",
    "    #per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=EPOCH_NUM,\n",
    "    weight_decay=0.001,\n",
    "    # load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd808074-ce3e-4851-a917-c464b5df3662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2233286/2344023472.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9a0df1-a134-488c-bdd2-066f564d77cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/trainer.py:3418: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "/sci/nosnap/michall/roeizucker/new_python_env/lib/python3.11/site-packages/transformers/trainer.py:3081: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='173068' max='334230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [173068/334230 1:00:24 < 27:15:44, 1.64 it/s, Epoch 5.18/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>167500</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168000</td>\n",
       "      <td>0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168500</td>\n",
       "      <td>0.020200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169000</td>\n",
       "      <td>0.017100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169500</td>\n",
       "      <td>0.020500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>0.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170500</td>\n",
       "      <td>0.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171000</td>\n",
       "      <td>0.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171500</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172000</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172500</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173000</td>\n",
       "      <td>0.020800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CONTINUE_FROM == \"\":\n",
    "    trainer.train()\n",
    "else:\n",
    "    trainer.train(resume_from_checkpoint=CONTINUE_FROM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdadc1-af9b-4f44-8e22-95d8575a2df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
