{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "654b867b-a336-4b3a-ab08-b4c443109b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas_plink import read_plink\n",
    "import pandas_plink as pdp\n",
    "from pandas_plink import get_data_folder\n",
    "import random\n",
    "import json\n",
    "from IPython.display import clear_output\n",
    "import xarray as xr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19721aa7-fe42-4bbf-9eca-798d3ab1256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to work as beta.\n",
    "# create a function that accepts a vector of Betas and a flag if its binary/continues and add an option for covariates\n",
    "# If its binary, add prevelance considiration\n",
    "# for a person J the score for J is the sum over all betas of SNPS * the number of alleles. \n",
    "# add a noise so that the variance of phenotype will be 1 (adding noise using 1-sigma) call this Yj\n",
    "# to do so calculate overall variance after simulation, and add a noise with 1- the variance (make sure that the genetic + covar variance is below 1)\n",
    "# in continues state, use Yj as phenotype (maybe mul by a constant)\n",
    "# in binary state the same, need to do a logit (sigmuid and then threashold or just threshold) transformation. detarmine the threshold so that X% of the population are sick\n",
    "# another way, in the binary state instead of noise can do a bernuli on the result of the sigmuid (a value between 0 and 1)\n",
    "# the covariats can be treated the same way the betas are used\n",
    "# read about genetic architucture, compare covariates and betas effect.\n",
    "# the aggregative effect of snps should be larger, but the effect of a single snp should be smaller.\n",
    "\n",
    "# look at GWAS catalog, use a table from there for a specific disease and take their betas (find out how to calculate beta from OR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c70db5f3-731b-4086-b385-c38ad3b55a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269113d-2cbc-4303-be8f-d63a267d7db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69e1b4a-f93b-46fd-ac73-9154e7434f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select variants randomly\n",
    "# select variants by rarity\n",
    "# use linear calculation\n",
    "# use non linear calculation\n",
    "# with/without PC\n",
    "# how rare should the phenotype be - like an existing one\n",
    "# create syntetic covariates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c841dc-da9a-464c-89a5-ae8e08096172",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENETICS_PATH = \"/sci/archive/michall/roeizucker/small_genetics\"\n",
    "COVARS_PATH = \"/sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK\"\n",
    "PLINK_PATH = \"/sci/nosnap/michall/roeizucker/plink2\"\n",
    "TEST_COVAR = \"covariates_sex_year_of_birth_AC_batch_40_PC.txt\"\n",
    "# TEST_CHROM = \"/sci/archive/michall/roeizucker/small_genetics/allchr_thin_12\"\n",
    "TEST_CHROM = \"allchr_thin_fml\"\n",
    "TEST_CHROM_FREQ_PATH = \"./basic_freq.afreq\"\n",
    "PLINK_RESULT_PATH = \"/sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/results/chr22_very_reduced_simulation\"\n",
    "PLINK_RESULT_DIR = \"/sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/results/\"\n",
    "PHENOTYPE_FILE_RESULT_PATH = \"/sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/phenotypes/generated_phen.txt\"\n",
    "PERSON_BATCH_SIZE = 2000\n",
    "VERBOSE = False\n",
    "\n",
    "# FILE_ENDINGS = [\"PHENO1.glm.logistic.hybrid\",\"PHENO1.glm.linear\"]\n",
    "VERSION = \"multi_effect_score_1.1\"\n",
    "COVAR_DF_PREFIX = \"temp_covar_df\"\n",
    "INCLUDED_COVARS = [\"sex\",\"year_of_birth\",\"AC\",\"batch\"]\n",
    "AC_VALS = ['AC_stockport_pilot','AC_manchester','AC_oxford','AC_cardiff','AC_glasgow','AC_edinburgh','AC_stoke','AC_reading','AC_bury','AC_newcastle','AC_bristol','AC_barts','AC_nottingham','AC_sheffield','AC_liverpool','AC_middlesborough','AC_hounslow','AC_croydon','AC_birmingham','AC_swansea','AC_wrexham','AC_cheadle_revisit','AC_cheadle_imaging','AC_reading_imaging','AC_newcastle_imaging']\n",
    "BATCH_VALS = ['batch_Batch_b001','batch_Batch_b002','batch_Batch_b003','batch_Batch_b004','batch_Batch_b005','batch_Batch_b006','batch_Batch_b007','batch_Batch_b008','batch_Batch_b009','batch_Batch_b010','batch_Batch_b011','batch_Batch_b012','batch_Batch_b013','batch_Batch_b014','batch_Batch_b015','batch_Batch_b016','batch_Batch_b017','batch_Batch_b018','batch_Batch_b019','batch_Batch_b020','batch_Batch_b021','batch_Batch_b022','batch_Batch_b023','batch_Batch_b024','batch_Batch_b025','batch_Batch_b026','batch_Batch_b027','batch_Batch_b028','batch_Batch_b029','batch_Batch_b030','batch_Batch_b031','batch_Batch_b032','batch_Batch_b033','batch_Batch_b034','batch_Batch_b035','batch_Batch_b036','batch_Batch_b037','batch_Batch_b038','batch_Batch_b039','batch_Batch_b040','batch_Batch_b041','batch_Batch_b042','batch_Batch_b043','batch_Batch_b044','batch_Batch_b045','batch_Batch_b046','batch_Batch_b047','batch_Batch_b048','batch_Batch_b049','batch_Batch_b050','batch_Batch_b051','batch_Batch_b052','batch_Batch_b053','batch_Batch_b054','batch_Batch_b055','batch_Batch_b056','batch_Batch_b057','batch_Batch_b058','batch_Batch_b059','batch_Batch_b060','batch_Batch_b061','batch_Batch_b062','batch_Batch_b063','batch_Batch_b064','batch_Batch_b065','batch_Batch_b066','batch_Batch_b067','batch_Batch_b068','batch_Batch_b069','batch_Batch_b070','batch_Batch_b071','batch_Batch_b072','batch_Batch_b073','batch_Batch_b074','batch_Batch_b075','batch_Batch_b076','batch_Batch_b077','batch_Batch_b078','batch_Batch_b079','batch_Batch_b080','batch_Batch_b081','batch_Batch_b082','batch_Batch_b083','batch_Batch_b084','batch_Batch_b085','batch_Batch_b086','batch_Batch_b087','batch_Batch_b088','batch_Batch_b089','batch_Batch_b090','batch_Batch_b091','batch_Batch_b092','batch_Batch_b093','batch_Batch_b094','batch_Batch_b095','batch_UKBiLEVEAX_b1','batch_UKBiLEVEAX_b10','batch_UKBiLEVEAX_b11','batch_UKBiLEVEAX_b3','batch_UKBiLEVEAX_b4','batch_UKBiLEVEAX_b5','batch_UKBiLEVEAX_b6','batch_UKBiLEVEAX_b7','batch_UKBiLEVEAX_b8','batch_UKBiLEVEAX_b9']\n",
    "ALL_COVARS = INCLUDED_COVARS[:-2] + AC_VALS + BATCH_VALS\n",
    "NUM_PC = 40\n",
    "for i in range(NUM_PC):\n",
    "    ALL_COVARS.append(f\"PC{i+1}\")\n",
    "\n",
    "\n",
    "# phenotype generation constants\n",
    "TARGET_VARIANCE = 1\n",
    "DISEASE_PREVELANCE = 0.02\n",
    "PHENOTYPE_DIR_RESULT_PATH = \"/sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/phenotypes/\"\n",
    "INCLUDE_COVAR = True\n",
    "AC_EFFECT_SCORE = 1\n",
    "BATCH_EFFECT_SCORE = 1\n",
    "\n",
    "CREATE_COVAR_FILES = False # should we save the covar files\n",
    "BINARY_VARIANT = True\n",
    "if BINARY_VARIANT:\n",
    "    FILE_ENDING = \"PHENO1.glm.logistic.hybrid\"\n",
    "else:\n",
    "    FILE_ENDING = \"PHENO1.glm.linear\"\n",
    "\n",
    "SEX_EFFECT = 3\n",
    "YOB_EFFECT = 1\n",
    "PC_EFFECT_MAX = 0.3\n",
    "\n",
    "COVAR_GENETIC_EFFECT_RATIO = 2\n",
    "HERETEBILITY = 0.2\n",
    "COVAR_EFFECT = 0.5\n",
    "\n",
    "\n",
    "HERETABILITY_SCORES = [0,0.1,0.2,0.3,0.5,0.7]\n",
    "COVARIATE_SCORES = [0,0.1,0.2,0.3,0.5,0.7]\n",
    "\n",
    "AC_EFFECT = [AC_EFFECT_SCORE]*len(AC_VALS)\n",
    "BATCH_EFFECT = [BATCH_EFFECT_SCORE]*len(BATCH_VALS)\n",
    "\n",
    "PC_EFFECT = list(np.arange(PC_EFFECT_MAX,0,-PC_EFFECT_MAX/NUM_PC))\n",
    "COVAR_EFFECTS = [SEX_EFFECT,YOB_EFFECT] + AC_EFFECT + BATCH_EFFECT + PC_EFFECT\n",
    "\n",
    "\n",
    "# variant parameter\n",
    "# NUMBER_OF_MAJOR_VARIANTS = 30\n",
    "# NUMBER_OF_MEDIUM_VARIANTS = 80\n",
    "# NUMBER_OF_LESSER_VARIANTS = 120\n",
    "\n",
    "NUMBER_OF_MAJOR_VARIANTS = [5,5,5]\n",
    "NUMBER_OF_MEDIUM_VARIANTS = [10,10,10]\n",
    "NUMBER_OF_LESSER_VARIANTS = [25,25,25]\n",
    "\n",
    "\n",
    "COMMON_VARIANT_THRESHOLD = 0.2\n",
    "RARE_VARIANT_TRHESHOLD = 0.05\n",
    "VERY_RARE_VARIANT_TRHESHOLD = 0.001\n",
    "BUTTOM_THRESHOLD_vARIANTS = 0.0001\n",
    "\n",
    "MAJOR_VARIANT_EFFECT = 0.05\n",
    "MEDIUM_VARIANT_EFFECT = 0.015\n",
    "LESSER_VARIANT_EFFECT = 0.005\n",
    "\n",
    "# change to betas\n",
    "MAJOR_VARIANT_DELTA = 1\n",
    "MEDIUM_VARIANT_DELTA = 0.3\n",
    "LESSER_VARIANT_DELTA = 0.01\n",
    "\n",
    "\n",
    "# TODO: implement memory and threads in script\n",
    "MEMORY = 38000\n",
    "THREADS = 20\n",
    "\n",
    "COVAR_GROUPS = [\n",
    "    \"covariates_year_of_birth_0_PC\",\n",
    "    \"covariates_sex_0_PC\",\n",
    "    \"covariates_sex_year_of_birth_0_PC\",\n",
    "    \"covariates_5_PC\",\n",
    "    \"covariates_sex_year_of_birth_40_PC\",\n",
    "    \"covariates_sex_year_of_birth_batch_40_PC\",\n",
    "    \"covariates_sex_year_of_birth_AC_40_PC\",\n",
    "    \"covariates_sex_year_of_birth_AC_batch_40_PC\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c476d1-cd16-4b2c-bc2b-fe6bb27cb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_df = pd.read_csv(os.path.join(COVARS_PATH,TEST_COVAR),sep=\"\\t\",header=None)\n",
    "covar_df = covar_df.set_axis([\"iid\",\"fid\"] + ALL_COVARS, axis=1)\n",
    "legal_iid = set(covar_df[\"iid\"].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66fcf83-2ec3-4789-8670-ebd89b7fb27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [01:30<00:00, 30.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# G_combined = xr.concat(G_values, dim=\"variant\")\n",
    "G_combined = pdp.read_plink1_bin(os.path.join(GENETICS_PATH,TEST_CHROM) + \".bed\",\n",
    "                            os.path.join(GENETICS_PATH,TEST_CHROM) + \".bim\",\n",
    "                            os.path.join(GENETICS_PATH,TEST_CHROM) + \".fam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80e8f571-0e7f-4ff8-8b37-3a8a792bca3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variants = set(G_combined.snp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "055cdf7e-b70f-40e1-b7ea-8ad2bce1cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.read_csv(TEST_CHROM_FREQ_PATH,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed17f1e-eefa-448e-b3c5-ebdeb859364b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5213db-24b9-463b-b47e-e93e12d83bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_unique_variants(variants):\n",
    "#     unique_variants = variants[:]\n",
    "#     random.shuffle(variants)\n",
    "#     return unique_variants\n",
    "\n",
    "def get_variant_groups(common_snps,rare_snps,very_rare_snps):\n",
    "    common_snps = common_snps[:]\n",
    "    rare_snps = rare_snps[:]\n",
    "    very_rare_snps = very_rare_snps[:]\n",
    "    random.shuffle(common_snps)\n",
    "    random.shuffle(rare_snps)\n",
    "    random.shuffle(very_rare_snps)\n",
    "    major_variants = [\n",
    "        set(common_snps[:NUMBER_OF_MAJOR_VARIANTS[0]]),\n",
    "        set(rare_snps[:NUMBER_OF_MAJOR_VARIANTS[1]]),\n",
    "        set(very_rare_snps[:NUMBER_OF_MAJOR_VARIANTS[2]])\n",
    "    ]\n",
    "    common_snps = common_snps[NUMBER_OF_MAJOR_VARIANTS[0]:]\n",
    "    rare_snps = rare_snps[NUMBER_OF_MAJOR_VARIANTS[1]:]\n",
    "    very_rare_snps = very_rare_snps[NUMBER_OF_MAJOR_VARIANTS[2]:]\n",
    "    \n",
    "    medium_variants = [\n",
    "        set(common_snps[:NUMBER_OF_MEDIUM_VARIANTS[0]]),\n",
    "        set(rare_snps[:NUMBER_OF_MEDIUM_VARIANTS[1]]),\n",
    "        set(very_rare_snps[:NUMBER_OF_MEDIUM_VARIANTS[2]])\n",
    "    ]\n",
    "\n",
    "    common_snps = common_snps[NUMBER_OF_MEDIUM_VARIANTS[0]:]\n",
    "    rare_snps = rare_snps[NUMBER_OF_MEDIUM_VARIANTS[1]:]\n",
    "    very_rare_snps = very_rare_snps[NUMBER_OF_MEDIUM_VARIANTS[2]:]\n",
    "    \n",
    "    lesser_variants = [\n",
    "        set(common_snps[:NUMBER_OF_LESSER_VARIANTS[0]]),\n",
    "        set(rare_snps[:NUMBER_OF_LESSER_VARIANTS[1]]),\n",
    "        set(very_rare_snps[:NUMBER_OF_LESSER_VARIANTS[1]])\n",
    "    ]\n",
    "    # major_variants = set(unique_variants[0:NUMBER_OF_MAJOR_VARIANTS])\n",
    "    # medium_variants = set(unique_variants[NUMBER_OF_MAJOR_VARIANTS:NUMBER_OF_MAJOR_VARIANTS+NUMBER_OF_MEDIUM_VARIANTS])\n",
    "    # lesser_variants = set(unique_variants[NUMBER_OF_MAJOR_VARIANTS+NUMBER_OF_MEDIUM_VARIANTS:NUMBER_OF_MAJOR_VARIANTS+NUMBER_OF_MEDIUM_VARIANTS+NUMBER_OF_LESSER_VARIANTS])\n",
    "    return major_variants, medium_variants, lesser_variants\n",
    "\n",
    "def create_effect_dict(included_variants,major_variants,medium_variants,lesser_variants):\n",
    "    effect_dict = {}\n",
    "    for var in included_variants:\n",
    "        if var in major_variants:\n",
    "            effect_dict[var] = MAJOR_VARIANT_DELTA\n",
    "            continue\n",
    "        if var in medium_variants:\n",
    "            effect_dict[var] = MEDIUM_VARIANT_DELTA\n",
    "            continue\n",
    "        effect_dict[var] = LESSER_VARIANT_DELTA\n",
    "    return effect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4fac86-54f1-47c1-9186-af713fdb9d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3e911-6df8-4cd0-a9c4-99bca06477b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "307a56ce-920d-426d-a120-8d764bad8aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 115 331\n"
     ]
    }
   ],
   "source": [
    "common_snps = (list(freq_df[freq_df[\"ALT_FREQS\"] >= COMMON_VARIANT_THRESHOLD][\"ID\"]))\n",
    "rare_snps = list(freq_df[(freq_df[\"ALT_FREQS\"] > RARE_VARIANT_TRHESHOLD) & (freq_df[\"ALT_FREQS\"] < COMMON_VARIANT_THRESHOLD)][\"ID\"])\n",
    "very_rare_snps = list(freq_df[(freq_df[\"ALT_FREQS\"] >= VERY_RARE_VARIANT_TRHESHOLD) & (freq_df[\"ALT_FREQS\"] < RARE_VARIANT_TRHESHOLD)][\"ID\"])\n",
    "print(len(common_snps),len(rare_snps),len(very_rare_snps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccb9f50-8304-49fa-8b8d-f66cd8856ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd8c7aae-908e-4271-ac90-07b68a7f0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# samples_list = G_combined.sample.to_series()\n",
    "# G_subset = G.sel(sample=test_ids)\n",
    "# sample_df = G_subset.to_dataframe()\n",
    "\n",
    "\n",
    "# sample_df = sample_df.reset_index()\n",
    "# sample_df = sample_df[sample_df[\"iid\"].isin(legal_iid)]\n",
    "# print(sample_df)\n",
    "# unique_variants = get_unique_variants(snp_list)\n",
    "major_variants, medium_variants, lesser_variants = get_variant_groups(common_snps,rare_snps,very_rare_snps)\n",
    "major_set = major_variants[0].union(major_variants[1]).union(major_variants[2])\n",
    "medium_set = medium_variants[0].union(medium_variants[1]).union(medium_variants[2])\n",
    "lesser_set = lesser_variants[0].union(lesser_variants[1]).union(lesser_variants[2])\n",
    "\n",
    "included_variants = major_set.union(medium_set).union(lesser_set)\n",
    "covar_df['year_of_birth'] = (covar_df['year_of_birth'] - covar_df['year_of_birth'].mean()) / covar_df['year_of_birth'].std()\n",
    "effect_dict = create_effect_dict(included_variants,major_variants,medium_variants,lesser_variants)\n",
    "covar_df[\"covar_effects_chance\"] = None\n",
    "covar_df[\"genetic_chance\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b43004f4-536b-4f12-9ef1-80ea1c67f1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_var_dict = {}\n",
    "real_variants = []\n",
    "\n",
    "snps = list(G_combined.snp.values)\n",
    "variants = list(G_combined.variant.values)\n",
    "\n",
    "for i in range(len(snps)):\n",
    "    snp_var_dict[snps[i]] = variants[i]\n",
    "\n",
    "\n",
    "for snp in included_variants:\n",
    "    real_variants.append(snp_var_dict[snp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cc2f2-0d10-4b00-8332-96c7adbaedc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e68cc30c-a5bc-4cdb-9e32-b08b0ca1e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_filtered = G_combined.sel(variant=list(real_variants))\n",
    "G_filtered = G_filtered.sel(sample=list(legal_iid.intersection(set(G_filtered.sample.values))))\n",
    "samples_list = G_filtered.sample.to_series()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dbc381-22d1-4f0c-a0bc-5b38d1afdcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2dddedbd-5302-4ed4-a4cc-05360105e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variant_specific_df(G_subset):\n",
    "    df = G_subset.to_dataframe()\n",
    "    df = df.reset_index()\n",
    "    df = df[df[\"snp\"].isin(included_variants)]\n",
    "    return df\n",
    "\n",
    "def get_genotype_dict(df):\n",
    "    genotype_dict = df[[\"iid\",\"snp\",\"genotype\"]].set_index([\"iid\",\"snp\"]).to_dict(orient=\"index\")\n",
    "    for key in genotype_dict.keys():\n",
    "        genotype_dict[key] = genotype_dict[key][\"genotype\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c85cc620-7b2e-46d8-bab7-37605602779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330000 99 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(0,len(samples_list),PERSON_BATCH_SIZE):\n",
    "    if i % 6000 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(i,int( 100*(i /len(samples_list))),\"%\")\n",
    "\n",
    "    start = i\n",
    "    end = i+PERSON_BATCH_SIZE\n",
    "    test_ids = list(samples_list[start:end])\n",
    "    G_subset = G_filtered.sel(sample=test_ids)\n",
    "    df = get_variant_specific_df(G_subset)\n",
    "    genotype_dict = get_genotype_dict(df)\n",
    "    # calculate effect for each variant for each person TODO: extract to function\n",
    "    df[\"effect\"] = df[\"snp\"].map(effect_dict)\n",
    "    df[\"effect\"] = df[\"effect\"]*df[\"genotype\"]\n",
    "    df = df.reset_index()\n",
    "\n",
    "    covar_df[\"covar_effects_chance\"] = ((np.array(covar_df[covar_df.columns[2:-2]]) @ (np.array(COVAR_EFFECTS)))) \n",
    "    \n",
    "    # TODO: change name\n",
    "    disease_chance_dict = (1 - df.groupby(\"iid\")[\"effect\"].sum()).to_dict()\n",
    "    # disease_chance_dict\n",
    "    # TODO: change name, this is not a chance, but a result\n",
    "    covar_df[\"genetic_chance\"] = covar_df[\"iid\"].astype(str).map(disease_chance_dict).fillna(covar_df[\"genetic_chance\"])\n",
    "    # adjust so genetic effect and covar effect match the requested\n",
    "\n",
    "\n",
    "# afterwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f42d0a8-31dd-413b-97cc-4a5f6964f70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_df = covar_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b93a11e6-58be-4d81-a529-76eebfa6a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_df[\"covar_effects_chance\"] = (covar_df[\"covar_effects_chance\"]-covar_df[\"covar_effects_chance\"].mean())/covar_df[\"covar_effects_chance\"].std()\n",
    "covar_df[\"genetic_chance\"] = (covar_df[\"genetic_chance\"]-covar_df[\"genetic_chance\"].mean())/covar_df[\"genetic_chance\"].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e161e25-3b75-426e-9bac-e587f717d884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a64178b-c827-407b-8797-1929c2e9ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_df.to_csv(VERSION + \"temp_covar_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d1f310b-6691-414b-bb61-9e38f3ef816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# covar_df = covar_df.set_index(\"iid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff63567-5080-4f54-8729-9004d1faa63b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288a8f6-1510-42ed-a467-de788b9517a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "469747b7-27c9-461f-aa86-448058e94a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PC effect 6.149999999999994\n",
      "other covars effect 6\n",
      "PC relative effect in PC 0.5061728395061725\n"
     ]
    }
   ],
   "source": [
    "# [SEX_EFFECT,YOB_EFFECT] + AC_EFFECT + BATCH_EFFECT + PC_EFFECT\n",
    "pc_total_effect = sum(PC_EFFECT)\n",
    "other_total_effect = sum([SEX_EFFECT,YOB_EFFECT] +[AC_EFFECT_SCORE,BATCH_EFFECT_SCORE])\n",
    "print(\"PC effect\",pc_total_effect)\n",
    "print(\"other covars effect\",other_total_effect)\n",
    "print(\"PC relative effect in PC\",pc_total_effect / (pc_total_effect+other_total_effect))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b757ec70-1c2a-48a8-a00c-7d86e15f64e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted pc relative effect in PC 0.804002242271349\n"
     ]
    }
   ],
   "source": [
    "# ((np.array(covar_df[covar_df.columns[2:-2]]) @ (np.array(COVAR_EFFECTS))))\n",
    "# COVAR_EFFECTS = [SEX_EFFECT,YOB_EFFECT] + AC_EFFECT + BATCH_EFFECT + PC_EFFECT\n",
    "abs_array = np.abs(np.array(covar_df[covar_df.columns[2:-2]]))\n",
    "abs_scores =  abs_array@np.array(COVAR_EFFECTS)\n",
    "covar_only_effects = [0,0] + [0]*(len(AC_EFFECT) + len(BATCH_EFFECT)) + PC_EFFECT\n",
    "covar_only_scores = abs_array@np.array(covar_only_effects)\n",
    "pc_only_scores = covar_only_scores/abs_scores\n",
    "print(\"weighted pc relative effect in PC\",np.mean(pc_only_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d7dbb00-b2c0-454a-8bc5-cbb6e07ca23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print create JSON with parameters\n",
    "parameters_to_save_for_version = {\n",
    "    \"VERSION\":VERSION,\n",
    "    \"TARGET_VARIANCE\":TARGET_VARIANCE,\n",
    "    \"DISEASE_PREVELANCE\":DISEASE_PREVELANCE,\n",
    "    \"NUMBER_OF_MAJOR_VARIANTS\":NUMBER_OF_MAJOR_VARIANTS,\n",
    "    \"NUMBER_OF_MEDIUM_VARIANTS\":NUMBER_OF_MEDIUM_VARIANTS,\n",
    "    \"NUMBER_OF_LESSER_VARIANTS\":NUMBER_OF_LESSER_VARIANTS,\n",
    "    \"MAJOR_VARIANT_DELTA\":MAJOR_VARIANT_DELTA,\n",
    "    \"MEDIUM_VARIANT_DELTA\":MEDIUM_VARIANT_DELTA,\n",
    "    \"LESSER_VARIANT_DELTA\":LESSER_VARIANT_DELTA,\n",
    "    \"BINARY_VARIANT\" : BINARY_VARIANT,\n",
    "    \"COMMON_VARIANT_THRESHOLD\":COMMON_VARIANT_THRESHOLD,\n",
    "    \"RARE_VARIANT_TRHESHOLD\":RARE_VARIANT_TRHESHOLD,\n",
    "    \"VERY_RARE_VARIANT_TRHESHOLD\":VERY_RARE_VARIANT_TRHESHOLD\n",
    "}\n",
    "\n",
    "\n",
    "if INCLUDE_COVAR:\n",
    "\n",
    "    parameters_to_save_for_version[\"AC_EFFECT_SCORE\"] = AC_EFFECT_SCORE\n",
    "    parameters_to_save_for_version[\"BATCH_EFFECT_SCORE\"] = BATCH_EFFECT_SCORE\n",
    "    parameters_to_save_for_version[\"SEX_EFFECT\"] = SEX_EFFECT\n",
    "    parameters_to_save_for_version[\"YOB_EFFECT\"] = YOB_EFFECT\n",
    "    parameters_to_save_for_version[\"PC_EFFECT_MAX\"] = PC_EFFECT_MAX\n",
    "json_file_name = VERSION + \"_parameters\" + \".json\"\n",
    "if os.path.exists(json_file_name):\n",
    "    raise Exception(\"Version file exists\")\n",
    "with open(json_file_name, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parameters_to_save_for_version, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87228e90-0955-4362-abb5-78e9f110f369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0.1\n",
      "0 0.2\n",
      "0 0.3\n",
      "0 0.5\n",
      "0 0.7\n",
      "0.1 0\n",
      "0.1 0.1\n",
      "0.1 0.2\n",
      "0.1 0.3\n",
      "0.1 0.5\n",
      "0.1 0.7\n",
      "0.2 0\n",
      "0.2 0.1\n",
      "0.2 0.2\n",
      "0.2 0.3\n",
      "0.2 0.5\n",
      "0.2 0.7\n",
      "0.3 0\n",
      "0.3 0.1\n",
      "0.3 0.2\n",
      "0.3 0.3\n",
      "0.3 0.5\n",
      "0.3 0.7\n",
      "0.5 0\n",
      "0.5 0.1\n",
      "0.5 0.2\n",
      "0.5 0.3\n",
      "0.5 0.5\n",
      "0.5 0.7\n",
      "0.7 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3365533/2502292744.py:17: RuntimeWarning: invalid value encountered in scalar power\n",
      "  noise = np.random.normal(mu, sigma**0.5, len(temp_cov_df))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 0.1\n",
      "0.7 0.2\n",
      "0.7 0.3\n",
      "0.7 0.5\n",
      "0.7 0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3365533/2502292744.py:17: RuntimeWarning: invalid value encountered in scalar power\n",
      "  noise = np.random.normal(mu, sigma**0.5, len(temp_cov_df))\n",
      "/tmp/ipykernel_3365533/2502292744.py:17: RuntimeWarning: invalid value encountered in scalar power\n",
      "  noise = np.random.normal(mu, sigma**0.5, len(temp_cov_df))\n"
     ]
    }
   ],
   "source": [
    "# TODO: add creation of multiple phenotype files\n",
    "covar_dfs = []\n",
    "for her_score in HERETABILITY_SCORES:\n",
    "    for cov_score in COVARIATE_SCORES:\n",
    "        print(her_score,cov_score)\n",
    "        temp_cov_df = covar_df.copy()\n",
    "        gene_var = ((temp_cov_df[\"genetic_chance\"]).var())\n",
    "        covar_var = (temp_cov_df[\"covar_effects_chance\"].var())\n",
    "        genetic_factor = np.sqrt(her_score/gene_var)\n",
    "        covar_factor = np.sqrt(cov_score/covar_var)\n",
    "        temp_cov_df[\"genetic_chance\"] = temp_cov_df[\"genetic_chance\"] * genetic_factor\n",
    "        temp_cov_df[\"covar_effects_chance\"] = temp_cov_df[\"covar_effects_chance\"] * covar_factor\n",
    "        temp_cov_df[\"disease_chance\"] = temp_cov_df[\"genetic_chance\"] + temp_cov_df[\"covar_effects_chance\"]\n",
    "        added_variance = TARGET_VARIANCE -temp_cov_df[\"disease_chance\"].var()\n",
    "        # input(added_variance)\n",
    "        mu, sigma = 0, added_variance # mean and standard deviation\n",
    "        noise = np.random.normal(mu, sigma**0.5, len(temp_cov_df))\n",
    "        temp_cov_df[\"disease_chance\"] = temp_cov_df[\"disease_chance\"] + noise\n",
    "        # temp_cov_df[\"disease_chance\"].var()\n",
    "        quantile = (temp_cov_df[[\"disease_chance\"]].quantile(1 - DISEASE_PREVELANCE, method=\"table\", interpolation=\"nearest\")[\"disease_chance\"])\n",
    "        if BINARY_VARIANT:\n",
    "            temp_cov_df['disease_indicator'] = (\n",
    "                temp_cov_df['disease_chance'] > quantile\n",
    "            ).astype(int).astype(float)\n",
    "        else:\n",
    "            temp_cov_df['disease_indicator'] = temp_cov_df['disease_chance']\n",
    "        # temp_cov_df.dropna()\n",
    "        covar_dfs.append([temp_cov_df,her_score,cov_score])\n",
    "        if CREATE_COVAR_FILES:\n",
    "            temp_cov_df.to_csv(f\"{COVAR_DF_PREFIX}_v={VERSION}_her={her_score}_cov={cov_score}.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e9d40-9ae8-4adf-8948-e5c80db4cdf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92cca73c-c1a2-4589-96c8-63f48d955229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020000300620781914\n",
      "her_score 0\n",
      "cov_score 0.1\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "print(covar_dfs[i][0][\"disease_indicator\"].dropna().mean())\n",
    "print(\"her_score\",covar_dfs[i][1])\n",
    "print(\"cov_score\",covar_dfs[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3d5e3a4-a31e-4fa3-b24c-1983458272b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>fid</th>\n",
       "      <th>sex</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>AC_stockport_pilot</th>\n",
       "      <th>AC_manchester</th>\n",
       "      <th>AC_oxford</th>\n",
       "      <th>AC_cardiff</th>\n",
       "      <th>AC_glasgow</th>\n",
       "      <th>AC_edinburgh</th>\n",
       "      <th>...</th>\n",
       "      <th>PC35</th>\n",
       "      <th>PC36</th>\n",
       "      <th>PC37</th>\n",
       "      <th>PC38</th>\n",
       "      <th>PC39</th>\n",
       "      <th>PC40</th>\n",
       "      <th>covar_effects_chance</th>\n",
       "      <th>genetic_chance</th>\n",
       "      <th>disease_chance</th>\n",
       "      <th>disease_indicator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5895328</td>\n",
       "      <td>5895328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.152223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419695</td>\n",
       "      <td>-1.412190</td>\n",
       "      <td>5.011600</td>\n",
       "      <td>-2.041770</td>\n",
       "      <td>-0.329472</td>\n",
       "      <td>-0.610231</td>\n",
       "      <td>-0.459752</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.028187</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5764375</td>\n",
       "      <td>5764375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.076080</td>\n",
       "      <td>0.282474</td>\n",
       "      <td>1.712010</td>\n",
       "      <td>-1.723410</td>\n",
       "      <td>-0.140920</td>\n",
       "      <td>1.834630</td>\n",
       "      <td>-0.202652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.282121</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5503073</td>\n",
       "      <td>5503073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.901703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.917620</td>\n",
       "      <td>3.797780</td>\n",
       "      <td>-0.783315</td>\n",
       "      <td>-1.714340</td>\n",
       "      <td>4.746740</td>\n",
       "      <td>-0.425447</td>\n",
       "      <td>0.351544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.501597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2563626</td>\n",
       "      <td>2563626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.150145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180031</td>\n",
       "      <td>0.908227</td>\n",
       "      <td>-1.488970</td>\n",
       "      <td>-2.507100</td>\n",
       "      <td>-2.025750</td>\n",
       "      <td>7.023560</td>\n",
       "      <td>-0.114147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.940750</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4980832</td>\n",
       "      <td>4980832</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.400665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000040</td>\n",
       "      <td>1.880090</td>\n",
       "      <td>0.433608</td>\n",
       "      <td>-2.291620</td>\n",
       "      <td>-2.461820</td>\n",
       "      <td>-3.445180</td>\n",
       "      <td>0.153879</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.429348</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333366</th>\n",
       "      <td>2243883</td>\n",
       "      <td>2243883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.275405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.756630</td>\n",
       "      <td>-5.489920</td>\n",
       "      <td>2.986760</td>\n",
       "      <td>-0.533189</td>\n",
       "      <td>-0.582788</td>\n",
       "      <td>0.070181</td>\n",
       "      <td>0.006754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.040355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333367</th>\n",
       "      <td>3570729</td>\n",
       "      <td>3570729</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.476153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.785830</td>\n",
       "      <td>1.074660</td>\n",
       "      <td>-2.581080</td>\n",
       "      <td>-0.671792</td>\n",
       "      <td>2.334690</td>\n",
       "      <td>-3.509470</td>\n",
       "      <td>0.393571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.570682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333368</th>\n",
       "      <td>1663456</td>\n",
       "      <td>1663456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.277482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.008020</td>\n",
       "      <td>-1.370330</td>\n",
       "      <td>-3.174280</td>\n",
       "      <td>-3.719140</td>\n",
       "      <td>-1.817060</td>\n",
       "      <td>0.452773</td>\n",
       "      <td>0.212045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.347663</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333369</th>\n",
       "      <td>3118989</td>\n",
       "      <td>3118989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.152223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.979120</td>\n",
       "      <td>4.153130</td>\n",
       "      <td>-5.280610</td>\n",
       "      <td>-2.201190</td>\n",
       "      <td>-0.167185</td>\n",
       "      <td>-0.494174</td>\n",
       "      <td>0.102179</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.697819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333370</th>\n",
       "      <td>4573284</td>\n",
       "      <td>4573284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.150145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343660</td>\n",
       "      <td>3.394630</td>\n",
       "      <td>-3.394460</td>\n",
       "      <td>1.916540</td>\n",
       "      <td>-1.548800</td>\n",
       "      <td>-3.454470</td>\n",
       "      <td>0.170340</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.504857</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>332645 rows × 178 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            iid      fid  sex  year_of_birth  AC_stockport_pilot  \\\n",
       "0       5895328  5895328  1.0      -1.152223                 0.0   \n",
       "1       5764375  5764375  0.0      -0.024885                 0.0   \n",
       "2       5503073  5503073  1.0      -0.901703                 0.0   \n",
       "3       2563626  2563626  1.0      -0.150145                 0.0   \n",
       "4       4980832  4980832  1.0      -0.400665                 0.0   \n",
       "...         ...      ...  ...            ...                 ...   \n",
       "333366  2243883  2243883  1.0      -0.275405                 0.0   \n",
       "333367  3570729  3570729  1.0       0.476153                 0.0   \n",
       "333368  1663456  1663456  1.0      -1.277482                 0.0   \n",
       "333369  3118989  3118989  0.0      -1.152223                 0.0   \n",
       "333370  4573284  4573284  0.0      -0.150145                 0.0   \n",
       "\n",
       "        AC_manchester  AC_oxford  AC_cardiff  AC_glasgow  AC_edinburgh  ...  \\\n",
       "0                 0.0        1.0         0.0         0.0           0.0  ...   \n",
       "1                 0.0        1.0         0.0         0.0           0.0  ...   \n",
       "2                 0.0        0.0         1.0         0.0           0.0  ...   \n",
       "3                 0.0        0.0         0.0         0.0           0.0  ...   \n",
       "4                 0.0        0.0         0.0         0.0           0.0  ...   \n",
       "...               ...        ...         ...         ...           ...  ...   \n",
       "333366            1.0        0.0         0.0         0.0           0.0  ...   \n",
       "333367            0.0        0.0         0.0         0.0           0.0  ...   \n",
       "333368            0.0        0.0         0.0         0.0           0.0  ...   \n",
       "333369            0.0        0.0         0.0         0.0           0.0  ...   \n",
       "333370            0.0        0.0         0.0         0.0           0.0  ...   \n",
       "\n",
       "            PC35      PC36      PC37      PC38      PC39      PC40  \\\n",
       "0      -0.419695 -1.412190  5.011600 -2.041770 -0.329472 -0.610231   \n",
       "1      -3.076080  0.282474  1.712010 -1.723410 -0.140920  1.834630   \n",
       "2       6.917620  3.797780 -0.783315 -1.714340  4.746740 -0.425447   \n",
       "3       0.180031  0.908227 -1.488970 -2.507100 -2.025750  7.023560   \n",
       "4       3.000040  1.880090  0.433608 -2.291620 -2.461820 -3.445180   \n",
       "...          ...       ...       ...       ...       ...       ...   \n",
       "333366 -1.756630 -5.489920  2.986760 -0.533189 -0.582788  0.070181   \n",
       "333367 -1.785830  1.074660 -2.581080 -0.671792  2.334690 -3.509470   \n",
       "333368 -4.008020 -1.370330 -3.174280 -3.719140 -1.817060  0.452773   \n",
       "333369 -1.979120  4.153130 -5.280610 -2.201190 -0.167185 -0.494174   \n",
       "333370 -0.343660  3.394630 -3.394460  1.916540 -1.548800 -3.454470   \n",
       "\n",
       "        covar_effects_chance  genetic_chance  disease_chance  \\\n",
       "0                  -0.459752            -0.0        1.028187   \n",
       "1                  -0.202652             0.0       -0.282121   \n",
       "2                   0.351544             0.0        0.501597   \n",
       "3                  -0.114147             0.0       -0.940750   \n",
       "4                   0.153879            -0.0        1.429348   \n",
       "...                      ...             ...             ...   \n",
       "333366              0.006754             0.0        1.040355   \n",
       "333367              0.393571             0.0       -0.570682   \n",
       "333368              0.212045             0.0       -1.347663   \n",
       "333369              0.102179            -0.0        0.697819   \n",
       "333370              0.170340            -0.0        1.504857   \n",
       "\n",
       "        disease_indicator  \n",
       "0                     0.0  \n",
       "1                     0.0  \n",
       "2                     0.0  \n",
       "3                     0.0  \n",
       "4                     0.0  \n",
       "...                   ...  \n",
       "333366                0.0  \n",
       "333367                0.0  \n",
       "333368                0.0  \n",
       "333369                0.0  \n",
       "333370                0.0  \n",
       "\n",
       "[332645 rows x 178 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar_dfs[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "318d5ecd-cf33-41de-98ff-010c509c64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_dict = {}\n",
    "\n",
    "# variant_dict[\"major_vars\"] = list(major_variants)\n",
    "# variant_dict[\"med_vars\"] = list(medium_variants)\n",
    "# variant_dict[\"lesser_vars\"] = list(lesser_variants)\n",
    "variant_dict[\"major_vars\"] = [list(x) for x in major_variants]\n",
    "variant_dict[\"med_vars\"] = [list(x) for x in medium_variants]\n",
    "variant_dict[\"lesser_vars\"] = [list(x) for x in lesser_variants]\n",
    "json_file_name = VERSION + \"_variants.json\"\n",
    "if os.path.exists(json_file_name):\n",
    "    raise Exception(\"Version file exists\")\n",
    "with open(json_file_name, 'w', encoding='utf-8') as f:\n",
    "    json.dump(variant_dict, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c79491a-9fb8-4cb0-98ef-cef649a3fb2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "799ac707-c3a8-4818-89c8-540b756c4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands = []\n",
    "for temp_cov_df,her_score,cov_score in covar_dfs:\n",
    "    phenotype_file_result_name = (\n",
    "        f\"generated_pehn_result_ver={VERSION}_heretability={her_score}_covar_effect={cov_score}\")\n",
    "    phenotype_file_result_name = phenotype_file_result_name+\".txt\"\n",
    "    phenotype_file_path = os.path.join(PHENOTYPE_DIR_RESULT_PATH,phenotype_file_result_name)\n",
    "    temp_cov_df[[\"iid\",\"fid\",\"disease_indicator\"]].to_csv(phenotype_file_path,index=False,header=False,sep=\"\\t\")\n",
    "\n",
    "    chrom_path = os.path.join(GENETICS_PATH,TEST_CHROM)\n",
    "    result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",\"\"))\n",
    "    for covar_type in COVAR_GROUPS:\n",
    "        covar_file = os.path.join(COVARS_PATH,covar_type + \".txt\")\n",
    "        result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type))\n",
    "        command =f\"{PLINK_PATH} --bed {chrom_path}.bed --bim {chrom_path}.bim --fam {chrom_path}.fam --pheno {phenotype_file_path} --covar {covar_file}  --out  {result_path} --glm hide-covar --mac 20 --covar-variance-standardize --freq --threads {THREADS} --memory {MEMORY} --1 --adjust\"\n",
    "        # print(command)\n",
    "        # print(f'sbatch --mem=12g -c10 --time=3-0 --requeue --killable --wrap=\"{command}\"')\n",
    "        commands.append(f'sbatch --mem={(MEMORY//1000) + 1}g -c{THREADS} --time=3-0 --requeue --killable --wrap=\"{command}\"')\n",
    "    command =f\"{PLINK_PATH} --bed {chrom_path}.bed --bim {chrom_path}.bim --fam {chrom_path}.fam --pheno {phenotype_file_path} --out  {result_path} --glm hide-covar --mac 20 --covar-variance-standardize --freq --threads {THREADS} --memory {MEMORY} --1  --adjust\"\n",
    "    # print(command)\n",
    "    result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",\"_no_covar\"))\n",
    "    # print(f'sbatch --mem=12g -c10 --time=3-0 --requeue --killable --wrap=\"{command}\"')\n",
    "    commands.append(f'sbatch --mem={(MEMORY//1000) + 1}g -c{THREADS} --time=3-0 --requeue --killable --wrap=\"{command}\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "613b98dc-8627-4a8e-8787-53fdc807b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = VERSION + \"_temp_file.sh\"\n",
    "counter = 0\n",
    "with open(temp_file,\"w\") as f:\n",
    "    for com in commands:\n",
    "        f.write(com + \"\\n\")\n",
    "        f.write(\"sleep 2\\n\")\n",
    "        counter+=1\n",
    "        if counter%30 == 0:\n",
    "            f.write(\"sleep 60\\n\")\n",
    "\n",
    "!chmod 744 {temp_file}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efc411ac-f24d-4be1-bf22-675aab73f26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sci/nosnap/michall/roeizucker/jupyter_notebooks/Covariate_tests/multi_effect_score_1.1_temp_file.sh\n"
     ]
    }
   ],
   "source": [
    "!echo `pwd`/{temp_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c962fadc-db72-4954-9d7c-b1db5d8e6095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbatch --mem=39g -c20 --time=3-0 --requeue --killable --wrap=\"/sci/nosnap/michall/roeizucker/plink2 --bed /sci/archive/michall/roeizucker/small_genetics/allchr_thin_fml.bed --bim /sci/archive/michall/roeizucker/small_genetics/allchr_thin_fml.bim --fam /sci/archive/michall/roeizucker/small_genetics/allchr_thin_fml.fam --pheno /sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/phenotypes/generated_pehn_result_ver=multi_effect_score_1.1_heretability=0_covar_effect=0.txt --covar /sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/covariates_year_of_birth_0_PC.txt  --out  /sci/archive/michall/roeizucker/covar_tests_non_coding_HADASSA_PLINK/results/generated_pehn_result_ver=multi_effect_score_1.1_heretability=0_covar_effect=0covariates_year_of_birth_0_PC --glm hide-covar --mac 20 --covar-variance-standardize --freq --threads 20 --memory 38000 --1 --adjust\"\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 {temp_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93c3980f-2c5a-4a44-898a-dabe0ba51735",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# included_variants = major_variants.union(medium_variants).union(lesser_variants)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# # TODO: add protective variants\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# included_variants = major_variants.union(medium_variants).union(lesser_variants)\n",
    "# # TODO: add protective variants\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34da0a-1eac-4985-b0f8-95fc1d269a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"variant\"].isin(included_variants)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b13cc-4d8f-4178-aebd-794f0817209b",
   "metadata": {},
   "source": [
    "## GWAS simulation visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b478a4e-6009-40a2-ac78-901a9cde73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PVAL_THRESHOLD = 5*10**-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e7c72-268b-4715-9be3-798b32575fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_name = VERSION + \"_variants.json\"\n",
    "with open(json_file_name) as json_file:\n",
    "        variant_data = json.load(json_file)\n",
    "major_variants = variant_data[\"major_vars\"]\n",
    "med_vars = variant_data[\"med_vars\"]\n",
    "lesser_vars = variant_data[\"lesser_vars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3cc716-8589-46bf-938b-8aa8e29221da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_dict = sample_df[[\"variant\",\"snp\"]].set_index(\"variant\").to_dict()[\"snp\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2653f75-a815-48dd-8d67-5e86ed16eeb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374ac18-2749-46f7-b58f-5023e15e722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in variant_data:\n",
    "#     for i in range(len(variant_data[key])):\n",
    "#         variant_data[key][i] = var_dict[variant_data[key][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622550d-ce4d-419b-a325-cbf8f504eb61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f54b753-3ba0-460f-a49a-8439e14bf43c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for her_score in HERETABILITY_SCORES:\n",
    "    for cov_score in COVARIATE_SCORES:\n",
    "        if her_score + cov_score > 1:\n",
    "            continue\n",
    "        for covar_type in COVAR_GROUPS:\n",
    "            try:\n",
    "                phenotype_file_result_name = (\n",
    "                    f\"generated_pehn_result_ver={VERSION}_heretability={her_score}_covar_effect={cov_score}\")\n",
    "                phenotype_file_result_name = phenotype_file_result_name+\".txt\"\n",
    "                result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type))\n",
    "                covar_file = os.path.join(COVARS_PATH,covar_type + \".txt\")\n",
    "                result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type) + f\".{FILE_ENDING}\")\n",
    "                df = pd.read_csv(result_path,sep=\"\\t\")\n",
    "                df = df[df[\"P\"] < PVAL_THRESHOLD]\n",
    "                variants = set(df[\"ID\"])\n",
    "                found_major_common = variants.intersection( set(major_variants[0]))\n",
    "                found_major_rare = variants.intersection( set(major_variants[1]))\n",
    "                found_major_very_rare = variants.intersection( set(major_variants[2]))\n",
    "                \n",
    "                found_medium_common = variants.intersection( set(med_vars[0]))\n",
    "                found_medium_rare = variants.intersection( set(med_vars[1]))\n",
    "                found_medium_very_rare = variants.intersection( set(med_vars[2]))\n",
    "\n",
    "                \n",
    "                found_lesser_common = variants.intersection( set(lesser_vars[0]))\n",
    "                found_lesser_rare = variants.intersection( set(lesser_vars[1]))\n",
    "                found_lesser_very_rare = variants.intersection( set(lesser_vars[2]))\n",
    "                \n",
    "                results.append(\n",
    "                    [her_score,cov_score,covar_type,\n",
    "                     len(found_major_common),len(found_major_rare),len(found_major_very_rare),\n",
    "                     len(found_medium_common),len(found_medium_rare),len(found_medium_very_rare),\n",
    "                     len(found_lesser_common),len(found_lesser_rare),len(found_lesser_very_rare),\n",
    "                     len(variants),len(variants - (found_major_common.union(found_major_rare).union(found_major_very_rare).union(\n",
    "                                                  found_medium_common).union(found_medium_rare).union(found_medium_very_rare).union(\n",
    "                                                  found_lesser_common).union(found_lesser_rare).union(found_lesser_very_rare)))])\n",
    "                # print(variants)\n",
    "            except OSError as exc:\n",
    "                print(exc)\n",
    "                print(covar_file,\"failed\")\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "                print(result_path,\"check if done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1ee633-dc62-4647-963a-c6151da5bd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41abc003-fdd0-4cef-91df-db7ee4851ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"her_score\",\"cov_score\",\"covar_type\",\n",
    "        \"found_major_common\",\"found_major_rare\",\"found_major_very_rare\",\n",
    "        \"found_medium_common\",\"found_medium_rare\",\"found_medium_very_rare\",\n",
    "        \"found_lesser_common\",\"found_lesser_rare\",\"found_lesser_very_rare\",\n",
    "        \"found_overall\",\"found_extra\"]\n",
    "res_df = pd.DataFrame(results,columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34aad02-40b5-4540-81c6-a13498a00783",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(VERSION + \"result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6ea49-f608-4b14-8da9-188904f1e7d1",
   "metadata": {},
   "source": [
    "## rerun section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9087ec-642b-4cc5-88eb-b584376379fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_ver = \"multi_effect_score_0.11\"\n",
    "rerun_ending = \"PHENO1.glm.logistic.hybrid\"\n",
    "max_count = 30\n",
    "results = []\n",
    "for her_score in HERETABILITY_SCORES:\n",
    "    if max_count <= 0:\n",
    "        break\n",
    "    for cov_score in COVARIATE_SCORES:\n",
    "        if max_count <= 0:\n",
    "            break\n",
    "        if her_score + cov_score > 1:\n",
    "            continue\n",
    "        for covar_type in COVAR_GROUPS:\n",
    "            if max_count <= 0:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                phenotype_file_result_name = (\n",
    "                    f\"generated_pehn_result_ver={rerun_ver}_heretability={her_score}_covar_effect={cov_score}\")\n",
    "                phenotype_file_result_name = phenotype_file_result_name+\".txt\"\n",
    "                result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type))\n",
    "                covar_file = os.path.join(COVARS_PATH,covar_type + \".txt\")\n",
    "                result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type) + f\".{rerun_ending}\")\n",
    "                df = pd.read_csv(result_path,sep=\"\\t\")\n",
    "            except OSError as exc:\n",
    "                print(exc)\n",
    "                print(covar_file,\"failed\")\n",
    "            except Exception as exc:\n",
    "                chrom_path = os.path.join(GENETICS_PATH,TEST_CHROM)\n",
    "                phenotype_file_path = os.path.join(PHENOTYPE_DIR_RESULT_PATH,phenotype_file_result_name)\n",
    "                command =f\"{PLINK_PATH} --bed {chrom_path}.bed --bim {chrom_path}.bim --fam {chrom_path}.fam --pheno {phenotype_file_path} --covar {covar_file}  --out  {result_path} --glm hide-covar --mac 20 --covar-variance-standardize --freq --threads {THREADS} --memory {MEMORY} --1 --adjust\"\n",
    "                print(f'sbatch --mem={(MEMORY//1000) + 1}g -c{THREADS} --time=3-0 --requeue --killable --wrap=\"{command}\"')\n",
    "                max_count-=1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f949b0-1e94-451c-9eaa-31b5f192f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PVAL_THRESHOLD = 5*10**-5\n",
    "rerun_ver = \"multi_effect_score_0.10\"\n",
    "rerun_ending = \"PHENO1.glm.logistic.hybrid\"\n",
    "json_file_name = rerun_ver + \"_variants.json\"\n",
    "with open(json_file_name) as json_file:\n",
    "        variant_data = json.load(json_file)\n",
    "major_variants = variant_data[\"major_vars\"]\n",
    "med_vars = variant_data[\"med_vars\"]\n",
    "lesser_vars = variant_data[\"lesser_vars\"]\n",
    "\n",
    "results = []\n",
    "for her_score in HERETABILITY_SCORES:\n",
    "    for cov_score in COVARIATE_SCORES:\n",
    "        if her_score + cov_score > 1:\n",
    "            continue\n",
    "        for covar_type in COVAR_GROUPS:\n",
    "            try:\n",
    "                phenotype_file_result_name = (\n",
    "                    f\"generated_pehn_result_ver={rerun_ver}_heretability={her_score}_covar_effect={cov_score}\")\n",
    "                phenotype_file_result_name = phenotype_file_result_name+\".txt\"\n",
    "                result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type))\n",
    "                covar_file = os.path.join(COVARS_PATH,covar_type + \".txt\")\n",
    "                result_path = os.path.join(PLINK_RESULT_DIR,phenotype_file_result_name.replace(\".txt\",covar_type) + f\".{rerun_ending}\")\n",
    "                if os.path.exists(result_path + \".\" + rerun_ending):\n",
    "                    result_path = result_path + \".\" + rerun_ending\n",
    "                df = pd.read_csv(result_path,sep=\"\\t\")\n",
    "                df = df[df[\"P\"] < PVAL_THRESHOLD]\n",
    "                variants = set(df[\"ID\"])\n",
    "                found_major_common = variants.intersection( set(major_variants[0]))\n",
    "                found_major_rare = variants.intersection( set(major_variants[1]))\n",
    "                found_major_very_rare = variants.intersection( set(major_variants[2]))\n",
    "                \n",
    "                found_medium_common = variants.intersection( set(med_vars[0]))\n",
    "                found_medium_rare = variants.intersection( set(med_vars[1]))\n",
    "                found_medium_very_rare = variants.intersection( set(med_vars[2]))\n",
    "\n",
    "                \n",
    "                found_lesser_common = variants.intersection( set(lesser_vars[0]))\n",
    "                found_lesser_rare = variants.intersection( set(lesser_vars[1]))\n",
    "                found_lesser_very_rare = variants.intersection( set(lesser_vars[2]))\n",
    "                \n",
    "                results.append(\n",
    "                    [her_score,cov_score,covar_type,\n",
    "                     len(found_major_common),len(found_major_rare),len(found_major_very_rare),\n",
    "                     len(found_medium_common),len(found_medium_rare),len(found_medium_very_rare),\n",
    "                     len(found_lesser_common),len(found_lesser_rare),len(found_lesser_very_rare),\n",
    "                     len(variants),len(variants - (found_major_common.union(found_major_rare).union(found_major_very_rare).union(\n",
    "                                                  found_medium_common).union(found_medium_rare).union(found_medium_very_rare).union(\n",
    "                                                  found_lesser_common).union(found_lesser_rare).union(found_lesser_very_rare)))])\n",
    "                # print(variants)\n",
    "            except OSError as exc:\n",
    "                print(exc)\n",
    "                print(covar_file,\"failed\")\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "                print(result_path,\"check if done\")\n",
    "cols = [\"her_score\",\"cov_score\",\"covar_type\",\n",
    "        \"found_major_common\",\"found_major_rare\",\"found_major_very_rare\",\n",
    "        \"found_medium_common\",\"found_medium_rare\",\"found_medium_very_rare\",\n",
    "        \"found_lesser_common\",\"found_lesser_rare\",\"found_lesser_very_rare\",\n",
    "        \"found_overall\",\"found_extra\"]\n",
    "res_df = pd.DataFrame(results,columns=cols)\n",
    "res_df.to_csv(rerun_ver + \"result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
